<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-12-02T00:21:06.272Z</id>
    <title>osmos::feed</title>
    <updated>2023-12-02T00:21:06.272Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Issues with accessing Help Center]]></title>
        <id>https://status.rippling.com/incidents/53rn1yy678tz</id>
        <link href="https://status.rippling.com/incidents/53rn1yy678tz"/>
        <updated>2023-12-02T00:12:24.000Z</updated>
        <summary type="html"><![CDATA[Dec  2, 00:12 UTC
Identified - The issue has been identified and a fix is being implemented.
Dec  2, 00:12 UTC
Investigating - Customers are experiencing issues with accessing the Rippling Help Center. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delay To And From Twilio Numbers Towards Multiple Network In South Africa Over A Subset Of Long Code]]></title>
        <id>https://status.twilio.com/incidents/k1d3yfq8s660</id>
        <link href="https://status.twilio.com/incidents/k1d3yfq8s660"/>
        <updated>2023-12-01T23:44:29.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 15:44 PST
Update - We are experiencing SMS delivery delays when sending and receiving messages To and From Twilio Numbers towards multiple network in South Africa over a subset of long code. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hour or as soon as more information becomes available.
Dec  1, 14:42 PST
Update - We are experiencing SMS delivery delays when sending messages to Multiple Networks In South Africa Over A Subset Of Long Code. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec  1, 13:44 PST
Investigating - We are experiencing SMS delivery delays when sending messages to Multiple Networks In South Africa Over A Subset Of Long Code. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Failures on Altice Network in the US]]></title>
        <id>https://status.twilio.com/incidents/hxyl7w6yyr6y</id>
        <link href="https://status.twilio.com/incidents/hxyl7w6yyr6y"/>
        <updated>2023-12-01T23:35:18.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 15:35 PST
Resolved - SMS delivery failures sending to Altice Network in the US has been resolved and is operating normally at this time.
Dec  1, 15:05 PST
Update - We continue to experience SMS delivery failures sending to Altice Network in the US. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 8 hours or as soon as more information becomes available.
Dec  1, 11:07 PST
Update - We continue to experience SMS delivery failures sending to Altice Network in the US. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 4 hours or as soon as more information becomes available.
Dec  1, 09:13 PST
Update - We continue to experience SMS delivery failures sending to Altice Network in the US. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 2 Hours or as soon as more information becomes available.
Dec  1, 08:16 PST
Investigating - We are experiencing SMS delivery failures sending to Altice Network in the US. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 Hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to 3 (Hutchison 3G) Network in the United Kingdom]]></title>
        <id>https://status.twilio.com/incidents/bpktz45ynzkf</id>
        <link href="https://status.twilio.com/incidents/bpktz45ynzkf"/>
        <updated>2023-12-01T21:32:16.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 13:32 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to 3 (Hutchison 3G) network in the United Kingdom. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec  1, 11:22 PST
Update - We continue to experience SMS delivery delays when sending messages to 3 (Hutchison 3G) network in the United Kingdom. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec  1, 10:22 PST
Investigating - We are experiencing SMS delivery delays when sending messages to 3 (Hutchison 3G) network in the United Kingdom. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.
Our engineering team continues to investigate the issue.
We will provide an update by Friday, 2023-12-01 12:15 US/Pacific with current details.
We apologize to all who are affected by the disruption.
Customer Symptoms : Customers may experience delays in their email delivery
Workaround : None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn"/>
        <updated>2023-12-01T20:36:58.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-12-01 18:52</strong> and ended at <strong>2023-12-01 20:20</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><p>The issue with Gmail has been resolved for all affected users as of Friday, 2023-12-01 12:20 US/Pacific.</p>
<p>We will publish an analysis of this incident once we have completed our internal investigation.</p>
<p>We thank you for your patience while we worked on resolving the issue.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDATE: We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.
Our engineering team continues to investigate the issue.
We will provide an update by Friday, 2023-12-01 12:15 US/Pacific with current details.
We apologize to all who are affected by the disruption.
Customer Symptoms : Customers may experience delays in their email delivery
Workaround : None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn"/>
        <updated>2023-12-01T20:16:26.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-12-01 18:52</strong> and ended at <strong>2023-12-01 20:20</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><p>Mitigation work is currently underway by our engineering team. Our engineers are seeing positive signs of recovery.</p>
<p>However we are actively monitoring to ensure the systems are fully recovered.</p>
<p>We will provide more information by Friday, 2023-12-01 13:00 US/Pacific.</p>
<p>Customer Symptoms : Customers may experience delays in their email delivery. Customers may not be required to re-send the affected messages unless they bounce back. Customers with messages traversing in our US-based servers may be impacted.</p>
<p>Workaround : No action required from Customers unless the email that has bounced back.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDATE: We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.
Our engineering team continues to investigate the issue.
We will provide an update by Friday, 2023-12-01 12:15 US/Pacific with current details.
We apologize to all who are affected by the disruption.
Customer Symptoms : Customers may experience delays in their email delivery
Workaround : None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn"/>
        <updated>2023-12-01T20:06:15.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-12-01 18:52</strong> and ended at <strong>2023-12-01 20:20</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><p>We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.</p>
<p>Our engineering team continues to investigate the issue.</p>
<p>We will provide an update by Friday, 2023-12-01 12:45 US/Pacific with current details.</p>
<p>We apologize to all who are affected by the disruption.</p>
<p>Customer Symptoms : Customers may experience delays in their email delivery. Customers may not be required to re-send the affected messages unless they bounce back. Customers with messages traversing in our US-based servers may be impacted.</p>
<p>Workaround : No action required from Customers unless the email that has bounced back.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDATE: We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.
Our engineering team continues to investigate the issue.
We will provide an update by Friday, 2023-12-01 12:15 US/Pacific with current details.
We apologize to all who are affected by the disruption.
Customer Symptoms : Customers may experience delays in their email delivery
Workaround : None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/LWGeK5YBtJL8kE4nfmNn"/>
        <updated>2023-12-01T19:24:57.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-12-01 18:52</strong> and ended at <strong>2023-12-01 20:20</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><p>We are experiencing an issue with Gmail beginning at Friday, 2023-12-01 10:52 US/Pacific.</p>
<p>Our engineering team continues to investigate the issue.</p>
<p>We will provide an update by Friday, 2023-12-01 12:15 US/Pacific with current details.</p>
<p>We apologize to all who are affected by the disruption.</p>
<p>Customer Symptoms : Customers may experience delays in their email delivery</p>
<p>Workaround : None at this time.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/fj7v3z6fy7ym</id>
        <link href="https://www.githubstatus.com/incidents/fj7v3z6fy7ym"/>
        <updated>2023-12-01T18:16:24.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 18:16 UTC
Resolved - This incident has been resolved.
Dec  1, 17:36 UTC
Update - A small percentage of Copilot Chat users are still experiencing long request times and errors. We are still investigating to determine the root cause.
Dec  1, 16:41 UTC
Update - Some customers are experiencing higher latency for Copilot Chat. We are continuing our investigation.
Dec  1, 15:55 UTC
Update - Copilot is experiencing degraded performance. We are continuing to investigate.
Dec  1, 15:53 UTC
Update - We are investigating reports that that some customers are experiencing increased latency and failed requests for Copilot Chat.
Dec  1, 15:49 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Login code delay]]></title>
        <id>https://status.notion.so/incidents/4htm8cyzfh5w</id>
        <link href="https://status.notion.so/incidents/4htm8cyzfh5w"/>
        <updated>2023-12-01T11:30:54.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 03:30 PST
Resolved - Between 3:30 AM - 5:50 AM PST, some users experienced delays with their sign up and login code emails sending.
This issue was resolved at 5:50 AM PST. All sign up and login code emails should be delivered as expected.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We are experiencing an issue with Gmail beginning on Thursday, 2023-11-30 08:30 US/Pacific.
Our engineering team continues to investigate the issue.
We will provide an update by Thursday, 2023-11-30 10:30 US/Pacific with current details.
We apologize to all who are affected by the disruption.
Customer Symptoms :
Customers impacted by this issue may see delay in sending the emails.
Workaround:
None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/Hjq8CpkhWroicw7R1wSh</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/Hjq8CpkhWroicw7R1wSh"/>
        <updated>2023-12-01T01:43:16.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-11-30 16:30</strong> and ended at <strong>2023-11-30 21:15</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced an impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 30 November 2023 08:30</p>
<p><strong>Incident End:</strong> 30 November 2023 13:15</p>
<p><strong>Duration:</strong> 4 hours, 45 minutes</p>
<p><strong>Affected Services and Features:</strong> Gmail</p>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Some Gmail users experienced a delay in sending emails globally for a duration of 4 hours and 45 minutes. Any emails that were delayed after entering our system were auto retried and should have been delivered within a couple of hours after mitigation. From preliminary analysis, the root cause of the issue appears to be an overload in Google Message Router (GMR). Google will complete a full incident report in the following days that will provide a detailed root cause.</p>
<p><strong>Customer Impact:</strong></p>
<p>Customers experienced delays while sending the emails.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in BLR1 for a Subset of Droplets]]></title>
        <id>https://status.digitalocean.com/incidents/1n8mgx97nfb0</id>
        <link href="https://status.digitalocean.com/incidents/1n8mgx97nfb0"/>
        <updated>2023-11-30T18:12:21.000Z</updated>
        <summary type="html"><![CDATA[Nov 30, 18:12 UTC
Resolved - Beginning around 16:48 UTC, our Engineering team identified an issue impacting a subset of Droplets in our BLR1 region. From 16:48 UTC to 17:45 UTC, impacted Droplets did not have correctly functioning networking capabilities. Additionally, users were unable to perform any events (such as power off, recycle, resize, etc) and any changes to the Cloud Firewalls for impacted Droplets were not being applied.
Our Engineering team has resolved the issue and networking and the Cloud Firewalls on all Droplets should be functioning normally.  
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with some users who are attempting to enroll their Windows computers with Rippling's MDM server]]></title>
        <id>https://status.rippling.com/incidents/3vm3nr1kcc68</id>
        <link href="https://status.rippling.com/incidents/3vm3nr1kcc68"/>
        <updated>2023-11-29T22:16:11.000Z</updated>
        <summary type="html"><![CDATA[Nov 29, 22:16 UTC
Resolved - This incident has been resolved.
Nov 29, 19:38 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 29, 17:22 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack not loading for some users]]></title>
        <id>https://status.slack.com//2023-11/edc9f768095e939b</id>
        <link href="https://status.slack.com//2023-11/edc9f768095e939b"/>
        <updated>2023-11-29T14:41:20.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 9:30 AM PST on November 28, 2023 to 12:30 AM PST on November 29, 2023, some users in Asia were experiencing issues with Slack loading. Most of the impacted users were in Japan and India.


We traced this back to an unexpected spike in a database load for which we quickly saw recovery.


We monitored the situation closely and as the load reduced without intervention, we did not take any mitigating actions.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/dq9m4n5tfyr6</id>
        <link href="https://www.githubstatus.com/incidents/dq9m4n5tfyr6"/>
        <updated>2023-11-28T19:40:12.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 19:40 UTC
Resolved - This incident has been resolved.
Nov 28, 19:36 UTC
Update - We were not able to publish webhooks in response to push events triggered between 16:23 and 17:12 UTC. To avoid further disruption to customer workflows, we’ve decided not to continue our attempts to re-process those events. 
Workflows that trigger on pull_request or git push events may not have been run during this time period. 
You can run impacted workflows manually or by pushing a new commit to the same branch.
Nov 28, 18:09 UTC
Update - Customers saw push event deliveries for Actions and Webhooks fail between 16:23 and 17:12 (UTC). We fixed the issue, and we are working to re-process push events for the affected time period.
Nov 28, 18:09 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/dsb3kn1zfdl7</id>
        <link href="https://www.githubstatus.com/incidents/dsb3kn1zfdl7"/>
        <updated>2023-11-28T17:59:15.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 17:59 UTC
Resolved - This incident has been resolved. An interaction between two feature flag rollouts caused us to suppress delivery of push webhooks between 16:23 and 17:11 UTC in a manner that evaded our existing observability. This affected 71,000 repositories whose users will have noted missing webhook deliveries and/or experienced Actions jobs with push triggers failing to start. During this period, around 25% of new Actions jobs were impacted. The issues were resolved by disabling one of the feature flags in question.
After weighing various options for retroactively dispatching old push webhooks, we concluded that the risk of delivering stale data to customers with these redeliveries outweighed the possible benefit.
As follow up to this incident, we are working to improve monitoring of webhook throughput and to document a policy around webhook redelivery timelines.
Nov 28, 17:59 UTC
Update - Actions is operating normally.
Nov 28, 17:36 UTC
Update - Customers saw pull requests push event deliveries for Actions and Webhooks fail between 16:23 and 17:12 (UTC). We fixed the issue, and we are working to re-process push events for the affected time period.
Nov 28, 17:24 UTC
Investigating - We are investigating reports of degraded performance for Actions and Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Datastore App timing out]]></title>
        <id>https://status.make.com/incidents/3yz9s3tkz2xh</id>
        <link href="https://status.make.com/incidents/3yz9s3tkz2xh"/>
        <updated>2023-11-28T08:55:30.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 09:55 CET
Resolved - The implemented workaround has been effective and eu1.make.celonis.com remains stable. The incident has been resolved.
Nov 27, 17:18 CET
Update - The situation stays stable. We are still investigating the root cause of the problem. We will maintain ongoing monitoring and notify once we fully resolve the incident.
Nov 27, 16:31 CET
Monitoring - The tuning of our Data Store infrastructure did relieve the situation and Scenarios using the Data Store App are stable again. We are continuously monitoring the state of our platform. We will come up with an update in the next 30 minutes. Thank you for your understanding and patience.
Nov 27, 15:59 CET
Update - We are still investigating the root cause of this incident. Meanwhile, we are tuning our Data Store infrastructure to relieve the situation and put the platform in a more stable state. We will come up with an update in the next 30 minutes. Thank you for your understanding and patience.
Nov 27, 15:21 CET
Investigating - We have detected multiple instances of the Datastore App timing out and failing on eu1.make.celonis.com. We are currently investigating the root cause and will come back to you once we find more information. Thank you for your understanding and patience.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests, Issues, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/66vhjmd266r9</id>
        <link href="https://www.githubstatus.com/incidents/66vhjmd266r9"/>
        <updated>2023-11-27T21:11:04.000Z</updated>
        <summary type="html"><![CDATA[Nov 27, 21:11 UTC
Resolved - This incident has been resolved.
On November 27, 2023 at 18:46 UTC, we attempted to rotate our OpenID Connect (OIDC) authentication flow certificates. Due to an error in the certificate formatting, we uploaded an invalid certificate configuration that was not observed in our pre-production testing. Our background job servers were unable to start because a valid configuration is required at worker start up. As a result, users experienced delays in Pull Requests, Webhooks, Issues, Actions and Projects. Rollback of the change was slowed by the invalid certificate as our deployment system relied on the same certificate. Rollback was completed at 20:35 UTC. Most services recovered by 20:44 UTC. 

Delayed updates to Issues and Pull Requests were applied normally once…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing our enhanced Stripe integration 🚀]]></title>
        <id>279856</id>
        <link href="https://changelog.bookingsync.com/introducing-our-enhanced-stripe-integration-279856"/>
        <updated>2023-11-24T13:25:46.000Z</updated>
        <summary type="html"><![CDATA[New!
 
Improvement
 
Action required
  
We're thrilled to share some fantastic updates on our Stripe integration, aiming to improve your Smily experience. 🎉
What's New?
Effortless Stripe Account Addition: managing multiple payment gateways just got easier! We've streamlined the process of adding new Stripe accounts for flexibility and convenience. 


Improved Customer Support Access: your satisfaction is our top priority. Now, our customer support is better equipped to assist you with any questions or issues related to your Stripe integration.


Why Does This Matter?
This upgrade is designed with your convenience in mind, offering the following benefits:
Time Saving: adding new Stripe accounts is now a few clicks away, eliminating lengthy back-and-forth communications.


Reduced Frustration: improved support ensures prompt resolution of queries or issues.


Maintained Control: you retain complete control over your Stripe accounts, payouts, and settings, experiencing no change in usage or payment reception.


What's Next?
Ready to unlock the enhanced Stripe Integration? Go to Settings > Payments in your Smily account, click connect on any unconnected Stripe payment gateways, and enjoy seamless transactions. 
Rest assured, these improvements maintain the highest security standards for your financial transactions.
For detailed information on connecting your Stripe account, check our FAQ.
We're here to support you every step of the way. If you have questions or need assistance, reach out to us. 
Let's embark on this journey together to make your vacation rental dreams a reality 🚀]]></summary>
        <author>
            <name>Megan, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with the Adobe integration]]></title>
        <id>https://status.rippling.com/incidents/sj7thsw4jlzl</id>
        <link href="https://status.rippling.com/incidents/sj7thsw4jlzl"/>
        <updated>2023-11-23T22:41:38.000Z</updated>
        <summary type="html"><![CDATA[Nov 23, 22:41 UTC
Resolved - This incident has been resolved.
Nov 23, 16:29 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 22, 18:03 UTC
Identified - The issue has been identified and a fix is being implemented.
Nov 22, 17:37 UTC
Investigating - Customers are experiencing issues with the assignment of Adobe licenses to users. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/69628qhdf41j</id>
        <link href="https://status.notion.so/incidents/69628qhdf41j"/>
        <updated>2023-11-22T02:00:20.000Z</updated>
        <summary type="html"><![CDATA[Nov 21, 18:00 PST
Resolved - This is resolved.
Nov 21, 16:50 PST
Update - Provider calls are more stable. We are monitoring our provider.
Nov 21, 16:43 PST
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 21, 16:42 PST
Identified - Notion AI is partially not working as one of our service providers is down]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/x39xrr5m11b3</id>
        <link href="https://www.githubstatus.com/incidents/x39xrr5m11b3"/>
        <updated>2023-11-21T11:27:43.000Z</updated>
        <summary type="html"><![CDATA[Nov 21, 11:27 UTC
Resolved - On November 21, 2023, at 09:50 UTC GitHub Actions jobs encountered delays due to an incident in our background job service caused by excessive rebalancing in a Kafka consumer group. After a quick mitigation, we began to see recovery on the job queues by 10:02 UTC. During this time window 100% of Actions jobs were delayed in starting for up to 11 minutes.
Unfortunately, the rapid queue recovery sent a thundering herd of jobs to Actions hosted runner pools, causing a database deadlock that resulted in some hosted runner pools having increased latency when accepting new jobs. This affected only a small percentage of overall jobs, around 2%. Configuration changes led to a resolution and the system was fully recovered by 11:27 UTC and all in progress jobs were processed.
The incident is now resolved.
Nov 21, 11:12 UTC
Update - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.
Nov 21, 10:24 UTC
Update - We have recovery for the underlying issue but are waiting for Actions queues to catch up. We expect this to be completed in less than 1 hour(s).
Nov 21, 10:11 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue affecting template duplication]]></title>
        <id>https://status.notion.so/incidents/tk0hmlbd3lg9</id>
        <link href="https://status.notion.so/incidents/tk0hmlbd3lg9"/>
        <updated>2023-11-18T00:04:17.000Z</updated>
        <summary type="html"><![CDATA[Nov 17, 16:04 PST
Resolved - Our team has now resolved the issue preventing template duplication, and this is working as normal again. We appreciate your patience while we worked through this issue.
Nov 17, 14:05 PST
Identified - We are experiencing an issue with duplicating published templates and our team is actively working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues with huddles]]></title>
        <id>https://status.slack.com//2023-11/2b97ec921a81988a</id>
        <link href="https://status.slack.com//2023-11/2b97ec921a81988a"/>
        <updated>2023-11-17T21:42:44.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

On November 8, 2023 from 7:55 AM PT until 8:27 PM PT, a small number of users experienced errors when attempting to join or start huddles.


We traced this back to an unexpected spike in a database load for which we quickly saw recovery.


We monitored the situation closely and in an effort to prevent further unexpected error spikes, we implemented a strategic configuration adjustment to optimize the load on our database.


Affected users should no longer experience issues with huddles.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/vvx0wz4pgd9k</id>
        <link href="https://status.notion.so/incidents/vvx0wz4pgd9k"/>
        <updated>2023-11-17T03:11:59.000Z</updated>
        <summary type="html"><![CDATA[Nov 16, 19:11 PST
Resolved - The incident has been resolved. Time of resolution Nov 16 2023 6:46PM PST
Nov 16, 19:09 PST
Identified - Notion AI is down. We are working with them on a fix. Time - Nov 16 2023 6:11PM PST]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent scenario execution errors in eu1, eu2 and us1 zones]]></title>
        <id>https://status.make.com/incidents/d82gd0vnvgz1</id>
        <link href="https://status.make.com/incidents/d82gd0vnvgz1"/>
        <updated>2023-11-16T08:45:00.000Z</updated>
        <summary type="html"><![CDATA[Nov 16, 09:45 CET
Resolved - Due to a configuration error, some scenarios in the eu1.make.com, eu2.make.com and us1.make.com zones may have intermittently failed to execute and in case of multiple consecutive errors get disabled by the system. According to our telemetry, the number of impacted scenarios was very small. Customers may have experienced this behavior between 9:45am and 8:00pm CET. We have addressed the configuration problem and all executions are now stable. We will continue monitoring the situation.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inquiry and Pre-Approval Issues]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/l9cxvbj0pq59</id>
        <link href="https://airbnbapi.statuspage.io/incidents/l9cxvbj0pq59"/>
        <updated>2023-11-16T08:00:00.000Z</updated>
        <summary type="html"><![CDATA[Nov 16, 00:00 PST
Resolved - On Nov 16 and 17, some inquiries and pre-approvals expired prematurely, causing guests to be unable to accept them because Airbnb incorrectly showed these listings as unavailable, even though they were actually available. Additionally, Hosts were experiencing difficulties in creating new pre-approvals during this period. We apologize for the inconvenience that this issue has caused.
This issue started on Nov 16 around 12:00 AM PST and was resolved on Nov 17 at 9:10 AM PST, and we expect this to be fully resolved. Please contact us if you are still seeing this issue.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/mnv0g944fncw</id>
        <link href="https://www.githubstatus.com/incidents/mnv0g944fncw"/>
        <updated>2023-11-15T11:34:14.000Z</updated>
        <summary type="html"><![CDATA[Nov 15, 11:34 UTC
Resolved - On 2023-11-15, from 09:44 to 10:42 UTC, some GitHub customers experienced increased latency or errors accessing repo data.
High concurrent access to a specific git object exposed a bug that forced a backend service to perform excessive calculations, overloading the service. Access to this repo was paused while load was re-rerouted, mitigating the problem.
The conditions that triggered the expensive operations have been identified and refactored.
Nov 15, 11:33 UTC
Update - Error rates and performance have returned to normal.
Nov 15, 11:21 UTC
Update - We have identified the source of the issue and have removed the additional load from the service. Sporadic delays in pull request experiences and intermittent 500s are still occurring and impacting a very small percentage of traffic. Next update is expected within 30 minutes.
Nov 15, 11:04 UTC
Update - We are seeing connectivity issues between some of our systems and git backend services. This is causing intermittent error responses and delays in pull request experiences for a very small percentage of traffic. We are investigating mitigations and expect to provide another update within 30 minutes.
Nov 15, 09:50 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Slack not loading for some users]]></title>
        <id>https://status.slack.com//2023-11/0e7bc0e7a7e7cd87</id>
        <link href="https://status.slack.com//2023-11/0e7bc0e7a7e7cd87"/>
        <updated>2023-11-15T03:01:39.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 3:15 PM PST on November 14, 2023 to around 4:13 PM PST, a small number of customers using the Slack desktop app were unable to connect to Slack. This may have manifested as a "Something's gone awry" error page.


A recent code change inadvertently introduced a logic error that prevented the desktop app from connecting as expected. We reverted this code change as an immediate mitigation step, then rolled out a fix to correct the logic error. 


The fix resolved the issue for all affected customers, restoring full access to Slack.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOKS in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/wpz7gdly2p32</id>
        <link href="https://status.digitalocean.com/incidents/wpz7gdly2p32"/>
        <updated>2023-11-15T01:59:25.000Z</updated>
        <summary type="html"><![CDATA[Nov 15, 01:59 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting DOKS across all regions. 
After the rollout, we are no longer reliant on that affected upstream provider for our DOKS product.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Nov 15, 00:42 UTC
Monitoring - Our Engineering team has completed the rollout to pivot away from the affected upstream provider and we are no longer reliant on that provider for our DOKS product. 
At this time, Users should no longer see any issues with nodes going into not ready states, creating new clusters, or scaling up additional nodes. 
We're monitoring the fix and will post another update once we confirm this issue is fully resolved.
Nov 14, 23:24 UTC
Update - Our Engineering team is currently working to pivot away from the affected upstream provider to mitigate impact from this incident. That fix is rolling out across our fleet and users should start to see conditions on affected clusters improve. 
As soon as the fix is rolled out completely, we'll post another update.
Nov 14, 22:20 UTC
Update - Our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS across all regions which was initially reported for a few regions. 
During this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state across all regions.
We will share an update as soon as we have any information from our upstream provider.
Nov 14, 21:59 UTC
Identified - Beginning around 20:00 UTC, our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS in our multiple regions. 
During this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state. 
We will share an update as soon as we have any information from our upstream provider.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EU2 hooks delayed processing]]></title>
        <id>https://status.make.com/incidents/dzjb57y129qn</id>
        <link href="https://status.make.com/incidents/dzjb57y129qn"/>
        <updated>2023-11-14T17:04:00.000Z</updated>
        <summary type="html"><![CDATA[Nov 14, 18:04 CET
Resolved - This incident has been resolved.
Nov 14, 13:53 CET
Update - The current situation is stable, and after performing a series of checks, no issues were observed. We will maintain ongoing monitoring for the next 4 hours, and if everything remains stable during this period, we will proceed to resolve the incident.
Nov 13, 21:16 CET
Monitoring - The issue is currently stable. We will continue careful monitoring of the situation and provide updates regularly.
Nov 13, 16:58 CET
Update - Our team is continuing to investigate the technical difficulties affecting webhooks and mailhooks on eu2.make.com. At this time, we have not yet identified the root cause, and users may still experience sporadic delays in the processing of these services.
Nov 13, 14:54 CET
Investigating - We are currently experiencing technical difficulties with webhooks and mailhooks on eu2.make.com. Users may encounter delays in the processing of webhooks and mailhooks.
We will provide another update on this Statuspage within the next 2 hours or as soon as more information becomes available.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/knl5pxnx0byt</id>
        <link href="https://www.githubstatus.com/incidents/knl5pxnx0byt"/>
        <updated>2023-11-13T21:38:40.000Z</updated>
        <summary type="html"><![CDATA[Nov 13, 21:38 UTC
Resolved - Between 20:35 and 21:38 we experienced up to a 20 minute delay delivering around 30,000 notifications due to side effects of some planned maintenance on supporting systems. We have noted the unexpected user impact of this type of maintenance and will address it in future maintenance planning.
Nov 13, 21:38 UTC
Update - An issue related to notifications has been resolved. Users should again be seeing their notifications.
Nov 13, 21:15 UTC
Update - We're seeing issues related to notifications.
Nov 13, 21:13 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: User presence is unexpectedly changing]]></title>
        <id>https://status.slack.com//2023-11/16fed44d7948cf49</id>
        <link href="https://status.slack.com//2023-11/16fed44d7948cf49"/>
        <updated>2023-11-13T15:57:36.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 4:00 PM PDT on October 31, 2023 to 2:00 PM PDT on November 9, 2023, user presence would unexpectedly change to away or inactive.


We determined that an error during a routine system optimization on connectivity state caused this issue. 


We reverted the change which fixed the issue for all affected customers.


Thank you for your patience while we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pages, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/wyk0ns67krlz</id>
        <link href="https://www.githubstatus.com/incidents/wyk0ns67krlz"/>
        <updated>2023-11-11T02:14:08.000Z</updated>
        <summary type="html"><![CDATA[Nov 11, 02:14 UTC
Resolved - On November 11, 2023, at 1:00 UTC, GitHub background jobs encountered delays lasting up to 50 minutes. This delay affected various services utilizing background jobs, including Actions, Webhooks, Pull Requests, and Pages. The impact persisted for approximately one hour until 2:10 UTC.
During the incident, some customers experienced delays in starting Github Actions workflow runs and Pages builds. We estimate that about 10% of Actions workflow runs were delayed during the impact window and 99% of Pages builds failed from 1:00 UTC to 1:20 UTC. Users may have experienced a delay in seeing recent pushes reflected in pull request views. This delay averaged between 5 and 10 minutes and affected up to 30% of pull request page views during the incident. 1% of pull requ…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/54s1f3rs3n56</id>
        <link href="https://status.rippling.com/incidents/54s1f3rs3n56"/>
        <updated>2023-11-10T23:59:07.000Z</updated>
        <summary type="html"><![CDATA[Nov 10, 23:59 UTC
Resolved - This incident has been resolved.
Nov  9, 19:56 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov  9, 18:41 UTC
Update - We are continuing to investigate this issue.
Nov  9, 18:39 UTC
Update - We are continuing to investigate this issue.
Nov  9, 18:34 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent 500 errors on the GET Resolutions API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt</id>
        <link href="https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt"/>
        <updated>2023-11-09T22:49:38.000Z</updated>
        <summary type="html"><![CDATA[Nov  9, 14:49 PST
Resolved - This incident has been resolved.
Nov  8, 10:38 PST
Identified - We have observed a higher number of intermittent 500 errors on the GET Resolutions API. We are actively working on resolving this issue and plan to deploy a fix tomorrow (Nov 9, 2023). Thank you for your understanding and patience as we work to rectify this situation.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with user status, read state, and file previews]]></title>
        <id>https://status.slack.com//2023-11/ea3a2a3e32e79902</id>
        <link href="https://status.slack.com//2023-11/ea3a2a3e32e79902"/>
        <updated>2023-11-09T04:29:31.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 10:00 PM PST on November 7, 2023 to 1:15 PM PST on November 8, 2023, some users experienced issues with their user status not updating, removing previews, and being unable to mark channels as read. Some keyboard shortcuts were also affected and were unable to be used.


We determined that a recent code change was the root cause for the unexpected behaviour with these features.


To restore functionality, we reverted the related code. We then did some additional testing and monitoring to confirm all issues were fully resolved and Slack was operating as expected.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking Connectivity Between NYC and SFO]]></title>
        <id>https://status.digitalocean.com/incidents/hxcw6mrw1ywv</id>
        <link href="https://status.digitalocean.com/incidents/hxcw6mrw1ywv"/>
        <updated>2023-11-08T18:07:20.000Z</updated>
        <summary type="html"><![CDATA[Nov  8, 18:07 UTC
Resolved - Our Engineering team has seen no recurrences and performance has remained stable since 16:40 UTC. This incident is fully resolved.
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience!
Nov  8, 17:23 UTC
Update - Our Engineering team identified a configuration that was responsible for the recurrence we saw. From 16:20 - 16:40 UTC, connectivity between SFO2 and the rest of DigitalOcean's network was impacted.
As of 16:40 UTC, all impact has subsided and users should no longer face any issues.
We are monitoring the situation closely and will share an update once the issue is completely resolved.
Nov  8, 16:32 UTC
Update - Our Engineering team is seeing a recurrence of network alerts that indicate we're exp…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/n1sp5244k09v</id>
        <link href="https://status.notion.so/incidents/n1sp5244k09v"/>
        <updated>2023-11-08T15:28:28.000Z</updated>
        <summary type="html"><![CDATA[Nov  8, 07:28 PST
Resolved - The incident has been resolved. Time of resolution Nov 8 2023 7:28AM PST
Nov  8, 07:28 PST
Monitoring - Our AI provider has implemented a fix and we are seeing Notion AI recover gradually since 7:28AM PST. We are currently monitoring the situation.
Nov  8, 06:57 PST
Update - The issue has been identified now and a fix is being worked on for this issue.
Nov  8, 06:29 PST
Identified - One of our AI providers is down. We are working with them on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Users unable to connect to Slack or send messages]]></title>
        <id>https://status.slack.com//2023-11/ef3e4b0ebcf16d8d</id>
        <link href="https://status.slack.com//2023-11/ef3e4b0ebcf16d8d"/>
        <updated>2023-11-08T01:59:22.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On November 4, 2023, between 5:09 PM PDT and 5:20 PM PDT, many customers were unable send messages or to connect to Slack.


A routine code change introduced a database error that prevented cached data from being cleared correctly, resulting in severe performance issues.


We rolled back the code change and refreshed all affected servers, resolving the issue for all users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/m61vxgn4kvh2</id>
        <link href="https://www.githubstatus.com/incidents/m61vxgn4kvh2"/>
        <updated>2023-11-07T14:25:40.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 14:25 UTC
Resolved - Our internal search infrastructure experienced increased latency and timeouts between 13:15 and 14:05 UTC leading to some end user timeouts and slow responses for any requests that made use of that subsystem. This included but was not limited to: user search, repository search, releases, and audit logs.
We mitigated the issue by migrating traffic to an older version of our search clusters and are investigating what caused the performance issues in our new clusters.
Nov  7, 14:25 UTC
Update - API Requests is operating normally.
Nov  7, 14:15 UTC
Update - Response times stabilized back to normal at 13:58 UTC.  We are continuing to monitor the slow dependency to ensure it's stable before resolving this incident.
Nov  7, 14:03 UTC
Update - We're seeing intermittent spikes in latency of API requests and page loads.  We are investigating but do not have an ETA at this time.
Nov  7, 13:50 UTC
Update - We are investigating reports of issues with service(s): Issues, API Requests. We will continue to keep users updated on progress towards mitigation.
Nov  7, 13:46 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Nov  7, 13:44 UTC
Investigating - We are investigating reports of degraded performance for API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Failing to execute scenarios]]></title>
        <id>https://status.make.com/incidents/g6gklsv4shx6</id>
        <link href="https://status.make.com/incidents/g6gklsv4shx6"/>
        <updated>2023-11-07T11:43:11.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 12:43 CET
Resolved - This incident has been resolved.
Nov  6, 16:30 CET
Monitoring - We have noticed a degradation of performance on eu1.make.celonis.com. A fix has been applied and the system is fully operational again.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/w8r2ldkc0rcj</id>
        <link href="https://status.rippling.com/incidents/w8r2ldkc0rcj"/>
        <updated>2023-11-07T07:51:01.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 07:51 UTC
Resolved - This incident has been fully resolved.
Nov  6, 19:08 UTC
Monitoring - The Rippling app has mostly recovered but there are still a few lags in performance that we're further monitoring and investigating.
Nov  6, 18:33 UTC
Investigating - We are continuing to see degraded performance in the Rippling app, so we are continuing to investigate.
Nov  6, 17:28 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov  6, 17:25 UTC
Investigating - There are issues loading Rippling. We are working on a fix.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/bw3r3j9b5ph5</id>
        <link href="https://status.digitalocean.com/incidents/bw3r3j9b5ph5"/>
        <updated>2023-11-04T08:18:05.000Z</updated>
        <summary type="html"><![CDATA[Nov  4, 08:18 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting App Platform Deployments. 
From 11:54 on Nov 2nd to 06:42 on Nov 4th UTC, App Platform users may have experienced delays when deploying new apps or when deploying updates to existing Apps. Our Upstream provider and the Engineering team closely worked together to resolve the issue. 
The impact has been completely subsided and users should no longer see any issues with the impacted services.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel. Thank you for your patience and we apologize for any inconvenience.
Nov  4, 06:53 UTC
Update - As per the recent update from our Upstream provider, they fully recovered the services used…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in SFO Regions]]></title>
        <id>https://status.digitalocean.com/incidents/nz33vs0sjhqy</id>
        <link href="https://status.digitalocean.com/incidents/nz33vs0sjhqy"/>
        <updated>2023-11-03T23:14:12.000Z</updated>
        <summary type="html"><![CDATA[Nov  3, 23:14 UTC
Resolved - Our Engineering team has confirmed the full resolution of issue impacting network connectivity in our SFO regions. 
Users should no longer experience any latency or timeout issue with any of the Droplet based services. 
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Nov  3, 20:58 UTC
Monitoring - As of 19:55 UTC, our Engineering team has confirmed that a fix has been implemented by our upstream carrier to mitigate the cause of the issue impacting network connectivity in our SFO region. 
We are closely monitoring the situation and will update as soon as we have more information from the provider.
Nov  3, 19:43 UTC
Identified - Our Engineering team has identified the cause of the issue impacting network connectivity in our SFO region. Upstream congestion with a network provider between Los Angeles and Dallas is impacting traffic traversing out of our SFO datacentres.
A case has been opened by our team with the provider. Our team has attempted to shift traffic to improve the situation, but unfortunately, we continue to see approximately 10% of customer traffic impacted by this issue.
Our team is working on an option to shift to an alternate provider if this issue is not able to be resolved by the provider in a timely manner. We will share another update once we have further information from the provider or we have an update from our Engineering team.
Nov  3, 19:27 UTC
Investigating - As of 17:40 UTC, our Engineering team is investigating an issue impacting networking in the SFO regions. During this time, a subset of users may experience packet loss/latency and timeouts with Droplet based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations, Issues, Pull Requests, Actions, API Requests, Codespaces, Packages, Pages and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/xb30mby9fs5x</id>
        <link href="https://www.githubstatus.com/incidents/xb30mby9fs5x"/>
        <updated>2023-11-03T19:21:48.000Z</updated>
        <summary type="html"><![CDATA[Nov  3, 19:21 UTC
Resolved - A performance and resilience optimization to the authorization microservice contained a memory leak that was exposed under high traffic. This resulted in a number of pages returning 404’s that should not have. Testing the build in our canary ring did not expose the service to sufficient traffic to discover the leak, allowing it to graduate to production at 6:37 PM UTC.  The memory leak under high load caused pods to crash repeatedly starting at 6:42 PM UTC, failing authorization checks. These failures triggered alerts at 6:44 PM UTC. Rolling back the authorization service change was delayed as parts of the deployment infrastructure relied on the authorization service and required manual intervention to complete. Rollback completed at 7:08 PM UTC and all impacte…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>