<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-03-09T00:20:38.288Z</id>
    <title>osmos::feed</title>
    <updated>2024-03-09T00:20:38.288Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with legacy custom integration configuration pages]]></title>
        <id>https://slack-status.com/2024-03/1f506e0a906e75bb</id>
        <link href="https://slack-status.com/2024-03/1f506e0a906e75bb"/>
        <updated>2024-03-08T14:35:09.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On March 6, 2024 from 7:45 PM PST to around 9:33 PM PST customers reported that they were receiving a 500 error when viewing the legacy Incoming Webhook custom integration page.

We identified a backend code change that inadvertently changed data that the Incoming Webhook page needed to load which consequently caused the page to respond with an HTTP 500 error.

We rolled back the change, correcting the issue and all customers should now be able to access the Incoming Webhook page as expected.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Huddles not working for some Windows users]]></title>
        <id>https://slack-status.com/2024-03/b9c8573e14eec256</id>
        <link href="https://slack-status.com/2024-03/b9c8573e14eec256"/>
        <updated>2024-03-08T10:54:49.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On March 5, 2024 from 6:05 PM PST to March 6, 2024 8:17 AM PST customers on the Windows operating systems reported that Huddles were freezing when screen sharing.
We determined that a recent change that was made to the video codec of Huddles, which may have been moving more of the video encoding to the computer's Graphic Processing Unit. This would cause performance problems & freezing during a screen share.
After narrowing down the exact cause we rolled back the change which restored full functionality to Huddles screen sharing.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/hjdlmbqk7krr</id>
        <link href="https://airbnbapi.statuspage.io/incidents/hjdlmbqk7krr"/>
        <updated>2024-03-07T22:40:08.000Z</updated>
        <summary type="html"><![CDATA[Mar  7, 14:40 PST
Resolved - This incident has been resolved as of March 6th, 2024, 12:08 AM PST. Error rates have returned to normal levels. Please retry any failed requests.
We apologize for the inconvenience caused and thank you for your patience and understanding.
Mar  6, 12:38 PST
Monitoring - We've identified an increased number of 500 errors across multiple endpoints starting today 11:52 AM PST.This issue has been mitigated at 12:08 AM PST. 
Please apply any failed requests during the incident.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: **Summary:**
Admin Console is experiencing elevated errors with the Chrome setting pages.
**Description:**
Mitigation work is currently underway by our engineering team.
The mitigation is expected to complete by Wednesday, 2024-03-06 16:15 US/Pacific.
We will provide more information by Wednesday, 2024-03-06 16:45 US/Pacific.
**Customer Symptoms:**
The impacted customers would encounter a HTTP 500 error while accessing user or device setting pages within Chrome Admin Console. Additionally, customers would not be able to view or change any policies.
**Workaround:**
None at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/Nr8avWyykjurdUqXYSDW</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/Nr8avWyykjurdUqXYSDW"/>
        <updated>2024-03-07T21:06:27.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2024-03-06 22:32</strong> and ended at <strong>2024-03-07 00:29</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 6 March 2023 14:32</p>
<p><strong>Incident End:</strong> 6 March 2023 16:29</p>
<p><strong>Duration:</strong> 1 hours, 57 minutes</p>
<p><strong>Affected Services and Features:</strong></p>
<ul>
<li>Admin Console Pages for Chrome user settings</li>
<li>Chrome device settings</li>
<li>Chrome devices</li>
</ul>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Google Workspace users attempting to work within the Admin Console encountered an HTTP 500 error when accessing user or device setting pages within the console. From preliminary analysis, the root cause of the issue is a change to the access control setting in a backend authentication service used by the Admin Console. This change resulted in users being unable to access the page.</p>
<p>Our Engineering team was alerted by our internal monitoring systems and swiftly mitigated the issue by rolling back the change to the access control settings.</p>
<p>This impacted users ability to view the current user and device policies, change or edit the existing policies and their ability to create new policies.</p>
<p>Google will complete a full Incident Report (IR) in the following days that will provide a detailed root cause.</p>
<p><strong>Customer Impact:</strong></p>
<ul>
<li>The impacted customers encountered a HTTP 500 error while accessing user or device setting pages within Chrome Admin Console.</li>
<li>Additionally, customers were unable to view or change any Chrome policies.</li>
</ul>
<hr>
</div><hr><p>Affected products: Admin Console</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delays creating accounts in third-party apps]]></title>
        <id>https://status.rippling.com/incidents/j0y55xs3d05q</id>
        <link href="https://status.rippling.com/incidents/j0y55xs3d05q"/>
        <updated>2024-03-07T18:09:18.000Z</updated>
        <summary type="html"><![CDATA[Mar  7, 18:09 UTC
Resolved - This incident has been resolved.
Mar  6, 22:57 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Mar  6, 16:58 UTC
Update - We are continuing to work on a fix for this issue. This affects users who have their accounts created prior to their start date.
Mar  6, 16:51 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Availability in SFO2]]></title>
        <id>https://status.digitalocean.com/incidents/1755wn45yqyw</id>
        <link href="https://status.digitalocean.com/incidents/1755wn45yqyw"/>
        <updated>2024-03-07T12:21:18.000Z</updated>
        <summary type="html"><![CDATA[Mar  7, 12:21 UTC
Resolved - Our Engineering team identified and resolved an issue that affected Spaces availability in the SFO2 region.
From 11:38 UTC to 11:58 UTC, users may have encountered errors while accessing Spaces objects and creating new buckets in the SFO2 region.
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in TOR1]]></title>
        <id>https://status.digitalocean.com/incidents/l9d26ggtbw8h</id>
        <link href="https://status.digitalocean.com/incidents/l9d26ggtbw8h"/>
        <updated>2024-03-06T19:09:05.000Z</updated>
        <summary type="html"><![CDATA[Mar  6, 19:09 UTC
Resolved - Our Engineering team has identified and resolved an issue in the TOR1 region that impacted the network connectivity for a subset of Droplets and Droplet-based services for a brief duration.
From 17:38 - 17:47 UTC, users might have experienced delays or errors while accessing and connecting to their resources in the TOR1 region from the public internet or from other resources in TOR1. Swift action was taken by our Engineering team that restored service and all services in TOR1 are operating correctly. 
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some customers may be unable to access the App Directory]]></title>
        <id>https://slack-status.com/2024-03/a20334cecc6fff7c</id>
        <link href="https://slack-status.com/2024-03/a20334cecc6fff7c"/>
        <updated>2024-03-06T07:23:50.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On March 5, 2024 from 3:36 PM PST until 9:36 PM PST some customers were unable to visit the app detail page in App Directory, instead seeing a page displaying a "Server Error" message.

We traced the issue to a recent backend code change and reverted the change, which fixed this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: 'Invite people' popup menu in Manage Members page populates name of wrong workspace.]]></title>
        <id>https://slack-status.com/2024-03/9c34d28b9b3f4e7b</id>
        <link href="https://slack-status.com/2024-03/9c34d28b9b3f4e7b"/>
        <updated>2024-03-06T05:00:19.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
From March 1, 2024 at 12:13 PM PST to March 4, 2024 at 7:06 PM PST, Enterprise Grid admins and owners may have experienced issues inviting members to workspaces. 

We made a code change that inadvertently introduced a logic error, resulting in the incorrect workspaces being listed in the invite modal. We reverted the code change, restoring normal invite functionality for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Databases Control Plane and Connectivity]]></title>
        <id>https://status.digitalocean.com/incidents/s5v9qwsvlxjv</id>
        <link href="https://status.digitalocean.com/incidents/s5v9qwsvlxjv"/>
        <updated>2024-03-05T23:41:12.000Z</updated>
        <summary type="html"><![CDATA[Mar  5, 23:41 UTC
Resolved - Our Engineering team has confirmed that this incident has been fully resolved.
If you continue to experience any issues with Managed Database Clusters please open a ticket with our support team. Thank you for your patience.
Mar  5, 20:28 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with our Managed Databases services. At this time, we're observing error rates returning to pre-incident levels and seeing operations such as create/fork/restore succeed. Trusted sources updates are also functioning normally, so connectivity to Database clusters from newly added resources to trusted sources is restored. 
We are monitoring the situation closely and will post an update as soon as we confirm the issue is fully resolved.
Mar  5, 17:39 â€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Availability Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/5qkdf7vy8v6v</id>
        <link href="https://airbnbapi.statuspage.io/incidents/5qkdf7vy8v6v"/>
        <updated>2024-03-05T21:22:24.000Z</updated>
        <summary type="html"><![CDATA[Mar  5, 13:22 PST
Resolved - This incident has been resolved.
Mar  4, 04:41 PST
Monitoring - We've mitigated the issue at 4:10AM PDT and will continue to monitor it.
Mar  4, 02:49 PST
Investigating - We are investigating an increased number of 500 errors across multiple availability related endpoints. These errors started on March 01, 2024.
Our engineering teams are actively working towards a resolution to restore service fully.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BLR1 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/gqwgnn1x1m4z</id>
        <link href="https://status.digitalocean.com/incidents/gqwgnn1x1m4z"/>
        <updated>2024-03-05T15:00:56.000Z</updated>
        <summary type="html"><![CDATA[Mar  5, 15:00 UTC
Completed - The scheduled maintenance has been completed.
Mar  5, 14:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Mar  5, 11:30 UTC
Scheduled - Start: 2024-03-05 14:00 UTC
End:  2024-03-05 15:00 UTC
During the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration. This maintenance was previously attempted on 2024-02-19 but the changes were reverted after our Engineers encountered unexpected issues, resulting from the maintenance. Our team has performed a thorough examination of the previous attempt and is confident in performing this maintenance, as well as measures to mitigate any negative outcomes. 
Expected impact:
As part of this maintenance, event processing in BLR1 will be delayed for a period of up to 15 minutes during the one-hour window. During this period, users will experience a delay with creating, destroying, or modifying new or existing DO services in BLR1(such as Droplets, DBaaS/DOKS clusters, etc.), existing services that are running should not be impacted.
If you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some customers are unable to share their screen during a huddle]]></title>
        <id>https://slack-status.com/2024-03/629a8aa472f5f9c2</id>
        <link href="https://slack-status.com/2024-03/629a8aa472f5f9c2"/>
        <updated>2024-03-05T04:32:04.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On March 4, 2024, from 4:18 PM PST until 10:17 PM PST some users experienced an issue when sharing their screen during a huddle. Attempting to share their screen would fail without any error or warning.

An incorrect piece of code was identified as the cause of the screen share issue. We safely reverted the problematic code, resolving the issue that users experienced. To capture the fix, a hard refresh (by pressing Cmd/Ctrl + Shift + R) may be required.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/47y20gmnl3h9</id>
        <link href="https://airbnbapi.statuspage.io/incidents/47y20gmnl3h9"/>
        <updated>2024-03-05T00:16:15.000Z</updated>
        <summary type="html"><![CDATA[Mar  4, 16:16 PST
Resolved - This incident has been resolved.
Error rates have returned to normal levels. Please retry any failed requests.
We apologize for the inconvenience caused and thank you for your patience and understanding.
Mar  4, 14:10 PST
Monitoring - The issue has been mitigated at 1:45 PM PDT. Please apply any failed requests during the incident.
Mar  4, 13:24 PST
Investigating - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started today (March 4, 2024) around 12:57 PM PDT. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue with reservation related webhooks]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/5vnndg8lvchr</id>
        <link href="https://airbnbapi.statuspage.io/incidents/5vnndg8lvchr"/>
        <updated>2024-03-04T22:25:36.000Z</updated>
        <summary type="html"><![CDATA[Mar  4, 14:25 PST
Resolved - This incident has been resolved. Don't hesitate to contact us if you need further assistance.
Mar  4, 11:17 PST
Monitoring - We recently experienced an incident that affected all reservation-related webhooks between 9:54 AM and 10:44 AM PT on March 4, 2024. During this time, you may have noticed reservations being requested, confirmed, altered, and canceled on your listings, but no corresponding webhooks were received. The issue has been resolved, and we will be sending all the missing webhooks in the upcoming hours. In the meantime, if you need to retrieve reservation information for your listings, you can use the GET reservations API. Please note that this incident only affected reservation-related webhooks.
We apologize for the inconvenience, and please don't hesitate to contact us if you need further assistance.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help website is currently unreachable]]></title>
        <id>https://status.make.com/incidents/1vm1k0hxtbdb</id>
        <link href="https://status.make.com/incidents/1vm1k0hxtbdb"/>
        <updated>2024-03-04T10:12:14.000Z</updated>
        <summary type="html"><![CDATA[Mar  4, 11:12 CET
Resolved - We are still in contact with our third-party provider to gather more information and prevent this issue from happening again. Meanwhile, the workaround that was implemented has been working as expected. The incident has been resolved.
Mar  1, 17:44 CET
Monitoring - We have successfully implemented a workaround, and the content of the affected pages is now accessible. However, we are still awaiting a response from the third-party service and further insight into the issue. Currently, the entire webpage is operational. We will continue to monitor the situation closely until we receive additional information from the third party.
Mar  1, 16:51 CET
Update - We are still investigating the issue, and our initial suspicion is that it may be related to one of our third-party services. We anticipate providing the next update within the next 2 hours, or as soon as more information becomes available.
Mar  1, 16:02 CET
Investigating - Our Help and API documentation websites are currently unreachable due to unknown reasons. We are currently investigating the root cause. The rest of the platform's functionalities are unaffected. We will provide an update in the next 30 minutes. Sorry for the inconvenience.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests, Copilot, Git Operations, Actions and Pages]]></title>
        <id>https://www.githubstatus.com/incidents/7x5z8plb48t6</id>
        <link href="https://www.githubstatus.com/incidents/7x5z8plb48t6"/>
        <updated>2024-03-01T17:42:41.000Z</updated>
        <summary type="html"><![CDATA[Mar  1, 17:42 UTC
Resolved - This incident has been resolved.
Mar  1, 17:42 UTC
Update - Git Operations is operating normally.
Mar  1, 17:41 UTC
Update - Actions and Pages are operating normally.
Mar  1, 17:36 UTC
Update - Copilot is operating normally.
Mar  1, 17:34 UTC
Update - Pages is experiencing degraded performance. We are continuing to investigate.
Mar  1, 17:34 UTC
Update - One of our clusters is experiencing problems, and we are working on restoring the cluster at this time.
Mar  1, 17:30 UTC
Investigating - We are investigating reports of degraded performance for API Requests, Copilot, Git Operations and Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests, Actions and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/wcl1sw4mzg60</id>
        <link href="https://www.githubstatus.com/incidents/wcl1sw4mzg60"/>
        <updated>2024-03-01T16:12:23.000Z</updated>
        <summary type="html"><![CDATA[Mar  1, 16:12 UTC
Resolved - On March 1, 2024, between 14:17 UTC and 15:54 UTC the service that sends messages from our event stream into our background job processing service was degraded and delayed the transmission of jobs for processing.  No data or jobs were lost.  From 14:17 to 14:41 UTC, there was a partial degradation, where customers would experience intermittent delays with PRs and Actions.  From 14:41 to 15:24 UTC, 36% of PRs users saw stale data, and 100% of in progress Actions workflows did not see updates , even though the workflows were succeeding.  At 15:24 UTC, we mitigated the incident by redeploying our service and jobs began to burn down, with full job catchup by 15:54 UTC. This was due to under provisioned memory and lack of memory based back pressure in the service, which overwhelmed consumers and led to OutOfMemory crashes.
We have adjusted memory configurations to prevent this problem, and are analyzing and adjusting our alert sensitivity to reduce our time to detection of issues like this one in the future.

Mar  1, 16:12 UTC
Update - Issues, Pull Requests and Actions are operating normally.
Mar  1, 15:48 UTC
Update - We're seeing our background job queue sizes trend down, and expect full recovery in the next 15 minutes.
Mar  1, 15:39 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Mar  1, 15:27 UTC
Update - We're continuing to investigate issues with background jobs that have impacted Actions and Pull Requests. We have a mitigation in place and are monitoring for recovery.
Mar  1, 14:51 UTC
Update - We're investigating issues with background jobs that are causing sporadic delays in pull request synchronization and reduced Actions throughput.
Mar  1, 14:39 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests and Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New improved payment description format]]></title>
        <id>287075</id>
        <link href="https://changelog.bookingsync.com/new-improved-payment-description-format-287075"/>
        <updated>2024-03-01T12:58:56.000Z</updated>
        <summary type="html"><![CDATA[Improvement
Â Â 
We've enhancing the payment descriptions that are displayed in your Stripe or BookingPay account on your transactions, to make them clearer and more useful for you, based on your valuable feedback.
Here's What's Changing:
The description will now include the booking reference:
Old Format: "Payment for Beautiful Example Rental from Monday 27 May 2024, 04:00 PM to Sunday 02 Jun 2024, 10:00 AM"
â€‹


New Format: "Payment for booking XYZ123 at Beautiful Example Rental from 27 May 2024 to 02 Jun 2024"


Why? To streamline tracking and management of your bookings, making reconciliation easier.
Thank you for being a valuable member of the Smily community. We're thrilled to keep enhancing your property management journey!]]></summary>
        <author>
            <name>Megan, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users unable to create or edit workflows]]></title>
        <id>https://slack-status.com/2024-02/35134eda119581f5</id>
        <link href="https://slack-status.com/2024-02/35134eda119581f5"/>
        <updated>2024-03-01T05:48:33.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 29, 2024 from 1:28 PM PST to around 4:36 PM PST, users were unable to create or update some workflows.

We determined that a recent code change had inadvertently introduced a logic problem, resulting in errors for workflows that used event triggers. We reverted this code change. However, rolling back reintroduced a separate issue that we'd resolved earlier in the day. 

We prepared a tailored revert to roll back the code causing the workflow problem while also including the fix for the previous issue. This second revert resolved both problems, restoring full functionality to Slack. 

Thank you for being patient with us.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Workflows are unable to be viewed or triggered]]></title>
        <id>https://slack-status.com/2024-02/939919184ea7655b</id>
        <link href="https://slack-status.com/2024-02/939919184ea7655b"/>
        <updated>2024-02-29T20:47:52.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 29, 2024 at 8:00 AM PST to 9:55 AM PST, users saw errors when attempting to trigger and manage legacy workflows.

Upon investigation, we discovered a code change prevented part of the infrastructure that supports workflows from functioning correctly.

To resolve the issue, we reverted the code change and all workflows were restored.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/5lc3f39mjcq8</id>
        <link href="https://www.githubstatus.com/incidents/5lc3f39mjcq8"/>
        <updated>2024-02-29T12:27:17.000Z</updated>
        <summary type="html"><![CDATA[Feb 29, 12:27 UTC
Resolved - On February 29, 2024, between 9:32 and 11:54 UTC, queuing in our background job service caused processing delays to Webhooks, Actions, and Issues. Nearly 95% of delays occurred between 11:05 and 11:27 UTC, with 5% during the remainder of the incident. During this incident, the following customer impacts occurred: 50% of webhooks experienced delays of up to 5m, 1% of webhooks experienced delays of 17m at peak; Actions: on average, 7% of customers experienced delays, with a peak of 44%; and many Issues saw a delay in appearing in searches. At 9:32 UTC our automated failover successfully routed traffic to a secondary cluster. But an improper restoration to primary at 10:32 UTC caused a significant increase in queued jobs until 11:21 UTC, when a correction was made and healthy services began burning down the backlog until full resolution.
We have made improvements to the automation and reliability of our fallback process to prevent recurrence. We also have larger work already in progress to improve the overall reliability of our job processing platform.

Feb 29, 12:21 UTC
Update - We're seeing recovery and are going to take time to verify that all systems are back in a working state.
Feb 29, 12:19 UTC
Update - Issues is operating normally.
Feb 29, 12:18 UTC
Update - Webhooks is operating normally.
Feb 29, 11:05 UTC
Update - We're continuing to investigate delayed background jobs. We've seen partial recovery for Issues, and there is ongoing impact to actions, notifications and webhooks.
Feb 29, 10:58 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Feb 29, 10:36 UTC
Update - We're seeing issues related to background jobs, which are causing delays for webhook delivery and search indexing, and other updates.
Feb 29, 10:33 UTC
Investigating - We are investigating reports of degraded performance for Issues and Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Recovery Image]]></title>
        <id>https://status.digitalocean.com/incidents/dj2p47c4zss1</id>
        <link href="https://status.digitalocean.com/incidents/dj2p47c4zss1"/>
        <updated>2024-02-29T05:50:18.000Z</updated>
        <summary type="html"><![CDATA[Feb 29, 05:50 UTC
Resolved - Our Engineering team identified and resolved an issue that was affecting the booting of Droplets from the Recovery ISO.
From 00:20 UTC to 05:24 UTC, users might have experienced errors when attempting to boot Droplets from the Recovery ISO.
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CreateMessage API - WRITE downtime]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/qfy422ht70s1</id>
        <link href="https://airbnbapi.statuspage.io/incidents/qfy422ht70s1"/>
        <updated>2024-02-29T01:05:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 17:05 PST
Completed - The scheduled maintenance has been completed.
Feb 28, 17:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 22, 23:23 PST
Scheduled - Please note upcoming maintenance occurring on Wednesday, 2/28/2024 at 5:00 PM PT, that will result in ~30-60 second WRITE downtime to CreateMessage API. Reads will not be affected.
Alerts and errors pertaining to Messaging on Wednesday, 2/28/2024 around 5:00 PM PT, are to be expected. Please consider any corrective actions needed to mitigate potential data loss during this ~30-60 second period.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Databases creation - PostgresSQL v16]]></title>
        <id>https://status.digitalocean.com/incidents/cd70bby9qynm</id>
        <link href="https://status.digitalocean.com/incidents/cd70bby9qynm"/>
        <updated>2024-02-28T22:53:47.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 22:53 UTC
Resolved - Our Engineering team has confirmed that creation, forking, and restoration of PostgreSQL clusters on v16 is functioning correctly.
Upgrades for lower-versioned PostgreSQL clusters to v16 remain unavailable at this time and users will see errors if they attempt to perform that upgrade. Our Engineering team continues to work on making upgrades to v16 available again, but we expect this to take some time.
If you continue to experience issues or have any questions, please open a ticket with our support team.
Feb 28, 20:56 UTC
Monitoring - After testing, teams have determined that PostgreSQL v16 is safe for new creations, as well as forks and restores for existing clusters. At this time, v16 is re-enabled in our Cloud Control Panel and users creating, forking, or reâ€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SFO Networking]]></title>
        <id>https://status.digitalocean.com/incidents/g4sdvwy4z4zm</id>
        <link href="https://status.digitalocean.com/incidents/g4sdvwy4z4zm"/>
        <updated>2024-02-28T22:49:41.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 22:49 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with networking in our SFO2 region. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!
Feb 28, 22:23 UTC
Monitoring - Our Engineering team has confirmed that the faulty network hardware component was the cause of this issue. From 21:39 - 22:11 UTC, this component was not functioning correctly, causing networking issues for a subset of customers in our SFO2 region, as well as internal alerts in our SFO1/SFO3 regions. 
At this time, all services should now be operating normally. We will monitor this incident for a short period of time to confirm full resolution.
Feb 28, 22:17 UTC
Identified - Our Engineering team has identified the cause of the issue with networking in our SFO regions to be related to an issue with a network hardware component in SFO2. They have isolated that component and we're observing error rates returning to pre-incident levels at this time. 
We are continuing to look into this failure, but users should be seeing recovery on their services. We'll provide another update soon.
Feb 28, 22:06 UTC
Investigating - Our Engineering team is currently investigating internal alerts and customer reports for an increase in networking errors in our SFO regions for Droplets and Droplet-based services. We will provide an update as soon as we have further information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlock your rental's potential: Expert Vrbo Strategies, Smily Innovations & Upcomming Events! ðŸ—ï¸]]></title>
        <id>286877</id>
        <link href="https://changelog.bookingsync.com/unlock-your-rental's-potential-expert-vrbo-strategies-smily-innovations-upcomming-events!-286877"/>
        <updated>2024-02-28T10:21:46.000Z</updated>
        <summary type="html"><![CDATA[Communications
Â Â 
In todayâ€™s Smily newsletter, we directly address Vrbo performance optimization, spotlighting essential optimization tips shared by our Partner Vrbo. We also introduce our latest releases, designed to streamline your operations, and preview upcoming events where Smily will be featured. This edition delivers focused insights and recent developments to strengthen your property management strategies. ðŸ™Œ
ðŸ”Optimization tips: Enhance your listings on Vrbo
Our partner Vrbo, has generously provided a selection of their top optimization practices to elevate your overall listing's performance. Below are the main highlights, along with guidance on implementing these strategies through your Smily account:
Optimize listing content - What makes a Vrbo listing attractive ðŸ’™
Key amenitieâ€¦]]></summary>
        <author>
            <name>Ella, Chief Customer Officer (CCO) &amp; Cofounder</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble signing in on mobile for some Owners and Admins]]></title>
        <id>https://slack-status.com/2024-02/2ea2fda2874858b7</id>
        <link href="https://slack-status.com/2024-02/2ea2fda2874858b7"/>
        <updated>2024-02-28T05:55:13.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 
On February 27, 2024 from around 8:30 PM PST to 10:06 PM PST Workspace Owners and Admins were experiencing issues while setting up two-factor authentication during sign in on iOS devices.

The issue was caused by a two-factor authentication feature deploy to users on the Pro subscription. After identifying the root cause the feature was was rolled back and this allowed all users to log back into Slack.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS3 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/7rzlkzs0ksqs</id>
        <link href="https://status.digitalocean.com/incidents/7rzlkzs0ksqs"/>
        <updated>2024-02-27T20:00:57.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 20:00 UTC
Completed - The scheduled maintenance has been completed.
Feb 27, 16:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 20, 15:19 UTC
Scheduled - Start: 2024-02-27 16:00 UTC
End:  2024-02-27  20:00 UTC

During the above window, our Networking team will be making changes to core networking infrastructure, to improve performance and scalability in the AMS3 region. 
Expected impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience increased latency or a brief disruption in network traffic. We will endeavor to keep any such impact to a minimum.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Close button for right pane not appearing for some users]]></title>
        <id>https://slack-status.com/2024-02/cedce965afd47ebb</id>
        <link href="https://slack-status.com/2024-02/cedce965afd47ebb"/>
        <updated>2024-02-27T15:41:56.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 26, 2024, from 9:21 AM PST to 9:16 AM PST on February 27, some users encountered the close button disappearing in the right pane where threads and profiles are viewed.

We discovered this was caused by a missing check in the change workspace process that prevented the right pane elements from rendering properly. Once the cause was identified, a change was made to the logic for drawing Slack's interface when switching workspaces. Once the change was rolled out, the issue was resolved.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users are not able to create workflows]]></title>
        <id>https://slack-status.com/2024-02/b1389c8cd318a7d1</id>
        <link href="https://slack-status.com/2024-02/b1389c8cd318a7d1"/>
        <updated>2024-02-27T02:50:48.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
From 12:30 PM PST to 1:05 PM PST on February 26, 2024, users were unable to create workflows.

We determined that the related API method needed updating in order to eliminate inconsistencies that were causing elevated replication lag.

We deployed an update to the API method correcting this issue, and all workflow creation will now work as expected.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues sending and loading messages]]></title>
        <id>https://slack-status.com/2024-02/4c0af99013428a7a</id>
        <link href="https://slack-status.com/2024-02/4c0af99013428a7a"/>
        <updated>2024-02-27T01:47:26.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 26, 2024 from 9:00 AM PST to 9:36 AM PST, some users experienced problems sending and loading messages in Slack. Others may have completed these actions successfully but more slowly than expected.

Earlier today, we deployed a routine update to part of our network infrastructure. During this work, we deprovisioned some of our proxy servers and provisioned a new set. Some of our webapp servers took longer than expected to sync with the new list of proxy servers, resulting in a spike in connection errors. This coincided with the regular 9:00 AM PST increase in traffic to our servers, which may have exacerbated impact.

By around 9:36 AM PST, error rates had decreased and user impact had completely subsided, most likely due to our automatic scaling functionality. However, we're continuing to analyze data from this event to fully understand the root cause and prevent similar issues in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues registering an account]]></title>
        <id>https://status.rippling.com/incidents/5pv7fqwgvtck</id>
        <link href="https://status.rippling.com/incidents/5pv7fqwgvtck"/>
        <updated>2024-02-27T01:21:53.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 01:21 UTC
Resolved - This incident has been resolved.
Feb 26, 23:24 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Feb 26, 22:06 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Problem with executing scenarios]]></title>
        <id>https://status.make.com/incidents/smcl14qd1qxw</id>
        <link href="https://status.make.com/incidents/smcl14qd1qxw"/>
        <updated>2024-02-26T23:06:42.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 00:06 CET
Resolved - This incident has been resolved.
Feb 26, 21:58 CET
Monitoring - A fix has been implemented and we are monitoring the results.
Feb 26, 21:34 CET
Investigating - We're encountering issues executing scenarios on eu2.make.com. Users may experience delays and errors when attempting to execute any scenarios. There are no other issues with scenario creation or general connectivity to Make. We're currently investigating this issue and will update this Statuspage within the next hour, or as more information comes to hand.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/f9ntcnd3fdvs</id>
        <link href="https://www.githubstatus.com/incidents/f9ntcnd3fdvs"/>
        <updated>2024-02-26T21:40:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 21:40 UTC
Resolved - On Monday, February 26th, from 20:45 UTC to 21:39 UTC, GitHub Packages reported an outage indicating a degradation in GitHub Container Registry and NPM package upload functionality. Upon investigation, we found a misconfigured observability metric which inadvertently pulled in data from a newly provisioned test environment. All failures being reported were traced back to this test environment. We confirmed that there was no real customer impact to GitHub Packages during this incident. We have since reconfigured our observability metrics to accurately report based on environment.
Feb 26, 21:20 UTC
Update - We are seeing some recovery in NPM and GitHub Container Registry functionality, but are maintaining red status until we are certain issues wonâ€™t recur.
Feb 26, 21:03 UTC
Update - NPM and GitHub Container Registry services are degraded, particularly the upload functionality. Investigation is underway.
Feb 26, 21:01 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks, Actions, Pull Requests and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/78qz8zwhx9tj</id>
        <link href="https://www.githubstatus.com/incidents/78qz8zwhx9tj"/>
        <updated>2024-02-26T19:37:32.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 19:37 UTC
Resolved - On February 26, 2024, between 18:34 UTC and 19:37 UTC our background job service was degraded and caused job start delays up to 15 minutes. Users experienced delays in Webhooks, Actions, and some UI updates (e.g. a delay in UI updates on pull requests). This was due to capacity problems with our job queueing service, and a failure of our automated failover system.
We mitigated the incident by manually failing over to our secondary cluster.   No data was lost - recovery began at 18:55 UTC, when the backlog of enqueued jobs began to process.
We are actively working to repair our failover automation and expand the capacity of our background job queuing service to prevent issues like this in the future.

Feb 26, 19:37 UTC
Update - Actions and Pull Requests are operating normally.
Feb 26, 19:37 UTC
Update - Webhooks and Issues are operating normally.
Feb 26, 19:05 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:57 UTC
Update - Pull Requests is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:55 UTC
Update - We have deployed a fix for issues affecting Webhooks, Actions, and some other services. We are beginning to see recovery and will continue to monitor and fix as needed.
Feb 26, 18:55 UTC
Update - Webhooks is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:48 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:47 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with the billing history page]]></title>
        <id>https://slack-status.com/2024-02/b4d41d5daac6431c</id>
        <link href="https://slack-status.com/2024-02/b4d41d5daac6431c"/>
        <updated>2024-02-26T14:30:45.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From 2:58 PM PST on February 22, 2024, until 9:47 PM PST on February 25, 2024, some users were unable to access their workspace billing history.

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users.

Thank you very much for your patience while we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manage your VRBO guest review from Smily interface]]></title>
        <id>286647</id>
        <link href="https://changelog.bookingsync.com/manage-your-vrbo-guest-review-from-smily-interface-286647"/>
        <updated>2024-02-26T10:54:27.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
Are you connected to VRBO OTA?
Starting today at 2pm UTC, we will be fetching guest reviews and ratings from VRBO, and you'll be able to respond to them directly from Smily interface :)
ðŸš§ Pain Points Addressed:
No need to visit VRBO to check and respond to guest reviews.
Previously, you couldn't rate a guest.
ðŸš€ Benefits for you:
Manage all your reviews in one place.
Easily read and respond to VRBO guest reviews from our Review section on the Smily interface.
Rate guests with ease.
â„¹ï¸ Information and next steps:
We will fetch all your past reviews.


You will be able to rate all past bookings matching those criteria:
--> Bookings without guest review that ended in the last 365 days;
--> Bookings for which a guest review is already present and 14 days havenâ€™t passed since the guest made the review;


Make use of this new feature and ensure to leave reviews for guests. This is an important criterion for VRBO and will impact your property ranking.


Lastly, you can use our "automated reviews" feature, released at the end of last year, for VRBO reviews too.


Have a great review day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/6hnyx0v9zjbh</id>
        <link href="https://status.make.com/incidents/6hnyx0v9zjbh"/>
        <updated>2024-02-26T07:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 08:00 CET
Completed - The scheduled maintenance has been completed.
Feb 26, 06:00 CET
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 15, 16:40 CET
Scheduled - Hello,
Please note there is scheduled infrastructure maintenance on 26.02.2024, between 6:00-8:00 AM CET, when you can expect a slower scenario processing. Specifically, scenarios using a datastore module might see reduced performance which can cause its longer execution.
We will inform you once the maintenance is completed.
Thank you very much for understanding.
Kind regards,
Make team]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some template duplication may fail]]></title>
        <id>https://status.notion.so/incidents/gnzy4hx09dq6</id>
        <link href="https://status.notion.so/incidents/gnzy4hx09dq6"/>
        <updated>2024-02-24T00:52:27.000Z</updated>
        <summary type="html"><![CDATA[Feb 23, 16:52 PST
Resolved - Our engineering team identified the issue and released a fix to resolve it.
Feb 23, 16:42 PST
Investigating - Customers may experience an error when duplicating some templates.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Threads aren't loading for some users]]></title>
        <id>https://slack-status.com/2024-02/548b72776056c03b</id>
        <link href="https://slack-status.com/2024-02/548b72776056c03b"/>
        <updated>2024-02-23T03:05:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
From 2:54 PM to 2:59 PM PDT on February 22, 2024, some users were having issues loading Threads.

We carried out a thorough investigation and observed trends, but found no evidence of an issue on our side. By around 2:59 PM PDT, error rates had already subsided and we received confirmation from several users that they could load Threads again by reloading Slack.

Whilst no remedial action was taken on our end, we're analyzing data from this event to understand ways to mitigate similar issues that may occur in the future.

We're confident that there will be no further impact to users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Screen readers are not announcing unread messages when using the Ctrl + K command on Windows.]]></title>
        <id>https://slack-status.com/2024-02/83f4c18fc4d11c93</id>
        <link href="https://slack-status.com/2024-02/83f4c18fc4d11c93"/>
        <updated>2024-02-23T01:46:26.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 21, 2024 from 6:30 PM PST to 8:00 PM PST, customers using the JAWS or NVDA screen readers on Windows may have experienced issues with announcements for unread messages in the Quick Switcher (Ctrl + K). 

A recent code change inadvertently introduced a conflict with an HTML label for an interactive element.

We reverted the code change, fixing the issue and restoring normal announcement behaviour.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with password resets]]></title>
        <id>https://slack-status.com/2024-02/e04276624660caf9</id>
        <link href="https://slack-status.com/2024-02/e04276624660caf9"/>
        <updated>2024-02-22T14:32:08.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 

Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are unable to upload, download, and view files in Slack.]]></title>
        <id>https://slack-status.com/2024-01/f39851209d6c471a</id>
        <link href="https://slack-status.com/2024-01/f39851209d6c471a"/>
        <updated>2024-02-22T14:31:13.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 

Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry Latency in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/n9211k9rmml7</id>
        <link href="https://status.digitalocean.com/incidents/n9211k9rmml7"/>
        <updated>2024-02-21T20:43:48.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 20:43 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting the Container Registry in multiple regions. 
Everything involving the Container Registry should now be functioning normally. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Feb 21, 18:03 UTC
Monitoring - Our Engineering team has identified an internal operation within the Container Registry service which was placing load on the service, leading to latency and errors. The team has paused that operation in order to resolve the issue impacting the Container Registry in multiple regions. Users should not be facing any latency issues while interacting with their Container registries and also while building their Apps. 
We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. 
Thank you for your patience and we apologize for the inconvenience.
Feb 21, 15:41 UTC
Investigating - Our Engineering team is investigating an issue with the DigitalOcean Container Registry service. Beginning around 20:00 UTC on February 20, there has been an uptick in 401 errors for image pulls from the Container Registry service.
During this time, a subset of customers may experience latency or see 401 errors while interacting with Container Registries. This issue also impacts App Platform builds and users may encounter delays while building their Apps or experience timeout errors in builds as a result. Users utilizing Container Registry for images for deployment to Managed Kubernetes clusters may also see latency or failures to deploy.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/wn6s1w8vkk1y</id>
        <link href="https://www.githubstatus.com/incidents/wn6s1w8vkk1y"/>
        <updated>2024-02-21T17:30:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 17:30 UTC
Resolved - On Wednesday February 21, 2024, 17:07 UTC, we deployed a configuration change to one of our services inside of Actions. At 17:14 UTC we noticed an increase in exceptions that impacted approximately 85% of runs at that time. 
At 17:18 UTC, we reverted the deployment and our service immediately recovered. During this timeframe, customers may have noticed their workflows failed to trigger or workflows were queued but did not progress.
To prevent this issue in the future we are improving our deployment observability tooling to detect errors earlier in the deployment pipeline.
Feb 21, 17:20 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Availability in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/rccs80k3p2pb</id>
        <link href="https://status.digitalocean.com/incidents/rccs80k3p2pb"/>
        <updated>2024-02-20T07:07:10.000Z</updated>
        <summary type="html"><![CDATA[Feb 20, 07:07 UTC
Resolved - As of 06:25 am UTC, our Engineering team has confirmed the resolution of the issue impacting Spaces availability in the BLR1 region.
Users should no longer experience issues with their Spaces resources in the BLR1 region.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Feb 20, 06:39 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the Spaces availability issues in the BLR1 region and is monitoring the situation. 
Users should no longer encounter errors when accessing Spaces in the BLR1 region and should be able to create new Spaces buckets from the cloud control panel. 
We will post an update as soon as the issue is fully resolved.
Feb 20, 05:46 UTC
Investigating - Our Engineering team is investigating an issue with Spaces availability in the BLR1 region. During this time users may encounter errors when accessing Spaces objects and creating new buckets in the BLR1 region. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BLR1 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/5z0npmmmnc1h</id>
        <link href="https://status.digitalocean.com/incidents/5z0npmmmnc1h"/>
        <updated>2024-02-19T17:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 17:00 UTC
Completed - The scheduled maintenance has been completed.
Feb 19, 14:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 16, 14:52 UTC
Scheduled - Start: 2024-02-19 14:00 UTC
End:Â  2024-02-19 17:00 UTC
Hello,
During the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration.
Expected impact:
As part of this maintenance, event processing in BLR1 will be disabled for a period of up to 15 minutes during the three-hour window. During this period, users won't be able to create, destroy, or modify new or existing DO services in BLR1 (such as Droplets, DBaaS/DOKS clusters, etc.).
If you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket

Thank you,
Team DigitalOcean]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Products Impacted in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/q2sl05rc7mgp</id>
        <link href="https://status.digitalocean.com/incidents/q2sl05rc7mgp"/>
        <updated>2024-02-19T15:50:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 15:50 UTC
Resolved - From 15:50 - 16:46, our team received customer reports of issues impacting multiple products in our BLR1 region, including the accessibility of Managed Databases and Managed Kubernetes clusters and general network connectivity disruption. These issues may be related to a scheduled maintenance event in the region, per our status post linked below:
https://status.digitalocean.com/incidents/5z0npmmmnc1h
Our team continues to review customer reports and diagnose the impact related to this maintenance. In the meantime, we have rolled back the maintenance process and all services should now be responding normally. If you experience any further issues, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/yp30nqsv310k</id>
        <link href="https://status.digitalocean.com/incidents/yp30nqsv310k"/>
        <updated>2024-02-18T19:08:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 18, 19:08 UTC
Resolved - As of 17:47 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Managed Kubernetes service in our NYC3 region. The Cilium pods inside the clusters should be functioning normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb 18, 18:09 UTC
Monitoring - Our Engineering team has deployed the fix for the issue with Managed Kubernetes service where users were experiencing network connectivity issues with Cilium pods being restarted inside the clusters. Cilium pods should now be functioning normally. 
We are monitoring the situation and will post another update once we confirm the fix resolves this incident.
Feb 18, 16:57 UTC
Investigating - Our Engineering team is investigating an issue with our Managed Kubernetes service in NYC3 region. 
During this time users may experience network connectivity issues specifically with the Cilium pods inside their clusters.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Reports]]></title>
        <id>https://status.rippling.com/incidents/p5wbyw5s471p</id>
        <link href="https://status.rippling.com/incidents/p5wbyw5s471p"/>
        <updated>2024-02-15T22:38:53.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 22:38 UTC
Resolved - This issue has been resolved. Customers should no longer be impacted by this issue.
Feb 15, 19:50 UTC
Identified - We are aware of issues with Reports loading. We have identified a fix and are working to roll it out.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Errors updating Notion subscription]]></title>
        <id>https://status.notion.so/incidents/qpyywl1mc14q</id>
        <link href="https://status.notion.so/incidents/qpyywl1mc14q"/>
        <updated>2024-02-15T17:30:36.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 09:30 PST
Resolved - On February 15th 2023 between 01:39 PST - 08:43 PST, Notion was experiencing an issue with subscription updates, which may have resulted in errors when changing billing plans.
Our engineering team identified the issue and increased capacity to this service to prevent errors, and this is now working as normal. We appreciate your patience and will work to prevent similar errors in future.
Feb 15, 08:52 PST
Investigating - Customers may experience an error when attempting to change their Notion subscription.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Profile photos aren't displaying properly]]></title>
        <id>https://slack-status.com/2024-02/16aecfd53af7219b</id>
        <link href="https://slack-status.com/2024-02/16aecfd53af7219b"/>
        <updated>2024-02-15T14:40:11.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

Starting around 12:46 PM PST on February 6, 2024, until around 4:06 PM PST on February 9, 2024, some users experienced issues with their profile pictures loading, resulting in the default avatar image displaying instead of their custom image.

We identified a bug that caused missing image data from a database cache. We fixed the bug and reset the backend cache to pull new data, which initially fixed the issue around 4:00 PM PST on February 6. However, some users began experiencing the problem again once their Slack client refreshed.

We continued investigating the issue, tracing it back to a piece of code that would push older image data to a database that housed profile information, and promptly reverted the incorrect code which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are now supporting and managing Booking.com' Sponsored Benefit program at Smily]]></title>
        <id>285929</id>
        <link href="https://changelog.bookingsync.com/we-are-now-supporting-and-managing-booking-com'-sponsored-benefit-program-at-smily-285929"/>
        <updated>2024-02-15T13:30:51.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
We are thrilled to share with you some great news :)
We are now supporting and managing Booking.comÂ Sponsored Benefit program at Smily;
Benefits for you:
You were facing the obstacle whereÂ booking.comÂ **was paying a small amount of the booking.com reservation due to a discount offered to the guest.
1/ it was not blocking the synchronisation of your listing but was creating an issue for the payment;
2/ you had to send manually a payment linkÂ to collect the remaining amount to the guests. This wonâ€™t be necessary anymore!


Now, as we do support the Sponsored benefit program, you will still see and receive this small amount in advanceÂ paid byÂ Booking.comÂ and we will process with success the rest of the payment!


You can now join this program if you were waiting for this functionality ðŸ™‚
How does this program benefit your business?
When applied, it offers the following advantages for your business:
Boosts bookings from price-conscious customers
Lowers cancellations with customers paying in advance
Increases your revenue by covering the reduced amount to drive more bookings
Take the next step:
Take a look at what this program is about andÂ check out if you are eligible to enable itÂ and enjoy it;
Check our new manual
Your business deserves the best, and with Smilyâ€™s latest feature, weâ€™re here to ensure you get just that.]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbnb - enrich your listing thanks to the listing expectation types]]></title>
        <id>285927</id>
        <link href="https://changelog.bookingsync.com/airbnb-enrich-your-listing-thanks-to-the-listing-expectation-types-285927"/>
        <updated>2024-02-15T13:24:14.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
The Airbnb Listing expectation type feature is now available on Smily. 
This update lets you enrich your listings with additional information, directly from Smily's user-friendly interface.
Pain Points Addressed:
Gone are the days of toggling between platforms. Previously, you had to navigate Airbnb's website to update listing details, a time-consuming and inefficient process. Smily's new feature streamlines this, offering a seamless integration within the interface.
ðŸš€ Benefits for you:
Easily accessible under the "amenities" section in the Smily interface, the new "Property Info" category empowers you to select and customize expectation types, with an optional description of up to 100 characters.
This information is then flawlessly synced with Airbnb for those using full synchronization, ensuring a cohesive and accurate listing.
It will also be synchronised on your website if you have one with Smily!
Such information is key and valuable for the guests to avoid any negative surprises on check-in day!
â†’ Here is where to find this new section on the Smily interfaceðŸ‘‡
-> Then click on the pen to edit/add an optional description;

â„¹ï¸ Next steps and important information
For those using this via Airbnb, we've already fetched your existing listing expectations from Airbnb as of February 9th.


To ensure data alignment with this new feature, please update any changes made since then before our official release on February 21st via the Smily interface;
â†’ On February 21st, we will start the synchronisation of your data from Smily to the Airbnb application. Any values not reflected on the Smily interface will be removed from Airbnb.


Please note, that the Smily interface allows a description limit of 100 characters compared to Airbnb's 300. Should this affect your listings, we'll reach out individually.
â†’ if you cannot update your description before February 21st, let us know please;


Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble with canvas.]]></title>
        <id>https://slack-status.com/2024-02/804d3a52e234f2fa</id>
        <link href="https://slack-status.com/2024-02/804d3a52e234f2fa"/>
        <updated>2024-02-13T22:15:24.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 12, 2024 from 6:00 AM PST to 8:32 AM PST, some users experienced latency when loading canvases or errors related to editing and saving canvases.

Upon investigation we determined that this was caused by rate limits put in place by an upstream provider. We restarted the impacted endpoints to mitigate the issue immediately, which resolved the issue for all affected users. We're continuing to partner with our upstream provider to reduce the likelihood of this occurring in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion app experienced a brief outage due to a deployment that didn't function as expected]]></title>
        <id>https://status.notion.so/incidents/2b5f3nmpht76</id>
        <link href="https://status.notion.so/incidents/2b5f3nmpht76"/>
        <updated>2024-02-12T23:34:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 15:34 PST
Resolved - This incident has been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/k0qjh7s64fxk</id>
        <link href="https://www.githubstatus.com/incidents/k0qjh7s64fxk"/>
        <updated>2024-02-12T18:14:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 18:14 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to inciâ€¦]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/vv6z7v5w80yr</id>
        <link href="https://www.githubstatus.com/incidents/vv6z7v5w80yr"/>
        <updated>2024-02-12T12:39:20.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 12:39 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to incidents like this in the future.
Feb 12, 12:29 UTC
Update - We are starting to see recovery based on the signals that the team have been monitoring, following mitigation steps being taken. When confident that recovery is complete, we will resolve this incident.
Feb 12, 12:00 UTC
Update - We are continuing to investigate increased failure rates for Copilot code completion for some users in Europe.
Feb 12, 11:38 UTC
Update - We are investigating reports that GitHub Copilot code completions are not working for some users in Europe.
Feb 12, 11:38 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You can now publish your listings on HomeToGo! ðŸŽ‰]]></title>
        <id>285614</id>
        <link href="https://changelog.bookingsync.com/you-can-now-publish-your-listings-on-hometogo!-285614"/>
        <updated>2024-02-12T11:32:13.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 


We are excited to announce our new partnership with a popular booking platform among travelers: HomeToGo!
  

ðŸ’¡What is HomeToGo?
HomeToGo is a German group founded in 2014 that has become a leader in the vacation rental industry. 
Thanks to its exponential growth, HomeToGo is now an essential booking platform for property managers worldwide. ðŸŒ



ðŸ’¡ Why pushing your listings on HomeToGo?

Quality bookings: 7 days of average Length of Stay and a superior average transaction value.
Access to a broad audience: Particularly from the Germany-Switzerland-Austria region, along with other Northern European countries, guests with high purchasing power.
Flexibility: Hosts can apply their own cancellation policies and terms and conditions.
Direct communication channels with guests: Both during and post-stay.
Enhanced visibility: Through HomeToGo's extensive network of brands.
 
ðŸ‘‰ Install the HomeToGo app here
 
ðŸ’Œ Learn more on how to connect your properties in our dedicated manual page.
  
If you have any questions, please don't hesitate to contact channel.manager@hometogo.com]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/my7r3l9lsqnk</id>
        <link href="https://www.githubstatus.com/incidents/my7r3l9lsqnk"/>
        <updated>2024-02-09T11:28:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  9, 11:28 UTC
Resolved - On February 9, 2024 between 10:34 UTC and 11:24 UTC, the Webhooks service was degraded and 63% of webhooks were delayed by up to 16 minutes with an average delay of 5 minutes. No webhook deliveries were lost. This was due to an issue with an overloaded backend data store that was unable to process network requests fast enough.
We mitigated the incident by manually failing over traffic to healthy hosts.
We are expanding the capacity of the backing store as well as making the Webhooks service more resilient to this kind of issue. 

Feb  9, 11:25 UTC
Update - Webhooks is operating normally.
Feb  9, 11:11 UTC
Update - We are investigating latency in processing webhooks. Customers may see a delay of around 5 minutes at this time. We will continue to keep users updated on progress towards mitigation.
Feb  9, 11:09 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage on eu2.make.com]]></title>
        <id>https://status.make.com/incidents/ldktvddm239d</id>
        <link href="https://status.make.com/incidents/ldktvddm239d"/>
        <updated>2024-02-08T16:03:02.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 17:03 CET
Resolved - This incident has been resolved.
Feb  8, 14:41 CET
Update - We are continuing to monitor for any further issues.
Feb  8, 14:39 CET
Monitoring - We have identified and resolved the issue associated with the recent code change. Stability has been restored to eu2.make.com since 14:20 CET. We will continue to monitor the situation for the next several hours.
Feb  8, 14:21 CET
Investigating - We are currently experiencing issues with the eu2.make.com zone. Login to the webpage is unreachable, and scenario executions are affected. We are actively investigating the issue and will provide an update within the next 60 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unresponsive shared hooks]]></title>
        <id>https://status.make.com/incidents/wtq70l2b2g74</id>
        <link href="https://status.make.com/incidents/wtq70l2b2g74"/>
        <updated>2024-02-08T12:52:07.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 13:52 CET
Resolved - This incident has been resolved.
Feb  8, 10:04 CET
Update - The fix has been successfully rolled out. Webhooks and mailhooks should now be fully functional for eu1.make.com. We will continue to monitor the situation for the next few hours.
Feb  8, 09:33 CET
Update - While monitoring the issue, we observed certain problems with mailhooks and webhooks. We have identified the issue and are working on rolling out the fix. The issue pertains only to eu1.make.com, the remaining zones are functional.
Feb  8, 01:19 CET
Monitoring - A fix has been implemented and we are monitoring the current behavior.
Feb  8, 00:45 CET
Investigating - We are currently experiencing issues with some of our services responsible for shared hooks on our clusters.
Shared hooks might currently be unresponsive. We are actively investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
</feed>