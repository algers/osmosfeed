<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-02-08T00:21:06.840Z</id>
    <title>osmos::feed</title>
    <updated>2024-02-08T00:21:06.840Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Czech Republic Voice Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/h0w6vjlkbp3g</id>
        <link href="https://status.twilio.com/incidents/h0w6vjlkbp3g"/>
        <updated>2024-02-08T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 7, 18:00 - 20:30 PST
Jan 24, 07:40 PST
Scheduled - Our Voice carrier partner in Czech Republic is conducting a planned maintenance from 07 February 2024 at 18:00 PST until 07 February 2024 at 20:30 PST. During the maintenance window, there could be call failures to and from a subset of Czech Republic phone numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Russia SMS Carrier Maintenance - Motiv]]></title>
        <id>https://status.twilio.com/incidents/b5m2sklz48tn</id>
        <link href="https://status.twilio.com/incidents/b5m2sklz48tn"/>
        <updated>2024-02-08T01:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 7, 17:00 - 18:00 PST
Feb  6, 06:06 PST
Scheduled - The Motiv network in Russia is conducting a planned maintenance from 07 February 2024 at 17:00 PST until 07 February 2024 at 18:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to Motiv Russia handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unresponsive shared hooks]]></title>
        <id>https://status.make.com/incidents/wtq70l2b2g74</id>
        <link href="https://status.make.com/incidents/wtq70l2b2g74"/>
        <updated>2024-02-08T00:19:53.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 01:19 CET
Monitoring - A fix has been implemented and we are monitoring the current behavior.
Feb  8, 00:45 CET
Investigating - We are currently experiencing issues with some of our services responsible for shared hooks on our clusters.
Shared hooks might currently be unresponsive. We are actively investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be having trouble connecting to Slack.]]></title>
        <id>https://status.slack.com//2024-02/558e3bb8ce654659</id>
        <link href="https://status.slack.com//2024-02/558e3bb8ce654659"/>
        <updated>2024-02-07T21:14:24.000Z</updated>
        <summary type="html"><![CDATA[Users should no longer be experiencing any trouble connecting to Slack. We apologize for the disruption and thank you for your patience.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Resize Events]]></title>
        <id>https://status.digitalocean.com/incidents/w1sspyd728kg</id>
        <link href="https://status.digitalocean.com/incidents/w1sspyd728kg"/>
        <updated>2024-02-07T20:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 20:54 UTC
Resolved - As of 19:37 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Droplet resize events in all regions. All the Droplet resize events should now be succeeding normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb  7, 19:41 UTC
Monitoring - Our Engineering team has fully deployed the fix for the issue with Droplet resizes and is now monitoring the situation. Users can now retry Droplet resizes and should see them succeed.
We'll post another update once we confirm the fix resolves this incident.
Feb  7, 15:49 UTC
Identified - Our Engineering team has identified the root cause of the issue with failed Droplet resizes and a fix is in the process of being deployed. 
Users attempting to resize Droplets where the image for the Droplet has been deleted or retired (e.g. a user created a Droplet from a Snapshot, but later deleted that Snapshot) will see failures. All other resizes are succeeding normally.
We'll post another update once the fix has completed deployment.
Feb  7, 15:32 UTC
Investigating - Our Engineering team is investigating an uptick in failed Droplet resizes, beginning Feb 6, 20:57 UTC. 
During this time, some users may experience failures when attempting to resize Droplets, in all regions. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent Errors When Accessing Twilio Documentation]]></title>
        <id>https://status.twilio.com/incidents/sw0snlpssh56</id>
        <link href="https://status.twilio.com/incidents/sw0snlpssh56"/>
        <updated>2024-02-07T18:12:09.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 10:12 PST
Resolved - 503 errors have been resolved and access to twilio.com/docs is working normally at this time.
Feb  7, 09:34 PST
Monitoring - Access to twilio.com/docs is now working normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.
Feb  7, 09:13 PST
Identified - Our engineers have identified the issue causing intermittent 503 errors when accessing twilio.com/docs and are working to deploy a fix. We expect to provide another update in 2 hours or as soon as more information becomes available.
Feb  7, 08:56 PST
Update - We are continuing to investigate intermittent 503 errors when accessing twilio.com/docs. We expect to provide another update in 2 hour or as soon as more information becomes available.
Feb  7, 07:50 PST
Update - We are investigating intermittent 503 errors when accessing twilio.com/docs. We expect to provide another update in 1 hour or as soon as more information becomes available.
Feb  7, 07:40 PST
Investigating - We are investigating intermittent 503 errors when accessing twilio.com/docs. We expect to provide another update in 30 minutes or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Async Request Processing]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz"/>
        <updated>2024-02-07T17:48:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 09:48 PST
Resolved - This incident has been resolved.
Feb  7, 08:55 PST
Monitoring - Our Async Request Processing system had an issue this morning, which resulted in increased delays between the time a request was enqueued and when it was processed, and may have resulted in an elevated processing failure rate. This began around 8:10 AM PST, and was resolved by 9:15 AM PST.
We are still actively monitoring, but are not expecting any ongoing issues at this time.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unable to launch Analyze and Dashboard View on Twilio Flex]]></title>
        <id>https://status.twilio.com/incidents/ztpmrrwtgpss</id>
        <link href="https://status.twilio.com/incidents/ztpmrrwtgpss"/>
        <updated>2024-02-07T16:37:27.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 08:37 PST
Resolved - Issue affecting the Analyze and Dashboard view from Flex Insights has been resolved and the service is operating normally at this time.
Feb  7, 08:05 PST
Monitoring - Analyze and Dashboard view from Flex Insights is now operating normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.
Feb  7, 04:00 PST
Identified - Our engineers have pinpointed the issue affecting the launch of Analyze and Dashboard View, and they are currently working with their vendor to fix it. We anticipate providing another update within 4 hours or as soon as additional information becomes available.
Feb  7, 02:00 PST
Update - We are investigating a service interruption with Analyze and Dashboard View on Twilio Flex. We expect to provide another update in 2 hours or as soon as more information becomes available.
Feb  7, 01:00 PST
Investigating - We are seeing issues with Twilio Flex where subset of customers are Unable to launch Analyze and Dashboard View on Twilio Flex.Our engineers are working to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indonesia Account Security Carrier Partner Maintenance - XL Axiata]]></title>
        <id>https://status.twilio.com/incidents/nrxh559lj8b4</id>
        <link href="https://status.twilio.com/incidents/nrxh559lj8b4"/>
        <updated>2024-02-07T15:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 07:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  5, 03:24 PST
Scheduled - Our carrier partner XL Axiata is conducting a planned maintenance from 07 February 2024 at 07:00 PST until 07 February 2024 at 19:00 PST. During the maintenance window, there could be intermittent API request failures for XL Axiata customers.

Impacted Products: Verify Silent Network Auth.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Failures to US Proximus Mobility Network from Shortcode Sender IDs.]]></title>
        <id>https://status.twilio.com/incidents/rt47t7bs9cl2</id>
        <link href="https://status.twilio.com/incidents/rt47t7bs9cl2"/>
        <updated>2024-02-07T12:06:31.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 04:06 PST
Resolved - We are no longer experiencing SMS delivery failures to US Proximus Mobility Network from Shortcode Sender IDs. This incident has been resolved.
Feb  7, 02:24 PST
Monitoring - We are observing successful SMS delivery to US Proximus Mobility Network from Shortcode Sender IDs.  We will continue to monitor to ensure full service recovery. We expect to provide another update in 2 hours or as soon as more information becomes available.
Feb  7, 01:32 PST
Investigating - We are experiencing SMS delivery Failures to US Proximus Mobility Network from Shortcode Sender IDs. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in NYC Regions]]></title>
        <id>https://status.digitalocean.com/incidents/y2cm9wm1bvv7</id>
        <link href="https://status.digitalocean.com/incidents/y2cm9wm1bvv7"/>
        <updated>2024-02-07T07:21:30.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 07:21 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting network latency in our NYC regions.
The issues were a direct result of traffic congestion from our upstream providers, which has been repaired. Users should no longer experience packet loss or increased latency while interacting with their resources in the NYC regions.
We sincerely apologize and thank you for your patience as we worked through this issue. In case of any questions or concerns, please open a ticket with our Support team.
Feb  7, 04:03 UTC
Investigating - Our Engineering team is investigating multiple reports of network latency when connecting to services in our NYC regions. During this time, users may experience intermittent packet loss or increased latency while interacting with their resources in the NYC regions.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chile SMS Carrier Maintenance - Entel]]></title>
        <id>https://status.twilio.com/incidents/qmjy9clvmfjl</id>
        <link href="https://status.twilio.com/incidents/qmjy9clvmfjl"/>
        <updated>2024-02-07T04:30:56.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 20:30 PST
Completed - The scheduled maintenance has been completed.
Feb  6, 18:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  6, 07:29 PST
Scheduled - The Entel network in Chile is conducting an emergency maintenance from 06 February 2024 at 18:00 PST until 06 February 2024 at 20:30 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from Entel Chile handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US, Canada and Puerto Rico Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/0cr7rsqhjp21</id>
        <link href="https://status.twilio.com/incidents/0cr7rsqhjp21"/>
        <updated>2024-02-07T04:00:57.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 20:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jan  4, 06:00 PST
Scheduled - Our SMS and MMS carrier partner in US, Canada and Puerto Rico is conducting a planned maintenance from 06 February 2024 at 20:00 PST until 08 February 2024 at 03:00 PST. During the maintenance window, there could be intermittent delays delivering SMS and MMS to and from US, Canada and Puerto Rico handsets via subset of Shortcodes and Longcodes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Tigo Network in Guatemala]]></title>
        <id>https://status.twilio.com/incidents/2s0qjvvkxpm8</id>
        <link href="https://status.twilio.com/incidents/2s0qjvvkxpm8"/>
        <updated>2024-02-07T00:54:35.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 16:54 PST
Resolved - We are no longer experiencing SMS delivery delays to Tigo network in Guatemala. This incident is resolved.
Feb  6, 14:58 PST
Monitoring - We are observing improvement with SMS delivery delays to Tigo network in Guatemala. Our engineers are continuing to monitor the situation. We will provide another update in 2 hours or as soon as more information becomes available.
Feb  6, 00:24 PST
Update - We continue to experience SMS delivery delays to Tigo network in Guatemala. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 24 hours or as soon as more information becomes available.
Feb  5, 08:58 PST
Update - We continue to experience SMS delivery delays to Tigo network in Guatemala. Our engineers are working with…]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/1jwy0vy3hjfb</id>
        <link href="https://status.digitalocean.com/incidents/1jwy0vy3hjfb"/>
        <updated>2024-02-06T22:21:17.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 22:21 UTC
Completed - The scheduled maintenance has been completed.
Feb  6, 17:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  2, 20:23 UTC
Scheduled - Start Time: 17:00 UTC Feb 6, 2024
End Time: 00:00 UTC Feb 7, 2024
During the above time, our Engineering Team will be performing maintenance to failover some internal databases from one cluster to another.
Extensive testing has been conducted to ensure this maintenance will be successful and result in minimal impact to DigitalOcean users. The actual failover is estimated to take less than 3 seconds.
Existing infrastructure, including Droplets and Droplet-based services, should continue running without issue. There is no network disruption to existing services expected as part of this maintenance. However, there are dependencies on multiple services. During the failover, there may be customer impacts that should be brief and transitory. The following actions may experience increased latency or failure rates during the maintenance period:
- API calls to the DigitalOcean public API 
- Events for Droplets and Droplet-based services such as create, delete, power on/off, resize, etc 
- Control operations through the DigitalOcean Cloud Control Panel 
Multiple teams will be engaged to keep downtime to a minimum and mitigate any impact that does occur. We’ll post updates here for any unexpected changes to this scheduled maintenance, as well as progress updates during the maintenance itself.
If you have any questions or concerns, please reach out to the Support team from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automations failing]]></title>
        <id>https://status.notion.so/incidents/2wtslqc6bv54</id>
        <link href="https://status.notion.so/incidents/2wtslqc6bv54"/>
        <updated>2024-02-06T17:58:31.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 09:58 PST
Resolved - This incident has been resolved.
Feb  6, 09:16 PST
Update - We are working hard to resolve the issue for you. Thank you for your continuous patience.
Feb  6, 07:00 PST
Update - We are still working on fixing the issue and appreciate your patience.
Feb  6, 05:05 PST
Identified - We are experiencing an issue with the Notion's database automations service that cause automation actions to fail or experience delays. Our engineers have identified this & are working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A number of South Korean users inadvertently banned from using vital APIs in Notion, potentially affecting some functionalities within the app.]]></title>
        <id>https://status.notion.so/incidents/f88y9tn65kl1</id>
        <link href="https://status.notion.so/incidents/f88y9tn65kl1"/>
        <updated>2024-02-06T00:32:44.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:32 PST
Resolved - This incident has been resolved.
Feb  5, 16:03 PST
Update - We are continuing to investigate this issue.
Feb  5, 16:02 PST
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/dmybzf132ygz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/dmybzf132ygz"/>
        <updated>2024-02-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:00 PST
Resolved - We've identified spikes in errors across multiple endpoints that have occurred over the past day. The two distinct spikes were:
- February 5th, 4:10 PM to 4:30 PM (PST)
- February 6th, 6:30 AM to 7:30 AM (PST)
The issue is now resolved, and we don't expect any ongoing impact.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automation Failures]]></title>
        <id>https://status.notion.so/incidents/w7ppqptykrdz</id>
        <link href="https://status.notion.so/incidents/w7ppqptykrdz"/>
        <updated>2024-02-05T14:56:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 06:56 PST
Resolved - Between 3:00 UTC - 14:22 UTC, some users may have experienced database automation failures or delays in actions being triggered.
This is now resolved, and the affected automations have completed. Database automation services are now running as normal. 
Thank you for your patience while we worked through this issue.
Feb  5, 05:30 PST
Monitoring - Notion's database automation service began experiencing problems at approximately 3 AM UTC today, which caused a number of automation actions to fail or experience delays. 
Our engineers have identified a fix, and are now retrying automations that failed to trigger during this period. 
We will continue to monitor the situation and share an update when this is fully resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/3k40h28fkb2r</id>
        <link href="https://www.githubstatus.com/incidents/3k40h28fkb2r"/>
        <updated>2024-02-05T09:53:13.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 09:53 UTC
Resolved - On 2024-02-05, from 09:26 to 13:20 UTC some GitHub customers experienced errors when trying to download raw files. An overloaded server exposed a bug, causing us to return HTTP 500 error codes.
The issue was mitigated by disabling the server and re-routing traffic. We are implementing improvements to our routing logic to more quickly avoid troublesome hosts in the future. 

Feb  5, 09:40 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/1q1jhbr85zvv</id>
        <link href="https://status.digitalocean.com/incidents/1q1jhbr85zvv"/>
        <updated>2024-02-05T05:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 05:54 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting Spaces CDN in our SGP1 region.
From 03:02 UTC - 05:15 UTC, users were experiencing errors for objects served over the CDN.
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.
Feb  5, 05:10 UTC
Monitoring - Our Engineering team has applied a fix to mitigate the issue related to the Spaces CDN in the SGP1 region. Users should no longer experience errors for objects served over the CDN. 
We apologize for the inconvenience and will post another update once we're confident that the issue is fully resolved.
Feb  5, 04:52 UTC
Identified - From 03:02 UTC, our Engineering team has identified an issue with the Spaces CDN in our SGP1 region and is actively working on a fix. During this time, users may experience errors for objects served over the CDN. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joining or creating a workspace from sidebar is broken]]></title>
        <id>https://status.notion.so/incidents/q9j3jvg634lx</id>
        <link href="https://status.notion.so/incidents/q9j3jvg634lx"/>
        <updated>2024-02-02T22:45:22.000Z</updated>
        <summary type="html"><![CDATA[Feb  2, 14:45 PST
Resolved - As of 2:44 PM PT, this incident has been resolved.
Feb  2, 11:52 PST
Monitoring - As of 11:44 AM PT, a fix has been implemented and we are monitoring the results.
Feb  2, 11:42 PST
Update - We are continuing to investigate this issue.
Feb  2, 11:42 PST
Update - Users cannot join and create a workspace from their sidebar. In addition, users may experience increased latency across search, viewing and editing content, and syncing content.
We are actively investigating these issues and will follow up here with an update.
Feb  2, 11:36 PST
Investigating - Currently, the "Join or create a workspace" button from the workspace sidebar is broken.
We are actively investigating the issue and will follow up here with an update.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users in Germany having trouble receiving 2FA SMS codes]]></title>
        <id>https://status.slack.com//2024-02/30dfe547672802a4</id>
        <link href="https://status.slack.com//2024-02/30dfe547672802a4"/>
        <updated>2024-02-01T14:43:18.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On February 1, 2024 between 12:57 AM PST and 3:52 AM PST, some users in Germany were having trouble receiving two-factor authentication codes via SMS.


This was caused by an issue with a service provider and has now been resolved.


Users in Germany should no longer be having trouble and may also choose to use an authentication app to receive their 2FA codes instead of SMS.


Thank you for being patient with us, we appreciate it.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/7k5wl5j4f1t4</id>
        <link href="https://www.githubstatus.com/incidents/7k5wl5j4f1t4"/>
        <updated>2024-02-01T04:41:36.000Z</updated>
        <summary type="html"><![CDATA[Feb  1, 04:41 UTC
Resolved - An update to our design system caused issues loading dynamic content in the global side navigation menu and in other page-specific sidebar navigation elements. Impacted users saw continuous loading spinners in place of dynamic menu content. User impact lasted from 0:55 UTC to 4:41 UTC on February 1st.
We are working on a number of improvements in response to this incident. We are adding request volume monitors to sidebar navigation endpoints and making changes to our front end escalation paths to improve our time to detect and time to recovery for incidents of this nature. We have also begun work to improve both automated and manual testing for these types of changes in order to prevent recurrence.
Feb  1, 04:41 UTC
Update - This issue has been resolved. A reload of your browser window/tab may be required if you continue to experience issues with the collapsable navigation sidebars not loading.
Feb  1, 04:21 UTC
Update - We are in the process of deploying a remediation, and expect to see restoration of impacted functionality within the next hour.
Feb  1, 03:55 UTC
Update - We have identified an issue that is preventing some navigation components from loading while browsing GitHub.com, and are testing a remediation prior to deployment.
Feb  1, 03:14 UTC
Update - We are currently investigating reports of some components of the GitHub.com website not loading for some users.
Feb  1, 03:13 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Search API endpoint is down]]></title>
        <id>https://status.notion.so/incidents/t93zz4ynglj2</id>
        <link href="https://status.notion.so/incidents/t93zz4ynglj2"/>
        <updated>2024-01-31T23:01:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:01 PST
Resolved - This incident has been resolved.
Jan 31, 15:00 PST
Update - We are continuing to work on a fix for this issue.
Jan 31, 13:41 PST
Identified - We identified the root cause and are preparing a hotfix.
Jan 31, 11:05 PST
Update - We are continuing to investigate this issue.
Jan 31, 11:05 PST
Investigating - As of 12:35 AM PT the /search endpoint (https://api.notion.com/v1/search) has been down. 
We are currently investigating the issue and will share an update once the issue has been identified.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues loading threads and sending mesages.]]></title>
        <id>https://status.slack.com//2024-01/8e9a7cb95549d34c</id>
        <link href="https://status.slack.com//2024-01/8e9a7cb95549d34c"/>
        <updated>2024-01-31T18:14:04.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On January 31, 2024 between 7:43 AM PST and 8:27 AM PST, some users were unable to load threads and send messages.


We traced the issue to a backend failure and immediately implemented a change which fixed the issue for all affected users.


We apologize for any disruption to your work day and appreciate your patience while we resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customer Support Ticket Portal]]></title>
        <id>https://status.digitalocean.com/incidents/swc8grzsb33n</id>
        <link href="https://status.digitalocean.com/incidents/swc8grzsb33n"/>
        <updated>2024-01-31T15:26:00.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:26 UTC
Resolved - Our team has confirmed the full resolution for the problem with our support portal at https://cloudsupport.digitalocean.com/s/ where customers were unable to create tickets with 'Billing' ticket type. 
We sincerely apologize and thank you for your patience as we worked through this issue. 
In case of any questions or concerns, please open a ticket with our Support team.
Jan 31, 15:12 UTC
Monitoring - Our Engineering team has identified the cause of the issue and implemented a fix to resolve the problem with the Support Portal. Users should now be able to create the tickets in the Support portal with Billing ticket type. 
We are monitoring the situation now and will post an update as soon as the issue is fully resolved.
Jan 31, 14:26 UTC
Investigating - Our Engineering team is investigating an issue with customers being unable to create the support tickets to our support portal for Ticket type "Billing" at https://cloudsupport.digitalocean.com. 
As a temporary workaround, users may still contact us via the form here: https://www.digitalocean.com/company/contact/support
We apologize for the inconvenience and will post an update as soon as further information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/5y8b8lsqbbyq</id>
        <link href="https://www.githubstatus.com/incidents/5y8b8lsqbbyq"/>
        <updated>2024-01-31T14:57:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 14:57 UTC
Resolved - This incident was the result of an infrastructure change that was made to our load balancers to prepare us for IPv6 enablement of GitHub.com. This change was deployed to a subset of our global edge sites.
The change had the unintended consequence of causing IPv4 addresses to start being passed as an IPv4-mapped IPv6-compatible address to our IP Allow List functionality.
For example 10.1.2.3 became ::ffff:10.1.2.3. While our IP Allow List functionality was developed with IPv6 in mind, it wasn't developed to handle these mapped addresses, and hence started blocking requests as it deemed these to be not in the defined list of allowed addresses. Request error rates peaked at 0.23% of all requests.
We have so far identified three remediation items here:
- Update the IP Allow List functionality to handle IPv4-mapped addresses.
- Audit the rest of our stack to confirm there are no further places this IPv4-mapped IPv6 addresses flaw exists.
- Improve our testing and monitoring processes to better catch these issues in the future.
Jan 31, 14:56 UTC
Update - We have resolved the issue and confirmed all regions are now operating as expected.
Jan 31, 14:49 UTC
Update - The fix for ip allow lists is currently rolling out; and we are awaiting confirmation from specific geographic regions.
Jan 31, 14:33 UTC
Update - We are rolling out a fix to resolve the issues with IP allow lists. This should be resolved shortly.
Jan 31, 14:14 UTC
Update - Some customers are experiencing issues with IP allow lists.
Jan 31, 14:14 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some latency with Slack]]></title>
        <id>https://status.slack.com//2024-01/0a000b0ad09623a0</id>
        <link href="https://status.slack.com//2024-01/0a000b0ad09623a0"/>
        <updated>2024-01-30T17:58:00.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:37 AM PST to 11:48 AM PST on January 24, 2024, we experienced an unexpected amount of API calls to our servers that occurred within a short window of time. The volume of API calls resulted in some users experiencing latency loading channels and general difficulties connecting to Slack. We investigated the impact with a broad lens and began to observe signs of recovery around 12:09 PM PST.


During this time, we cautiously observed our health metrics to ensure our servers were accurately recovering. As a result of our thorough monitoring, there was a delay before we could confirm the issue was fully resolved. Our teams have also put in measures to help reduce the likelihood of this occurring again in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots Page - Cloud Control Panel]]></title>
        <id>https://status.digitalocean.com/incidents/39r97dl3v2l0</id>
        <link href="https://status.digitalocean.com/incidents/39r97dl3v2l0"/>
        <updated>2024-01-30T16:42:28.000Z</updated>
        <summary type="html"><![CDATA[Jan 30, 16:42 UTC
Resolved - Our Engineering team identified and resolved an issue impacting the Snapshots page in our Cloud Control Panel. 
From 13:00 - 15:00 UTC, users attempting to navigate to https://cloud.digitalocean.com/images/snapshots (via Images -> Snapshots) were unable to access the page, and instead saw an error page returned. 
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Pasting via Cmd/Ctrl + V not working]]></title>
        <id>https://status.slack.com//2024-01/8ad463b92a084387</id>
        <link href="https://status.slack.com//2024-01/8ad463b92a084387"/>
        <updated>2024-01-30T03:44:32.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

From 11:45 AM PST to around 5:20 PM PST on January 29, 2024, some customers experienced problems using keyboard shortcuts to paste text into Slack. 


A code change inadvertently introduced an issue that prevented the use of Cmd/Ctrl + V to paste text into Slack. We reverted this change, then deployed a fix to fully resolve the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2024-002]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2024-002</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2024-002"/>
        <updated>2024-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: We resolved an information disclosure vulnerability in the
hello.ts.net service.
What happened?
On January 15 2024, we became aware of a potential information disclosure
vulnerability in the hello.ts.net service, which could show the identity of a
different Tailscale user when loaded. The hello.ts.net service receives
identity information and public keys of nodes tied to their IP address. On
November 28 2023, we made a change to how IPs are assigned to
Tailscale nodes, making them globally non-unique. When the Tailscale service
assigned the same IP to multiple nodes, hello.ts.net would receive identity
information for one of the nodes at random. We confirmed on January 26 2024
that, if one of the other nodes with that IP loaded hello.ts.net, they would
see another user's name, email, and hostname.
The Tailscale Security Team immediately took hello.ts.net offline while the
fix was in progress. The issue has been fixed and the hello.ts.net service
was restored on January 29 2024.
Who was affected?
The incident was isolated to 10 users across 9 tailnets who could have had
their information leaked to other Tailscale users. We notified the tailnet
security contacts directly in accordance with our obligations under applicable
data privacy laws. Due to the random nature of the vulnerability, we cannot
confirm that all of those users were indeed affected.
Regular shared nodes always see unique node IPs and were not
vulnerable in a manner similar to hello.ts.net.
What was the impact?
A small number of users had their name, email, and hostname potentially exposed
to other Tailscale users that had nodes sharing the same IP.
In addition, the hello.ts.net service was offline between January 26-29
2024. Several users reported being negatively impacted by this.
What do I need to do?
No action is needed at this time.
If you have a dependency on hello.ts.net as a probing target for Tailscale
connectivity, consider using a different probing
mechanism.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNS Resolution in FRA1, AMS3 and LON1 Regions]]></title>
        <id>https://status.digitalocean.com/incidents/r9w0yrbyy9ls</id>
        <link href="https://status.digitalocean.com/incidents/r9w0yrbyy9ls"/>
        <updated>2024-01-29T18:36:45.000Z</updated>
        <summary type="html"><![CDATA[Jan 29, 18:36 UTC
Resolved - Our Engineering team has confirmed the workaround fix is successful and all services should now be operating normally. We will now close this incident and work with the DNS provider separately on the root cause. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Jan 29, 18:18 UTC
Monitoring - Our Engineering team has identified the root cause of the issue with DNS resolution. DigitalOcean resolvers in use in FRA1, AMS3, and LON1 are unable to reach an upstream DNS provider, resulting in resolution for a subset of domain names being unavailable from our resolvers. Our Engineering team is reaching out to the provider for assistance.
In the meantime, our Engineering team has been able to implement a workaround fix by filtering some incorrectly announced network routes. At this time, we are seeing recovery and resolution of hostnames returning to normal in the impacted regions. We'll continue to await an update from the DNS provider. We're now monitoring the workaround fix for stability and will post an update once we are confident it is successful.
Jan 29, 17:27 UTC
Investigating - Our Engineering team is currently investigating issues with DNS resolution in FRA1, AMS3, and LON1. During this time, customers may experience errors trying to resolve domain names from within DigitalOcean services in those regions, including Droplets and Droplet-based services, as well as App Platform. Additionally, App Platform builds may fail or experience delays. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/g6drnqm54qd4</id>
        <link href="https://www.githubstatus.com/incidents/g6drnqm54qd4"/>
        <updated>2024-01-28T14:42:55.000Z</updated>
        <summary type="html"><![CDATA[Jan 28, 14:42 UTC
Resolved - On January 28, 2024, between 01:00 UTC and 14:00 UTC the Avatars service was degraded and could not return all avatar images requested by users, instead it would return a default, fallback avatar image. This incident impacted, at peak time 6% of the requests for viewing Avatars. Requests that were impacted did not prevent the users from continuing to use any GitHub services. This was due to an issue with the Avatars service connecting to a database host.
 We mitigated the incident by restarting the malfunctioning hosts that were not able to return the user avatar images.
 We are working to improve alerting and monitoring of our services to reduce our time to detection and mitigation.
Jan 28, 14:27 UTC
Update - We have mitigated all customer impact. We are no longer serving fallback avatar icons when loading web pages for some customers. We continue to monitor the results.
Jan 28, 13:57 UTC
Update - A fix has been implemented for customers seeing the default avatar (octocat) when loading web pages and we are monitoring the results.
Jan 28, 13:20 UTC
Update - Some requests for getting the Avatars are returning the fallback response instead of the asked avatar since they are having issues connecting to the Mysql host
Jan 28, 13:20 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are unable to upload, download, and view files in Slack.]]></title>
        <id>https://status.slack.com//2024-01/f39851209d6c471a</id>
        <link href="https://status.slack.com//2024-01/f39851209d6c471a"/>
        <updated>2024-01-27T00:22:52.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 1:00pm PST on January 23, 2024 to 2:30pm PST on January 23, 2024, some users encountered trouble uploading, downloading, and viewing files in Slack.


We determined that an API call was not functioning correctly, and made a change to mitigate the issue. The upload problems were caused by a slowing down of responses due to a higher than normal volume of API calls due to the malfunction. This issue has been resolved and steps have been taken to avoid it happening in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Service wide connection issues]]></title>
        <id>https://status.slack.com//2024-01/8b623c0b5640c28f</id>
        <link href="https://status.slack.com//2024-01/8b623c0b5640c28f"/>
        <updated>2024-01-26T14:18:08.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On January 24, 2024 from 5:40 AM PST to around 9:00 AM PST, a small number of users experienced issues connecting to Slack and running workflows.


A change to routing in our servers resulted in requests failing due to a lack of available resources. This change was rolled back and Slack returned to a normal state.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots are failing in SFO3 and NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/2yxmsbx1r89b</id>
        <link href="https://status.digitalocean.com/incidents/2yxmsbx1r89b"/>
        <updated>2024-01-25T02:56:12.000Z</updated>
        <summary type="html"><![CDATA[Jan 25, 02:56 UTC
Resolved - Our Engineering team has resolved the issue with snapshots taken by customers in the NYC3 and SFO3 regions. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Jan 25, 01:50 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with snapshots taken by customers in the NYC3 and SFO3 regions and are monitoring the situation closely. 
We will post another update once we're confident that the issue is fully resolved.
Jan 25, 00:26 UTC
Identified - Our Engineering team has identified an issue with snapshots taken by customers in the NYC3 and SFO3 regions and is actively working on a fix. We will post an update as soon as additional information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes Cluster in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/s4kt15q6xq19</id>
        <link href="https://status.digitalocean.com/incidents/s4kt15q6xq19"/>
        <updated>2024-01-23T22:50:40.000Z</updated>
        <summary type="html"><![CDATA[Jan 23, 22:50 UTC
Resolved - Our Engineering team has completed mitigation efforts for the issue impacting Managed Kubernetes in the FRA1 region and we are marking this incident as Resolved. 
At this time, functionality to impacted clusters has been restored but customers may need to reconfigure some Kubernetes resources. Customer Support is contacting impacted customers directly with further instructions. 
If you have any questions or concerns regarding this incident, please open a ticket with our support team.
Jan 23, 18:44 UTC
Update - Our Engineering team continues to work on mitigation efforts. An additional small bug has been discovered and remediated. About 10% of clusters have had accessibility restored and restoration efforts are ongoing. 
We will post another update as soon as we have new developments.
Thank you for your patience and we apologize for any inconvenience.
Jan 23, 15:12 UTC
Identified - Our Engineering team has identified the cause of the issue with Managed Kubernetes clusters in the FRA1 region. 200 clusters are impacted by the issue and remain inaccessible to users at this time. 
Our Engineering team is engaged in remediating these clusters to restore accessibility. As soon as we are able to provide an estimated time to restore, we will provide an update.
Jan 23, 13:02 UTC
Investigating - As of 12:18 UTC, our Engineering team is investigating an issue with Kubernetes clusters in the FRA1 region. During this time, users may experience errors while communicating with their clusters in the FRA1 region. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/bsk6hmj0d7nv</id>
        <link href="https://www.githubstatus.com/incidents/bsk6hmj0d7nv"/>
        <updated>2024-01-23T18:53:29.000Z</updated>
        <summary type="html"><![CDATA[Jan 23, 18:53 UTC
Resolved - On January 23, 2024 at 14:36 UTC, our internal metrics began showing an increase in exceptions originating from our live update service. Live updates to Issues, PRs, Actions, and Projects were failing, but refreshing the page successfully updated page content. We resolved the issue by rolling back a problematic dependency update and reenabled live updates at 18:53 UTC. 
We are working to improve alerting and monitoring of our live update service to reduce our time to detection and mitigation.
Jan 23, 18:53 UTC
Update - Live updates have been restored and the system is operating normally.
Jan 23, 18:14 UTC
Update - We have identified and are beginning to roll out a potential fix for issues with live updates to our Web UI that power automatic page updates such as…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some Enterprise Grid users may be seeing unexpected Slackbot responses]]></title>
        <id>https://status.slack.com//2024-01/166eb312bd134031</id>
        <link href="https://status.slack.com//2024-01/166eb312bd134031"/>
        <updated>2024-01-23T03:06:24.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 12:20pm PST until 3:00pm PST on January 22, 2024, some Enterprise Grid users noticed multiple Slackbot responses being triggered unexpectedly.


We determined that the rollout of a fix for an older bug report pertaining to Slackbot responses not working in org-wide or multi-workspace channels, was the root cause. 


Whilst the fix for the bug was intended to improve this features behaviour, ensuring that Slackbot custom responses would work in these channel types, our wider team concluded that the fixed behaviour might not function well for organizations with potentially thousands of custom Slackbot responses.


We rolled back the deployment which caused this behaviour. Customers will no longer see these Slackbot custom responses being triggered unexpectedly.


A discussion is underway about the long-term future of Slackbot custom response behaviour within large organizations.


Thank you for your patience whilst we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network connectivity in LON1]]></title>
        <id>https://status.digitalocean.com/incidents/9y133zrfqngf</id>
        <link href="https://status.digitalocean.com/incidents/9y133zrfqngf"/>
        <updated>2024-01-22T18:38:44.000Z</updated>
        <summary type="html"><![CDATA[Jan 22, 18:38 UTC
Resolved - As of 18:37 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. 
Thank you for your patience and we apologize for any inconvenience.
Jan 22, 13:32 UTC
Monitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaas, Managed Kubernetes, and Managed Database. 
We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.
Jan 22, 12:25 UTC
Identified - Our Engineering team has identified the cause of the issue impacting networking in the LON1 region and is actively working on a fix.
During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaaS, Managed Kubernetes, and Managed Databases.
We will post an update as soon as additional information is available
Jan 22, 11:36 UTC
Investigating - Our Engineering team is investigating a networking issue in our LON1 region. At this time, you may experience packet loss or dropped connections.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smily’s enhanced deposit management: seamless synchronization with booking.com]]></title>
        <id>283940</id>
        <link href="https://changelog.bookingsync.com/smily-s-enhanced-deposit-management-seamless-synchronization-with-booking-com-283940"/>
        <updated>2024-01-22T17:40:37.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
Introducing Smily’s enhanced deposit management: seamless synchronization with booking.com
Say goodbye to data discrepancies and hello to effortless property management!
Key Features:
Easier deposit handling: Our system now allows you to manage properly damage deposits directly through Smily, ensuring perfect synchronization with Booking.com.
No more data mismatch: We’ve revolutionized our sync logic to eliminate discrepancies between Smily and Booking.com. This means the information you see on one platform is exactly what you’ll find on the other.
Enhanced user experience: Managing your properties has never been smoother. Our updates are tailored to make your workflow more intuitive and efficient.
Revamped collection & return methods: Say farewell to the hassle of cash deposits and wire returns. With our update, deposit collection and returns are streamlined through credit card transactions, simplifying the process for you and your guests.
Benefits for you:
Peace of mind: With matched data across platforms, reduce the risk of errors and enjoy a more streamlined management experience.
Time savings: Minimize manual work, allowing you to focus on growing your business and enhancing guest experiences.
Consistent information: Rest assured knowing that what you set in Smily is precisely reflected on Booking.com.
-> Update your knowledge with our revised manual. Get all the latest information and tips for maximizing the benefits of this new feature at Smily's updated manual.
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action required for continued Email communication from your own domain]]></title>
        <id>283898</id>
        <link href="https://changelog.bookingsync.com/action-required-for-continued-email-communication-from-your-own-domain-283898"/>
        <updated>2024-01-22T13:29:03.000Z</updated>
        <summary type="html"><![CDATA[Action required
  
We have an important update regarding your email communications with your guests via the notification app.
To maintain uninterrupted service, some adjustments are needed on your domain provider to keep sending notifications from your own domain name.
In case you are confident to do the changes yourself, please follow the instructions on our manual and let us know once it is done so we can verify, otherwise please contact our Customer Support team (Yannick or Pauline) to receive proper instructions and/or schedule a video meeting and we will be happy to help you.
Note: Please make sure you have access to your domain provider before the call.
Your prompt attention to this matter is appreciated to prevent any disruptions. Please note that the changes need to be done before February 13 2024.
Thank you for your cooperation.]]></summary>
        <author>
            <name>Yannick, Customer Care Team Leader - Pro Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces availability in SFO2]]></title>
        <id>https://status.digitalocean.com/incidents/299xh1cfs8s5</id>
        <link href="https://status.digitalocean.com/incidents/299xh1cfs8s5"/>
        <updated>2024-01-22T10:32:23.000Z</updated>
        <summary type="html"><![CDATA[Jan 22, 10:32 UTC
Resolved - Our Engineering team has resolved the issue impacting Spaces API availability in our SFO2 region. From approximately 09:00 UTC - 10:00 UTC, users may have experienced latency or timeouts when trying to access or manage their Spaces buckets. Spaces should now be operating normally.
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Jan 22, 10:10 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting SFO2 Spaces API availability and monitoring the situation. We will post an update as soon as the issue is fully resolved.
Jan 22, 09:59 UTC
Identified - As of 09:50 UTC, our Engineering team has identified the cause of the issue impacting Spaces API availability in our SFO2 region and is actively working on a fix.
We will post an update as soon as additional information is available
Jan 22, 09:13 UTC
Investigating - As of 09:00 UTC, Our Engineering team is investigating an issue impacting Spaces API availability in our SFO2 region.
During this time, users may experience slowness or timeouts when trying to access or manage their Spaces resources in SFO2.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/7ck5966p1073</id>
        <link href="https://www.githubstatus.com/incidents/7ck5966p1073"/>
        <updated>2024-01-21T09:34:34.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 09:34 UTC
Resolved - On 2024-01-21 at 3:38 UTC, we experienced an incident that affected customers using Codespaces. Customers encountered issues creating and resuming Codespaces in multiple regions due to operational issues with compute and storage resources.
Around 25% of customers were impacted, primarily in East US and West Europe. We re-routed traffic for Codespace creations to less impacted regions, but existing Codespaces in these regions may have been unable to resume during the incident.
By 7:30 UTC, we had recovered connectivity to all regions except West Europe, which had an extended recovery time due to increased load in that particular region. The incident was resolved on 2024-01-21 at 9:34 UTC once Codespace creations and resumes were working normally in all regions.
…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/hmvr5kpgzc45</id>
        <link href="https://www.githubstatus.com/incidents/hmvr5kpgzc45"/>
        <updated>2024-01-21T06:19:52.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 06:19 UTC
Resolved - On 2024-01-21 from 02:05 UTC to 06:19 UTC, GitHub Hosted Runners experienced increased error rates from our main cloud service provider. The errors were initially limited to a single region and we were able to route around the issue by transparently failing over to other regions. However, errors gradually expanded across all regions we deploy to and led to our available compute capacity being exhausted.
During the incident, up to 35% of Actions jobs using Larger Runners and 2% of Actions jobs using GitHub Hosted Runners overall may have experienced intermittent delays in starting. Once the issue was resolved by our cloud service provider, our systems made a full recovery without intervention.
We’re working closely with our service provider to understand the cause of the outage and mitigations we can put in place. We’re also working to increase our resilience to outages of this nature by expanding the regions we deploy to beyond the existing set, especially for Larger Runners.
Jan 21, 05:54 UTC
Update - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.
Jan 21, 05:26 UTC
Update - We have mitigated the issues impacting Actions Larger Runners. We are still experiencing delays starting normal jobs, and are continuing to investigate.
Jan 21, 04:53 UTC
Update - The team has identified the cause of the issues with Actions Larger Runners and has begun mitigation.
Jan 21, 04:16 UTC
Update - The team continues to investigate issues with some Actions jobs being queued for a long time and a percentage of jobs failing. We will continue providing updates on the progress towards mitigation.
Jan 21, 03:45 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet rebuild]]></title>
        <id>https://status.digitalocean.com/incidents/x0f8bvr688rs</id>
        <link href="https://status.digitalocean.com/incidents/x0f8bvr688rs"/>
        <updated>2024-01-17T09:34:03.000Z</updated>
        <summary type="html"><![CDATA[Jan 17, 09:34 UTC
Resolved - As of 09:17 UTC, our Engineering team has confirmed the full resolution of the issue impacting the Droplet rebuild via the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 17, 09:22 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue impacting the Droplet rebuild via Cloud Control Panel and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues when rebuilding Droplets from the Cloud Control Panel. We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 17, 08:26 UTC
Identified - Our Engineering team has identified the cause of the issue impacting the Droplet rebuild via the Cloud Control Panel and is actively working on a fix. During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel. We will post an update as soon as additional information is available.
Jan 17, 07:15 UTC
Investigating - As of 06:50 UTC, our Engineering team is investigating an issue impacting the Droplet rebuild via the Cloud Control Panel.
During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Functionality]]></title>
        <id>https://status.digitalocean.com/incidents/j4jwmwl3szxz</id>
        <link href="https://status.digitalocean.com/incidents/j4jwmwl3szxz"/>
        <updated>2024-01-16T13:34:24.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 13:34 UTC
Resolved - Our Engineering team has resolved the issue impacting multiple spaces-related functionalities. From approximately 10:30 UTC - 13:30 UTC, users may have experienced issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API. Spaces-related functionalities should now be operating normally.
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Jan 16, 12:51 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue affecting multiple Spaces-related functionalities and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues with Spaces-related functionalities. 
We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 16, 12:03 UTC
Identified - Our Engineering team has identified the issue affecting multiple Spaces-related functionalities and is actively working on a fix.
During this time, users may experience issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API.
Additionally, this may also impact Container Registry creation and issues with transferring images between regions. 
We apologize for the inconvenience and will share an update once we have more information.
Jan 16, 10:54 UTC
Investigating - As of 10:30 UTC, our Engineering team is investigating an issue with multiple Spaces functionalities via the Cloud Control Panel. 
During this time, users may experience errors when attempting to delete objects via the Cloud Control Panel. At this moment we are investigating the exact impact and will share more information as soon as we have it.
We apologize for the inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functions]]></title>
        <id>https://status.digitalocean.com/incidents/47dqh6pzqybt</id>
        <link href="https://status.digitalocean.com/incidents/47dqh6pzqybt"/>
        <updated>2024-01-16T09:30:15.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 09:30 UTC
Resolved - As of 08:50 UTC, our Engineering team has confirmed the full resolution of the issue impacting the ability to access and manage Functions through the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 16, 08:59 UTC
Monitoring - Our Engineering team has been able to mitigate the issue related to the access and operations with Functions through the Cloud Control Panel.
Users should no longer face any problems in accessing or operating Functions using the Cloud Control Panel. 
We apologize for the inconvenience. We are monitoring the situation and will post an update once we confirm this incident is fully resolved.
Jan 16, 08:17 UTC
Investigating - As of 03:00 AM UTC, our Engineering team is investigating an issue impacting Functions. 
During this time, users may experience issues with accessing Functions via the Cloud Control Panel. At this time, API operations shouldn't be impacted and should continue to function as intended. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🆕 New partnership announcement with KeyNest!]]></title>
        <id>283413</id>
        <link href="https://changelog.bookingsync.com/new-partnership-announcement-with-keynest!-283413"/>
        <updated>2024-01-16T06:58:43.000Z</updated>
        <summary type="html"><![CDATA[New!
  


We are thrilled to announce a new partnership that will revolutionise key management for vacation rental hosts and property managers like you. 🔑
  

💡What is KeyNest Points?
KeyNest Points is a global network of over 5,500 locations where you can securely store and exchange keys with ease. No more hassles of on-site visits or high installation costs.
Most KeyNest Points are open 24/7, ensuring flexibility for key exchanges at your convenience. Discover your nearest Point and its opening hours on the interactive map provided.

 


🏠 KeyNest Points - Your Trusted Partners
These points are typically local businesses such as convenience stores, cafes, hotels, or petrol stations, conveniently situated near your properties.
Each KeyNest Point is managed by trained staff, ensuring instant and secure key exchanges. Your peace of mind is paramount.

🔤 How Does it Work?

Here's a quick overview of how KeyNest Points operate:
Drop your keys at a local KeyNest Point.
Your keys are tagged and logged into the system for real-time tracking.
Staff at the Point securely stores your keys.
Send the location of the key and the code to your guests within your automated Smily messages.
Guests or cleaners visit the Point, show their safety code, and receive the key.
Upon return, they drop the key back at the same Point, and you are notified.
 
👉 Install the Keynest app here
 
💌 Learn more on our dedicated manual page.
  
If you have any questions, please don't hesitate to contact support@keynest.com.]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may not be able to configure 2FA]]></title>
        <id>https://status.slack.com//2024-01/9301a4fb7cc577ea</id>
        <link href="https://status.slack.com//2024-01/9301a4fb7cc577ea"/>
        <updated>2024-01-15T23:51:07.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:26 AM PST on January 12, 2024 to 10:04 AM PST on January 15, 2024, some users were unable to configure 2FA on their accounts. We were made aware of this after a spike in reports early in the morning of Monday, January 15.


Upon investigation, this issue was traced back to a recent code change which we discovered was preventing users from being redirected back to the 2FA configuration page after entering their password during the 2FA setup process. We immediately reverted this change which fully resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degraded behavior when moving pages in bulk]]></title>
        <id>https://status.notion.so/incidents/wn8zmpphxwr9</id>
        <link href="https://status.notion.so/incidents/wn8zmpphxwr9"/>
        <updated>2024-01-15T19:19:20.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 11:19 PST
Resolved - We've pushed a fix for this issue now and users can move pages in bulk again without errors. We apologize for the earlier disruption and thank you for bearing with us through this. 
Please open Notion and press Cmd/Ctrl + Shift + R to reload the latest changes before trying to move pages again.
Jan 15, 10:24 PST
Identified - Our team has identified the cause of problems when moving pages in bulk across Notion databases and is working on a fix. We will share further updates as soon as the problem is resolved.
Jan 15, 07:53 PST
Investigating - Users may be experiencing degraded behavior when moving pages in bulk across Notion databases. We are investigating this issue and will share an update as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monday app not working]]></title>
        <id>https://status.make.com/incidents/h3gvmlbldj1q</id>
        <link href="https://status.make.com/incidents/h3gvmlbldj1q"/>
        <updated>2024-01-15T15:18:40.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 16:18 CET
Resolved - We have reactivated the affected scenarios. Please note that this reactivation will not be visible in the scenario logs. Currently, the Monday app is fully operational.
Jan 15, 11:42 CET
Update - A fix has been rolled and we are investigating options to re-enable affected scenarios automatically.
Jan 15, 10:27 CET
Monitoring - A fix has been implemented and we are monitoring the results.
Jan 15, 09:47 CET
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue with MFA Login]]></title>
        <id>https://status.notion.so/incidents/qbkb93dwrm9r</id>
        <link href="https://status.notion.so/incidents/qbkb93dwrm9r"/>
        <updated>2024-01-12T06:56:42.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 22:56 PST
Resolved - This incident has been resolved.
Jan 11, 20:52 PST
Identified - We are experiencing issues with MFA  login method. The team has identified the cause and is working on the fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Networking]]></title>
        <id>https://status.digitalocean.com/incidents/33vqf05m8396</id>
        <link href="https://status.digitalocean.com/incidents/33vqf05m8396"/>
        <updated>2024-01-11T00:18:25.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 00:18 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
From approximately 20:15 - 21:45 UTC, DigitalOcean experienced a global networking issue that impacted multiple services and products. Users saw increased error rates and latency for event processing, accessing our Cloud Control Panel/API, applying Cloud Firewall policies, accessing www.digitalocean.com and our Community site, and DNS resolution. Additionally, users saw timeouts/increased latency for networking requests to Droplets and Droplet-based services, as well as connections to existing App Platform Apps. 
We sincerely apologize for the disruption. If you continue to experience issues or have questions, please reach out to our Support team by opening a ticket from within your account. …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New fix for your booking.com reservations]]></title>
        <id>283019</id>
        <link href="https://changelog.bookingsync.com/new-fix-for-your-booking-com-reservations-283019"/>
        <updated>2024-01-10T17:36:04.000Z</updated>
        <summary type="html"><![CDATA[New!
 
Improvement
  
We're thrilled to share a new feature we’ve just released for booking.com properties.
What's New?
The rate rule Booking at least 'x' day ahead is now synchronised on booking.com channel;
Why Does This Matter?
You were facing the obstacle where Guests were making a booking for the same day or 'x' days (based on your settings) even though the setting to prevent that was activated;
-> It won’t happen anymore.
What's Next?
If you face any obstacle with this setting, reach out to us; 
We are and will be working on more improvements to make your day-to-day easier :)
--
💡 For more detailed information, simply click here;
Let's embark on this journey together to make your vacation rental dreams a reality 🚀
Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, API Requests, Pull Requests, Actions, Pages, Git Operations, Webhooks, Packages and Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/pxg3dz4yg7lp</id>
        <link href="https://www.githubstatus.com/incidents/pxg3dz4yg7lp"/>
        <updated>2024-01-09T14:40:45.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 14:40 UTC
Resolved - On January 9 between 12:45 and 13:56 UTC, services in one of our three sites experienced elevated latency for connections.  This led to a sustained period of timed out requests across a number of services, including but not limited to our git backend.  An average of 5% and max of 10% of requests failed with a 5xx response or timed out during this period.  This was caused by a combination of events that led to connection limits being hit in load balancer proxies in that site.  An upgrade of hosts was in flight, which meant a subset of proxy hosts were draining and coming offline as the upgrade rolled through the fleet.  A config change event also triggered a connection reset across all services in that site.  These events are commonplace, but led to a spike in c…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Database Operations]]></title>
        <id>https://status.digitalocean.com/incidents/8fm83lgfsdhm</id>
        <link href="https://status.digitalocean.com/incidents/8fm83lgfsdhm"/>
        <updated>2024-01-09T12:55:23.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 12:55 UTC
Resolved - As of 12:30  UTC, our Engineering team has resolved the issue impacting CRUD (create, read, update, delete) operations for Managed Database Clusters in all the regions.
Everything should now be functioning normally. We appreciate your patience throughout the process.
If you continue to experience problems, please open a ticket with our support team.
Jan  9, 10:17 UTC
Monitoring - Our Engineering team has confirmed that the action taken to mitigate the recurrence of the issue is successful. Users should no longer experience errors in CRUD (create, read, update, delete) operations for Managed Database Clusters in all regions, via both the Cloud Control Panel and API requests.
We are actively monitoring the situation to ensure stability and will provide an update …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
</feed>