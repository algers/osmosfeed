<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-05-27T00:23:16.995Z</id>
    <title>osmos::feed</title>
    <updated>2023-05-27T00:23:16.996Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[United Kingdom Account Security Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/m8yhl0rxzbbs</id>
        <link href="https://status.twilio.com/incidents/m8yhl0rxzbbs"/>
        <updated>2023-05-27T01:30:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT May 26, 18:30 PDT  -  May 29, 10:00 PDT
May 16, 03:00 PDT
Scheduled - Our Carrier Partner Three UK is conducting a planned maintenance from 26 May 2023 at 18:30 PDT until 29 May 2023 at 10:00 PDT. During the maintenance window, there could be intermittent API request failures for UK.
Impacted Products: Lookup SIM Swap Identity Match and SNA]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Russia SMS Carrier Maintenance - Tele2]]></title>
        <id>https://status.twilio.com/incidents/1nj1yzqlm43x</id>
        <link href="https://status.twilio.com/incidents/1nj1yzqlm43x"/>
        <updated>2023-05-27T00:00:02.000Z</updated>
        <summary type="html"><![CDATA[May 26, 17:00 PDT
Completed - The scheduled maintenance has been completed.
May  3, 13:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Apr 28, 07:48 PDT
Scheduled - The Tele2 network in Russia is conducting a planned maintenance from 03 May 2023 at 13:00 PDT until 26 May 2023 at 17:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from Tele2 Russia handsets.

Note, the maintenance will be carried out on each of the following dates and times:

03 May 2023 at 13:00 PDT until 03 May 2023 at 17:00 PDT

10 May 2023 at 13:00 PDT until 10 May 2023 at 17:00 PDT

12 May 2023 at 13:00 PDT until 12 May 2023 at 17:00 PDT

17 May 2023 at 13:00 PDT until 17 May 2023 at 17:00 PDT

19 May 2023 at 13:00 PDT until 19 May 2023 at 17:00 PDT

24 May 2023 at 13:00 PDT until 24 May 2023 at 17:00 PDT

26 May 2023 at 13:00 PDT until 26 May 2023 at 17:00 PDT]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Egypt SMS Carrier Maintenance - Etisalat]]></title>
        <id>https://status.twilio.com/incidents/mr2pfxs1zbrg</id>
        <link href="https://status.twilio.com/incidents/mr2pfxs1zbrg"/>
        <updated>2023-05-26T23:02:25.000Z</updated>
        <summary type="html"><![CDATA[May 26, 16:02 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 25, 14:07 PDT
Scheduled - The Etisalat network in Egypt is conducting an emergency maintenance from 26 May 2023 at 16:00 PDT until 26 May 2023 at 21:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from Etisalat Egypt handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Meteor Network in Ireland]]></title>
        <id>https://status.twilio.com/incidents/86cm39974h11</id>
        <link href="https://status.twilio.com/incidents/86cm39974h11"/>
        <updated>2023-05-26T23:02:20.000Z</updated>
        <summary type="html"><![CDATA[May 26, 16:02 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Meteor network in Ireland. This incident has been resolved.
May 26, 14:50 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Meteor network in Ireland. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
May 26, 11:29 PDT
Update - We continue to experience SMS delivery delays when sending messages to Meteor network in Ireland. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
May 26, 09:32 PDT
Update - We continue to experience SMS delivery delays when sending messages to Meteor network in Ireland. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
May 26, 08:32 PDT
Investigating - We are again experiencing SMS delivery delays when sending messages to Meteor network in Ireland. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connectivity Lost on ~7.7% of Super SIM Devices]]></title>
        <id>https://status.twilio.com/incidents/nxykt3jykftf</id>
        <link href="https://status.twilio.com/incidents/nxykt3jykftf"/>
        <updated>2023-05-26T22:28:04.000Z</updated>
        <summary type="html"><![CDATA[May 26, 15:28 PDT
Resolved - We will close the post on Twilio’s Status Page. If you have been impacted by this incident and wish to continue to receive updates, please open a ticket with customer support.
May 26, 13:30 PDT
Monitoring - We are now seeing recovery of the previously reported connectivity issue. We are continuing to monitor for service stability. We will provide another update in 2 hours or as soon as more information becomes available.
May 26, 12:50 PDT
Update - We are still experiencing Super SIM connectivity issues where we're observing ~7.7% of active data sessions on Super SIM being dropped. Devices that attempted to reconnect immediately after losing connectivity should have been able to successfully reconnect to the redundant instances while the impacted one recovered. Our engineering team is actively working with the affected network provider to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
May 26, 11:55 PDT
Update - Beginning at 6:32PM UTC, we observed ~7.7% of active data sessions on Super SIM being dropped. Devices that attempted to reconnect immediately after losing connectivity should have been able to successfully reconnect to the redundant instances while the impacted one recovered. Our engineering team is actively working with the affected network provider to resolve the issue. We will provide another update in 1 hour or sooner once more information becomes available.
May 26, 11:48 PDT
Investigating - Our monitoring systems have detected a potential issue with Super SIM. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry in SFO3]]></title>
        <id>https://status.digitalocean.com/incidents/93nbbqq83slf</id>
        <link href="https://status.digitalocean.com/incidents/93nbbqq83slf"/>
        <updated>2023-05-26T02:38:56.000Z</updated>
        <summary type="html"><![CDATA[May 26, 02:38 UTC
Resolved - As of 02:10 UTC, Our Engineering team has confirmed that the issue with our Container Registry services in the SFO3 region has been fully resolved. 
All operations should now be operating normally with our Container Registry services. If you continue to experience any trouble with these services please open a ticket with our support team. 
Thank you for your patience and we apologize for the inconvenience.
May 25, 23:31 UTC
Monitoring - As of 22:10 UTC, Our engineering team has implemented a fix to resolve the issue with our Container Registry services in the SFO3 region and is monitoring the situation closely. 
Users should no longer see 500-type errors when uploading/pushing/deleting images, slow cleanup operations, creation failures for new registries, or other errors when interacting with registries in the SFO3 region. 
We are going to continue to monitor the situation and will post an update once we are confident this issue will not recur.
May 25, 16:20 UTC
Investigating - We are observing some customer reports for issues with DigitalOcean Container Registries in the SFO3 region. Our Engineering team is investigating any potential issues that are causing these reports. This seems to be a reoccurrence of the incident mentioned in the below link:
https://status.digitalocean.com/incidents/mmngtxzmm6gs  
At this time, users may see 500 type errors when uploading/pushing/deleting images, slow cleanup operations, creation failures for new registries, or other errors when interacting with registries in SFO3. 
We will post an update as soon as we have further information. Thank you for your patience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are reporting issues triggering shortcut Workflows]]></title>
        <id>https://status.slack.com//2023-05/d3da25ad25333a58</id>
        <link href="https://status.slack.com//2023-05/d3da25ad25333a58"/>
        <updated>2023-05-25T15:36:20.000Z</updated>
        <summary type="html"><![CDATA[We've resolved the issue with triggering shortcut workflows. Thank you very much for your patience while we worked on getting this fix completed. We're sorry for the disruption caused.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments in SFO region]]></title>
        <id>https://status.digitalocean.com/incidents/kg0b7wl8mshl</id>
        <link href="https://status.digitalocean.com/incidents/kg0b7wl8mshl"/>
        <updated>2023-05-25T09:45:27.000Z</updated>
        <summary type="html"><![CDATA[May 25, 09:45 UTC
Resolved - Our Engineering team has confirmed the full resolution of this incident.
From 06:00 - 08:00 UTC, users were unable to create and deploy new Apps and experienced errors when updating, and deploying existing Apps. The App deployments should now be operating normally.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
May 25, 09:20 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with App deployments in the SFO region and is monitoring the situation. We will post an update as soon as the issue is fully resolved.
May 25, 08:47 UTC
Identified - Our Engineering team has identified the cause of the issue with network latency that is impacting the management and creation of Apps in the SFO region and is actively working on a fix. During this time, a subset of users might see errors when updating and deploying new/existing Apps. We will post an update as soon as additional information is available.
May 25, 08:30 UTC
Investigating - Our Engineering team is investigating an issue with network latency that is impacting the management and creation of Apps in the SFO region. As of 06:00 UTC, users are unable to create and deploy new Apps and may see errors when updating, and deploying existing Apps. At this time, previously deployed running Apps are not impacted. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled maintenance]]></title>
        <id>https://status.make.com/incidents/zgjx54wdw2y9</id>
        <link href="https://status.make.com/incidents/zgjx54wdw2y9"/>
        <updated>2023-05-25T07:00:25.000Z</updated>
        <summary type="html"><![CDATA[May 25, 09:00 CEST
Completed - The scheduled maintenance has been completed.
May 25, 08:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 23, 15:00 CEST
Scheduled - We would like to inform you about a scheduled maintenance window that will be taking place on 25th May between 08:00 CEST - 09:00 CEST.
During this maintenance period, there will be a short interruption resulting in the unavailability of the login page and organizations page. 
However, we assure you that there will be no data loss and all scenarios will continue running properly.
We apologize for any inconvenience caused and appreciate your understanding.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry in SFO3]]></title>
        <id>https://status.digitalocean.com/incidents/mmngtxzmm6gs</id>
        <link href="https://status.digitalocean.com/incidents/mmngtxzmm6gs"/>
        <updated>2023-05-25T02:49:34.000Z</updated>
        <summary type="html"><![CDATA[May 25, 02:49 UTC
Resolved - Our Engineering team has identified the root cause of the incident to be multiple DB calls causing DB contention. During this time, users might have experienced issues interacting and authenticating with the DigitalOcean Container Registry, creating new container registries, and pushing/deleting images to/from registries.
As of 00:10 UTC, we have confirmed the full resolution of the issue affecting the DigitalOcean Container Registry in the SFO3 region. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
May 24, 22:53 UTC
Update - Our Engineering team continues to investigate the root cause of this incident but has observed a reduction in the error rate with DigitalOcean Container Registry in the SFO3 region.
At this time, users should no longer experience errors while interacting and authenticating with the DigitalOcean Container Registry, creating new container registries, and pushing/deleting images to/from registries.
We will post an update as soon as we have further information. Thank you for your patience.
May 24, 19:28 UTC
Investigating - Following an uptick in customer reports of issues with DigitalOcean Container Registries in SFO3, our Engineering team is investigating any potential issues that are causing these reports. 
At this time, users may see 500 type errors when uploading/pushing/deleting images, slow cleanup operations, creation failures for new registries, or other errors when interacting with  registries in SFO3. 
We will post an update as soon as we have further information. Thank you for your patience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: A small percentage of users are experiencing issues with Slack.]]></title>
        <id>https://status.slack.com//2023-05/f1df0cac8d8d8d68</id>
        <link href="https://status.slack.com//2023-05/f1df0cac8d8d8d68"/>
        <updated>2023-05-24T22:29:00.000Z</updated>
        <summary type="html"><![CDATA[On May 23, 2023 between 8:38 AM PDT and 9:44 AM PDT some users were unable to send and edit messages, load threads, join channels and create group DMs.


Our investigation uncovered that a recent code change we made had caused an overwhelming impact on our databases. We rolled this change back and temporarily redirected user traffic as our databases recovered. Once we rolled back the change and our databases were in a healthier state, the issue became resolved for impacted users.


We will continue to implement new ways to detect issues like this early-on.


Thank you for your patience while we investigated and resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots and Backups Failure in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/y3033ln7n73w</id>
        <link href="https://status.digitalocean.com/incidents/y3033ln7n73w"/>
        <updated>2023-05-24T21:30:01.000Z</updated>
        <summary type="html"><![CDATA[May 24, 21:30 UTC
Resolved - As of 21:15 UTC, Our Engineering team has confirmed the full resolution of this incident. We have verified that the Snapshot and Backup events in the SGP1 region are processing without any failures and we will now mark this issue as resolved. 
Thank you for your patience and understanding throughout this process. If you should encounter any further issues at all, then please open a ticket with our Support team.
May 24, 21:05 UTC
Update - We are continuing to monitor for any further issues.
May 24, 20:44 UTC
Monitoring - As of 19:30 UTC, Our Engineering team was able to take action to mitigate the impact of this incident and allow Snapshot and Backup events to process normally in the SGP1 region. We will post an update as soon as the issue is fully resolved. 
Please note that while the situation has improved, there may still be a backlog of older events that are in the process of being resolved. We kindly ask for your patience as our team works diligently to address these remaining events. 
We apologize for any inconvenience caused and assure you that we are committed to resolving all outstanding issues.
May 24, 19:20 UTC
Investigating - As of 13:30 UTC our Engineering team is investigating an issue with intermittent Snapshot and Backup failures in our SGP1 region. Users may experience errors when performing Snapshots, but may eventually see retries succeed. 
Any Backup failures are automatically being retried within the Backup window for individual Droplets. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS3 Network Maintenance Phase 3]]></title>
        <id>https://status.digitalocean.com/incidents/gqtn5dtkstzg</id>
        <link href="https://status.digitalocean.com/incidents/gqtn5dtkstzg"/>
        <updated>2023-05-24T18:35:49.000Z</updated>
        <summary type="html"><![CDATA[May 24, 18:35 UTC
Completed - The scheduled maintenance has been completed.
May 24, 16:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 24, 15:05 UTC
Scheduled - Start: 2023-05-24 16:00 UTC
End:  2023-05-24 20:00 UTC

During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the AMS3 region. This will be the final phase of the three maintenance activities performed by our team in AMS3 on consecutive days.
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket .]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subsea Fiber Faults in the APAC region]]></title>
        <id>https://status.digitalocean.com/incidents/qbszx5q5dd19</id>
        <link href="https://status.digitalocean.com/incidents/qbszx5q5dd19"/>
        <updated>2023-05-24T14:32:54.000Z</updated>
        <summary type="html"><![CDATA[May 24, 14:32 UTC
Update - Our Engineering team has detected large amounts of loss and latency on network routes between NYC/TOR and Singapore. Users may experience higher-than-normal latency or amounts of packet loss for traffic traversing those routes. The team is reviewing any possible traffic shifts to alleviate the situation. 
If you have any questions or concerns, please open a support ticket from within your account.
May 19, 14:35 UTC
Update - Our Engineering team detected large amounts of loss and latency on network routes between Singapore and Frankfurt, from 13:10 - 13:20 UTC, today. The issue self-recovered, likely due to upstream providers shuffling traffic. 
Additionally, our team is seeing high levels of loss and latency on network routes between Bangalore and Sydney. Users m…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creates on Functions in NYC1]]></title>
        <id>https://status.digitalocean.com/incidents/6z8tp8bvmpkj</id>
        <link href="https://status.digitalocean.com/incidents/6z8tp8bvmpkj"/>
        <updated>2023-05-24T11:10:47.000Z</updated>
        <summary type="html"><![CDATA[May 24, 11:10 UTC
Resolved - Our engineering team has confirmed the full resolution of this issue. From approximately 09:05 UTC - 10:40 UTC, users were seeing errors when attempting to create new Functions, invoking or updating existing deploys. Functions should now be operating normally. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
May 24, 10:50 UTC
Monitoring - Our Engineering team has deployed a fix to resolve an issue with Serverless Functions in the NYC1 region. All users should be able to create new Functions, invoking or updating existing deploys should be operational. We are monitoring the situation closely and will share an update once the issue is resolved completely.
May 24, 10:34 UTC
Identified - Our Engineering team has identified the cause of the issue with Serverless Functions in the NYC1 region and is actively working on a fix. During this time users may see errors when attempting to create new Functions, as well as when invoking or updating existing deploys. We will post an update as soon as additional information is available.
May 24, 10:09 UTC
Investigating - As of 09:05 UTC, our Engineering team is investigating an issue with Serverless Functions in the NYC1 region. Users may see errors when attempting to create new Functions, as well as when invoking or updating existing deploys in the NYC1 region. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users reporting trouble loading profile information]]></title>
        <id>https://status.slack.com//2023-05/b98bebfae64a9007</id>
        <link href="https://status.slack.com//2023-05/b98bebfae64a9007"/>
        <updated>2023-05-24T06:34:17.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On May 23, 2023 from 5:30 PM PDT to around 8:15 PM PDT, some customers may have experienced blank profile pictures, display names, channel names, and emoji in Slack.


A code change inadvertently altered the firewall rules for some of our regional servers, preventing the Slack clients connected to these regions from loading profile and channel elements as expected.


As an immediate mitigation step, we stopped the code change from rolling out any further, and pushed all traffic from the affected servers to other regions. This resolved the issue for impacted customers. 


We then reverted the code change and manually restored the correct firewall rules on the affected servers. Once the correct rules were back in place, we reintroduced traffic to these regions, fully resolving the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users unable to post reminders with custom emoji]]></title>
        <id>https://status.slack.com//2023-05/8e89e3e3b313b225</id>
        <link href="https://status.slack.com//2023-05/8e89e3e3b313b225"/>
        <updated>2023-05-23T21:07:14.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

On May 18, 2023 from 12:50 PM PDT to May 22, 2023 07:35 AM PDT, some users found that channel reminders containing emojis couldn't be sent. 


We determined that a recent change in code added a property that was invalid in the reminder creation schema, which caused reminders to be rejected upon sending.


Once we reverted the change, the issue was resolved for affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS3 Network Maintenance 2023-05-23 16:00 UTC Phase 2]]></title>
        <id>https://status.digitalocean.com/incidents/xhrg6p90g0yv</id>
        <link href="https://status.digitalocean.com/incidents/xhrg6p90g0yv"/>
        <updated>2023-05-23T19:11:07.000Z</updated>
        <summary type="html"><![CDATA[May 23, 19:11 UTC
Completed - The scheduled maintenance has been completed.
May 23, 16:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 23, 15:20 UTC
Scheduled - Start: 2023-05-23 16:00 UTC
End: 2023-05-23 20:00 UTC

During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the AMS3 region. This will be the second of three maintenance activities performed by our team in AMS3 on consecutive days.
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket .]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users from Indonesia are having trouble connecting to Slack]]></title>
        <id>https://status.slack.com//2023-05/df664b4db7d12627</id>
        <link href="https://status.slack.com//2023-05/df664b4db7d12627"/>
        <updated>2023-05-23T06:16:18.000Z</updated>
        <summary type="html"><![CDATA[On May 23, 2023 from around 7:00 PM to 10:37 PM PDT, some users located in Indonesia may have experienced connectivity issues with Slack. 


We investigated, and determined that a local Internet service provider (ISP) is not resolving Slack domains as expected. 


If you're having trouble connecting to Slack, please reach out to your ISP for assistance.


Note: This post has been edited to accurately reflect the duration of this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS3 Network Maintenance 2023-05-22 16:00 UTC Phase 1]]></title>
        <id>https://status.digitalocean.com/incidents/6gkqfc6j4jq5</id>
        <link href="https://status.digitalocean.com/incidents/6gkqfc6j4jq5"/>
        <updated>2023-05-22T19:30:09.000Z</updated>
        <summary type="html"><![CDATA[May 22, 19:30 UTC
Completed - The scheduled maintenance has been completed.
May 22, 16:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 22, 15:20 UTC
Scheduled - Start: 2023-05-22 16:00 UTC
End: 2023-05-22 20:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the AMS3 region. This maintenance will occur in three parts on consecutive days, and we will send other maintenance notices for the second and third phases. 
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket .]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missing Cleaning Fee in Reservations]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/y9spcdfq87h1</id>
        <link href="https://airbnbapi.statuspage.io/incidents/y9spcdfq87h1"/>
        <updated>2023-05-22T16:24:50.000Z</updated>
        <summary type="html"><![CDATA[May 22, 09:24 PDT
Resolved - We have identified and fixed an issue related to missing PASS_THROUGH_CLEANING_FEE standard fee that occurred between 13:54 PDT May 17th until 17:07 PDT May 18th 2023. 
The issue caused standard fee PASS_THROUGH_CLEANING_FEE to be cleared and as a result some reservations might be missing the cleaning fee. 
Please reapply the Standard Fees by making PUT pricing_settings requests with the desired fees.
This should re-populate the listings' Standard Fees array as intended, namely the PASS_THROUGH_CLEANING_FEE.
If you have any further queries related to this incident, please submit a ticket via the Partner Support Portal and our team will get back to you as soon as possible.
Apologies for the inconvenience caused and thank you for your understanding.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled maintenance]]></title>
        <id>https://status.make.com/incidents/drg7g8lyvk6c</id>
        <link href="https://status.make.com/incidents/drg7g8lyvk6c"/>
        <updated>2023-05-22T08:00:25.000Z</updated>
        <summary type="html"><![CDATA[May 22, 10:00 CEST
Completed - The scheduled maintenance has been completed.
May 22, 09:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 18, 13:23 CEST
Scheduled - We would like to inform you about a scheduled maintenance window that will be taking place on 22nd May between 09:00 CEST - 10:00 CEST.
During this maintenance period, there will be a short interruption resulting in the unavailability of the login page and organizations page. 
However, we assure you that there will be no data loss and all scenarios will continue running properly.
We apologize for any inconvenience caused and appreciate your understanding.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNS Services and App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/m32tbckd7mgh</id>
        <link href="https://status.digitalocean.com/incidents/m32tbckd7mgh"/>
        <updated>2023-05-20T14:55:05.000Z</updated>
        <summary type="html"><![CDATA[May 20, 14:55 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
From 12:24 - 13:26 UTC, we experienced an issue that impacted our DNS API services. During this time, customer deployments to App Platform will have failed to process, and managing domains and DNS records through both the Cloud Control Panel and API would have been unavailable. 
If you continue to experience problems with either of these services please open a ticket with our support team from within your Cloud Control Panel. Thank you for your patience.
May 20, 13:57 UTC
Monitoring - Our Engineering team identified that the root cause of the issues impacting App Platform deployments was a wider issue involving our DNS API. Along with the App Platform deployment errors, customers would have experienced trouble viewing and editing domains and their DNS records through both the Cloud Control Panel and the API. 
A fix has been rolled out and as of 13:26 UTC customers should no longer be experiencing any issues involving the services listed above. Our team will continue to monitor the situation to ensure stability and provide a final update as soon as we confirm the issue has been fully resolved. 
Thank you for your patience.
May 20, 12:46 UTC
Investigating - As of 12:24 UTC, Our Engineering team is investigating a global issue with our App Platform service. During this time, users may experience issues while performing any operations for new, updated and deleting deployments. 
At this time, previously deployed running Apps are not impacted. We will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event processing in FRA1 region]]></title>
        <id>https://status.digitalocean.com/incidents/tsb7y4bqyg1d</id>
        <link href="https://status.digitalocean.com/incidents/tsb7y4bqyg1d"/>
        <updated>2023-05-20T11:19:41.000Z</updated>
        <summary type="html"><![CDATA[May 20, 11:19 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. From 07:01 UTC - 11:00 UTC, we have verified that there is no further risk to event processing in the FRA1 region, and we will now mark this issue as Resolved. Thank you for your patience and understanding throughout this process. If you should encounter any further issues at all, then please open a ticket with our Support team.
May 20, 08:43 UTC
Monitoring - Our Engineering team was able to take action to mitigate the impact of this incident and allow events to process normally. We will post an update as soon as the issue is fully resolved. Please note that while the situation has improved, there may still be a backlog of older events that are in the process of being resolved. We kindly ask for your patience as our team works diligently to address these remaining events. We apologize for any inconvenience caused and assure you that we are committed to resolving all outstanding issues.
May 20, 08:20 UTC
Identified - Our Engineering team has identified the cause of the issue with event processing in the FRA1 region and is actively working on a fix. During this time, only a subset of users may experience delays during creates, destroys, and power events in the cloud panel. We will post an update as soon as additional information is available.
May 20, 07:59 UTC
Investigating - Our Engineering team is investigating an issue with event processing in the FRA1 region. Beginning 07:01 UTC, users may experience delays during creates, destroys, and power events. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Migration Enforcement - Phase 5]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/jfj0pfxnz17q</id>
        <link href="https://airbnbapi.statuspage.io/incidents/jfj0pfxnz17q"/>
        <updated>2023-05-20T07:00:08.000Z</updated>
        <summary type="html"><![CDATA[May 20, 00:00 PDT
Completed - The scheduled maintenance has been completed.
May 15, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 10, 09:58 PDT
Scheduled - The deprecation date for API Version 2021.12.31 was March 31, 2023. Any applications that have not fully migrated will be automatically migrated to a newer version, which can result in application breakage.
Automatic migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 5 is the last phase and begins on May 15, 2023 for the following endpoints: Listings, Listing Permits, Async Calendars]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volumes Availability and Downstream Services in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/q8m2ql93bwtj</id>
        <link href="https://status.digitalocean.com/incidents/q8m2ql93bwtj"/>
        <updated>2023-05-18T18:48:03.000Z</updated>
        <summary type="html"><![CDATA[May 18, 18:48 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
From 17:06 - 17:39 UTC, we experienced an availability outage on an internal storage cluster, due to an issue with a networking component. Users may have seen degraded performance with Volumes, issues connecting to Managed Kubernetes clusters, issues creating/deleting Mongo clusters, and delayed deploys/updates to existing Apps in our FRA1 region. 
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. Thank you for your patience throughout this incident.
May 18, 17:59 UTC
Monitoring - Our Engineering team has confirmed the issue with the networking component of the internal storage cluster was the root cause and the remediatio…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack is not loading properly for some users]]></title>
        <id>https://status.slack.com//2023-05/72e5e8644f160e6a</id>
        <link href="https://status.slack.com//2023-05/72e5e8644f160e6a"/>
        <updated>2023-05-17T22:40:00.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On May 17, 2023 from 11:28 AM to 11:55 AM PDT some users encountered error messages while connecting to Slack.


This issue was a result of an operational change made in error, which caused our databases to become inaccessible. We rolled back this change, fixing the problem for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Users having issues loading Slack]]></title>
        <id>https://status.slack.com//2023-05/f2b711ba48d8803a</id>
        <link href="https://status.slack.com//2023-05/f2b711ba48d8803a"/>
        <updated>2023-05-17T14:58:25.000Z</updated>
        <summary type="html"><![CDATA[Our engineering team deployed a minor technical change to the handling of cryptographic operations, such as API token validation.

During this redirection we saw a higher than anticipated volume of failed API requests which resulted in an impact to the Slack service. As a result, Slack was down between 1:56 PM PDT - 2:03 PM PDT.

This was resolved when these changes were rolled back by infrastructure teams.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled maintenance]]></title>
        <id>https://status.make.com/incidents/c9xkycy5vc6v</id>
        <link href="https://status.make.com/incidents/c9xkycy5vc6v"/>
        <updated>2023-05-17T10:00:17.000Z</updated>
        <summary type="html"><![CDATA[May 17, 12:00 CEST
Completed - The scheduled maintenance has been completed.
May 17, 10:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 12, 14:13 CEST
Scheduled - We would like to inform you about a scheduled maintenance window that will be taking place on 17th May, 10:00 CEST.
During this maintenance period, there will be a short interruption resulting in the unavailability of the login page and organizations page. 
However, we assure you that there will be no data loss and all scenarios will continue running properly.
We apologize for any inconvenience caused and appreciate your understanding.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/wbscgqf3dc4s</id>
        <link href="https://www.githubstatus.com/incidents/wbscgqf3dc4s"/>
        <updated>2023-05-17T09:58:38.000Z</updated>
        <summary type="html"><![CDATA[May 17, 09:58 UTC
Resolved - This incident has been resolved.
May 17, 09:22 UTC
Update - We identified an issue that impacts Codespaces customers in Europe West geographic area and are working on mitigation. We will continue to keep you updated on progress.
May 17, 08:15 UTC
Update - We are investigating issues with Codespaces in the Europe West geographic area. Some users may not be able to connect to their Codespaces at this time. We will update you on mitigation progress.
May 17, 07:36 UTC
Investigating - We are investigating reports of degraded performance for Codespaces.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Custom reminders are not working]]></title>
        <id>https://status.slack.com//2023-05/f8fb9eb17b590a12</id>
        <link href="https://status.slack.com//2023-05/f8fb9eb17b590a12"/>
        <updated>2023-05-17T03:12:27.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On May 16, 2023, from 8:17 AM PDT to 5:30 PM PDT, some users were unable to set custom reminders via the "remind me about this message" prompt on desktop and mobile. We’ve rolled back the code that caused this issue which will resolve this automatically for affected users. We appreciate you bearing with us while we investigated.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NYC2 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/yznm20nscmwt</id>
        <link href="https://status.digitalocean.com/incidents/yznm20nscmwt"/>
        <updated>2023-05-16T23:49:13.000Z</updated>
        <summary type="html"><![CDATA[May 16, 23:49 UTC
Completed - The scheduled maintenance has been completed.
May 16, 22:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 16, 21:06 UTC
Scheduled - Start: 2023-05-16 22:00 UTC
End: 2023-05-17 02:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the NYC2 region. This will be the second of the two maintenance activities performed by our team in the region on consecutive days.
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket .]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Codespaces, Git Operations, Issues, Pages and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/v2xjymn61dv8</id>
        <link href="https://www.githubstatus.com/incidents/v2xjymn61dv8"/>
        <updated>2023-05-16T21:32:24.000Z</updated>
        <summary type="html"><![CDATA[May 16, 21:32 UTC
Resolved - This incident has been resolved.
May 16, 21:29 UTC
Update - API Requests, Codespaces, Git Operations, Issues, Pages and Pull Requests are operating normally.
May 16, 21:21 UTC
Update - API Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May 16, 21:21 UTC
Update - Pages is experiencing degraded performance. We are continuing to investigate.
May 16, 21:21 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
May 16, 21:20 UTC
Update - Codespaces is now experiencing degraded availability. We are continuing to investigate.
May 16, 21:20 UTC
Update - Git Operations is now experiencing degraded availability. We are still investigating and will provide an update when we have one.
May 16, 21:19 UTC
Update - Codespaces is experiencing degraded performance. We are continuing to investigate.
May 16, 21:19 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
May 16, 21:19 UTC
Update - Pull Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May 16, 21:14 UTC
Investigating - We are investigating reports of degraded performance for Git Operations.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS2 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/r04278jpttyp</id>
        <link href="https://status.digitalocean.com/incidents/r04278jpttyp"/>
        <updated>2023-05-16T16:00:11.000Z</updated>
        <summary type="html"><![CDATA[May 16, 16:00 UTC
Completed - The scheduled maintenance has been completed.
May 16, 12:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 13, 12:18 UTC
Scheduled - Start: 2023-05-16 12:00 UTC
End: 2023-05-16 16:00 UTC
During the above window, our networking team will be making changes to core networking infrastructure to improve performance and scalability in the AMS2 region.
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer network traffic due to this maintenance. 
Should an unexpected issue arise, a possible outcome would be a failure of control plane events, including, but not limited to, Droplet and Droplet-based service creates, Snapshots, Backups, etc. Should this failure occur, we will update this post with further details. 
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NYC2 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/m951p2pvw8wy</id>
        <link href="https://status.digitalocean.com/incidents/m951p2pvw8wy"/>
        <updated>2023-05-16T01:11:47.000Z</updated>
        <summary type="html"><![CDATA[May 16, 01:11 UTC
Completed - The scheduled maintenance has been completed.
May 15, 22:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 15, 21:10 UTC
Scheduled - Start: 2023-05-15 22:00 UTC
End: 2023-05-16 02:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the NYC2 region. This maintenance will occur in two parts, and we will post another maintenance notice for the second phase. 
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket .]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/dwyd2ql4mypg</id>
        <link href="https://www.githubstatus.com/incidents/dwyd2ql4mypg"/>
        <updated>2023-05-15T20:28:33.000Z</updated>
        <summary type="html"><![CDATA[May 15, 20:28 UTC
Resolved - This incident has been resolved.
May 15, 20:14 UTC
Investigating - We are investigating reports of degraded performance for Copilot.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion Android app crashing]]></title>
        <id>https://status.notion.so/incidents/mxzyk8yx32b5</id>
        <link href="https://status.notion.so/incidents/mxzyk8yx32b5"/>
        <updated>2023-05-12T22:16:34.000Z</updated>
        <summary type="html"><![CDATA[May 12, 15:16 PDT
Resolved - The issue has been resolved. Affected users should update their Notion Android app to the latest version.
May 12, 11:54 PDT
Identified - The issue has been identified and we are working on a fix.
May 12, 11:42 PDT
Investigating - We are investigating an issue causing the Notion Android app to crash for many users.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Codespaces, Git Operations, Issues, Pages, Pull Requests and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/nf7s6933tnn8</id>
        <link href="https://www.githubstatus.com/incidents/nf7s6933tnn8"/>
        <updated>2023-05-11T19:00:31.000Z</updated>
        <summary type="html"><![CDATA[May 11, 19:00 UTC
Resolved - This incident has been resolved.
May 11, 18:54 UTC
Update - Reindexing is complete, and the /pulls and /search pages are up to date.
May 11, 18:19 UTC
Update - We’ve reindexed a little over half of the pull requests missing from the /pulls and /search pages.
May 11, 17:53 UTC
Update - We have reindexed about 20% of the pull requests missing from the /pulls and /search pages.
May 11, 17:21 UTC
Update - We’ve identified the full set of impacted pull requests not showing up on the /pulls and /search pages. We’ve kicked off a job to reindex them.
May 11, 16:55 UTC
Update - We’ve completed re-running the push jobs. We continue to investigate some pull requests not appearing in the /pulls and /search pages.
May 11, 16:23 UTC
Update - We’re continuing to investigate s…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/23mgd6f1r30h</id>
        <link href="https://airbnbapi.statuspage.io/incidents/23mgd6f1r30h"/>
        <updated>2023-05-11T01:42:33.000Z</updated>
        <summary type="html"><![CDATA[May 10, 18:42 PDT
Resolved - This incident has been resolved.
If you encounter further errors, please reach out via the Support Portal.
Apologies for the inconvenience caused and thank you for your patience and understanding.
May 10, 10:24 PDT
Monitoring - The error rate has dropped and the issue is mitigated.
May 10, 07:52 PDT
Investigating - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started around 7:20 AM PDT. Please know our engineering and operations teams are working hard to get everything up and running again, and we will update you with the latest information as soon as possible]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Codespaces, Git Operations, Pages and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/pr3498h3qkfy</id>
        <link href="https://www.githubstatus.com/incidents/pr3498h3qkfy"/>
        <updated>2023-05-11T00:34:42.000Z</updated>
        <summary type="html"><![CDATA[May 11, 00:34 UTC
Resolved - This incident has been resolved.
May 11, 00:34 UTC
Update - API Requests is operating normally.
May 11, 00:06 UTC
Update - GitHub App token creation and write load has recovered. Actions, Codespaces, and Pull Requests are within SLOs again. Suspended Apps are being re-enabled while we monitor the recovery.
May 10, 23:27 UTC
Update - Token creation for GitHub Apps is failing in 11% of requests due to high write load, impacting core GitHub.com operations. High-traffic Apps are being temporarily disabled to aid system recovery.
May 10, 23:09 UTC
Update - We are continuing to investigate ongoing issues with delayed Actions runs, Codespace connections, and PR creation. We are performing a series of steps to mitigate the impact.
May 10, 21:12 UTC
Update - We are cont…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We're investigating reports of an issue with Gmail. We will provide more information shortly.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/pTK88GYX4tqfeXd7h1Ws</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/pTK88GYX4tqfeXd7h1Ws"/>
        <updated>2023-05-10T21:05:59.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-05-05 19:02</strong> and ended at <strong>2023-05-05 19:58</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Cloud Support using <a href="https://cloud.google.com/support">https://cloud.google.com/support</a>  or to Google Workspace Support using help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 5 May 2023 12:02</p>
<p><strong>Incident End:</strong> 5 May 2023 12:58</p>
<p><strong>Duration:</strong> 56 minutes</p>
<p><strong>Affected Services and Features:</strong></p>
<p>Gmail, Google Cloud Networking</p>
<p><strong>Regions/Zones:</strong> Multi Region Europe</p>
<p><strong>Description:</strong></p>
<p>Multiple Google services, including GCP and Workspace, experienced an outage for a duration of up to 56 minutes in and around Spain/Morocco. From preliminary analysis, the root cause of the issue was a Google initiated network routing change at the edge of our network there. The issue was mitigated at 12:58 by removing the problematic routers from service, forcing traffic to use other paths.</p>
<p><strong>Customer Impact:</strong></p>
<h4>GCP</h4>
<ul>
<li>Google Cloud Networking experienced a full outage for all internet service providers (ISPs) connected to two peering routers in Madrid, Spain.</li>
<li>All GCP services were unavailable for users reaching GCP from impacted ISP(s) until mitigation in and around Spain/Morocco.</li>
</ul>
<h4>Workspace</h4>
<ul>
<li>Some Gmail customers had delivery delays and availability issues during the outage in Spain.</li>
</ul>
<p>If your service or application was affected, we apologize — this is not the level of quality and reliability we strive to offer you, and we have taken and are taking immediate steps to improve the platform’s performance and availability.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Status Page Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/zcbjf2dm7wqk</id>
        <link href="https://status.digitalocean.com/incidents/zcbjf2dm7wqk"/>
        <updated>2023-05-10T15:48:07.000Z</updated>
        <summary type="html"><![CDATA[May 10, 15:48 UTC
Completed - Status page maintenance is now complete. Thank you for your patience throughout this process. If you encounter any issues with the new status page please let us know by opening a Support ticket through the Cloud Support page at https://cloudsupport.digitalocean.com/s/createticket.
If you have any general feedback regarding the redesigned status page, we encourage you to share your thoughts in our Ideas forum at the following URL: https://ideas.digitalocean.com/interfaces/p/status-page-redesign-may-2023
May 10, 14:01 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May 10, 01:46 UTC
Scheduled - During the above window, we will be updating the DigitalOcean Status page with a fresh design and some exciting new features. During the maintenance window, our status page at https://status.digitalocean.com may show outdated content. Users will still see up-to-date notices in their Cloud Control Panel for any ongoing incidents. 
We’re thrilled to bring our customers a fresh look and improved experience with our redesigned status page. If you have any questions or concerns about this maintenance window, please open a Support ticket through the Cloud Support page at https://cloudsupport.digitalocean.com/s/createticket.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/wj1qbq7r9tcl</id>
        <link href="https://www.githubstatus.com/incidents/wj1qbq7r9tcl"/>
        <updated>2023-05-10T00:06:00.000Z</updated>
        <summary type="html"><![CDATA[May 10, 00:06 UTC
Resolved - This incident has been resolved.
May  9, 23:31 UTC
Update - We are seeing slower than usual times to run git clone over HTTPS. SSH is not impacted. We continue to investigate.
May  9, 22:52 UTC
Update - We are seeing slower than usual times to run git clone over HTTPS. SSH is not impacted. We continue to investigate.
May  9, 22:39 UTC
Investigating - We are investigating reports of degraded performance for Git Operations.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Codespaces, Git Operations, Issues, Pages, Pull Requests and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/f0mhbz9xn497</id>
        <link href="https://www.githubstatus.com/incidents/f0mhbz9xn497"/>
        <updated>2023-05-09T21:14:09.000Z</updated>
        <summary type="html"><![CDATA[May  9, 21:14 UTC
Resolved - This incident has been resolved.
May  9, 19:43 UTC
Update - All services continue to operate normally and we are continuing work on restoring delayed pull request updates for some impacted customers. We will go back to green once that work has completed.
May  9, 16:55 UTC
Update - Git Operations is operating normally.
May  9, 16:38 UTC
Update - All services continue to operate normally and we are continuing work on restoring delayed pull request updates for some impacted customers. We will go back to green once that work has completed.
May  9, 15:07 UTC
Update - All services are operating normally but we are working on restoring delayed pull request updates for some impacted customers and will go back to green once that work has completed.
May  9, 13:40 UTC
Upd…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/wzv2wd9zvhpr</id>
        <link href="https://www.githubstatus.com/incidents/wzv2wd9zvhpr"/>
        <updated>2023-05-09T10:04:30.000Z</updated>
        <summary type="html"><![CDATA[May  9, 10:04 UTC
Resolved - This incident has been resolved.
May  9, 09:50 UTC
Update - Our engineers continue to monitor the delivery of backlogged notifications.
May  9, 08:44 UTC
Update - We have mitigated the issue impacting the delivery of notifications, and are working to deliver the backlog of notifications.
May  9, 08:09 UTC
Update - We are investigating delays in the delivery of Pull Request notifications.
May  9, 08:07 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Migration Enforcement - Phase 4]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/c1zh41bsb899</id>
        <link href="https://airbnbapi.statuspage.io/incidents/c1zh41bsb899"/>
        <updated>2023-05-09T07:00:21.000Z</updated>
        <summary type="html"><![CDATA[May  9, 00:00 PDT
Completed - The scheduled maintenance has been completed.
May  8, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May  3, 10:08 PDT
Scheduled - The deprecation date for API Version 2021.12.31 was March 31, 2023. Any applications that have not fully migrated will be automatically migrated to a newer version, which can result in application breakage.
Automatic migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 4 begins May 8, 2023 for the Critical Pricing APIs:
- Availability Rules
- Pricing Settings
- Seasonal Rule Groups
- Seasonal Rule Groups Timeline
- Rate Plans
- LOS Records]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in SFO]]></title>
        <id>https://status.digitalocean.com/incidents/x7b44tkx6stq</id>
        <link href="https://status.digitalocean.com/incidents/x7b44tkx6stq"/>
        <updated>2023-05-09T04:00:42.000Z</updated>
        <summary type="html"><![CDATA[May  9, 04:00 UTC
Resolved - Impact: minor;
Our upstream provider will continue to work to fully resolve this issue, but at this time, we have not been provided an ETA. Until we receive communication from them, traffic will remain shifted away to alternate providers to mitigate user impact. 
Due to this, we will go ahead and close out this incident. The traffic shift to reintroduce upstream providers, when it occurs, should be seamless for users in the SFO regions. 
If you have any questions or concerns about this or continue to experience network connectivity issues, please open a ticket with our support team from within your Cloud Control Panel.
Affected Components: Services - Networking Regions - SFO1 Regions - SFO2 Regions - SFO3 
Status: resolved
2023-05-09 04:12 UTC
-
We have receive…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNS Resolution in SFO1]]></title>
        <id>https://status.digitalocean.com/incidents/jp6lt1bmhx85</id>
        <link href="https://status.digitalocean.com/incidents/jp6lt1bmhx85"/>
        <updated>2023-05-09T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[May  9, 02:00 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
Reports show that from approximately 20:00 UTC on May 8th to 02:30 UTC on May 9th, DNS resolution was inoperative for services in our SFO1 region. 
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
Resolved: May 09, 2023 - 03:42 UTC
-
Our Engineering team identified an issue with the path that resolver traffic was taking for our SFO1 region, resulting in services in SFO1 being unable to complete DNS lookups/resolve domains. A fix has been put in place and we've confirmed that DNS resolution is now functioning normally.
We're now monitoring the situation and will post an update shortly once we confirm full resolution of this incident.
Monitoring: May 09, 2023 - 02:49 UTC
-
Our Engineering team is investigating user reports of services in our SFO1 region being unable to complete DNS lookups/resolve domains. This includes running commands on Droplets that utilize DNS lookups, such as "wget" and "apt update", etc. Inbound network connections to services should still be operating normally. 
Thank you for your patience as our team works to understand the root cause of the issue. We'll post an update once we have further information.
Investigating: May 09, 2023 - 02:06 UTC
-
This incident affected: Services (DNS) and Regions (SFO1).]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbnb API is down]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/2g99rmdkn9lg</id>
        <link href="https://airbnbapi.statuspage.io/incidents/2g99rmdkn9lg"/>
        <updated>2023-05-04T21:49:03.000Z</updated>
        <summary type="html"><![CDATA[May  4, 14:49 PDT
Resolved - This incident has been resolve. We apologize the inconvenience caused. Don't hesitate to contact us if you need technical support.
May  4, 10:29 PDT
Monitoring - We have deployed a fix at 10:05 AM PDT, and our systems are back up and running again. We will continue to monitor the results in the upcoming hours.
May  4, 08:40 PDT
Investigating - Starting today, from 08:15 AM PDT we are experiencing issues across Airbnb platform and the API - any requests to the API are returning a 503 error. We are aware of the issue and are working on it urgently.
Please know our engineering and operations teams are working hard to get everything up and running again, and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Object Uploading/Downloading through Cloud Control Panel]]></title>
        <id>https://status.digitalocean.com/incidents/tj9r8tq5ycw6</id>
        <link href="https://status.digitalocean.com/incidents/tj9r8tq5ycw6"/>
        <updated>2023-05-04T18:48:50.000Z</updated>
        <summary type="html"><![CDATA[May  4, 18:48 UTC
Resolved - Impact: minor;
Our Engineering team has confirmed full resolution of this incident. 
From approximately 18:00 - 19:00 UTC, users were unable to upload or download objects in the control panel for Spaces buckets in all regions. 
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
Affected Components: Services - Spaces Regions - Global 
Status: resolved
2023-05-04 19:49 UTC
-
Our Engineering team identified the root cause of the issue impacting Spaces object uploads and downloads in the Cloud Control Panel and has reverted the breaking change. Users should now be able to use Spaces normally. 
We will continue to monitor the situation to ensure stability and provide a final update once we confirm the issue has been fully resolved. 
Thank you again for your patience and we apologize for the inconvenience.
Affected Components: Services - Spaces Regions - Global 
Status: monitoring
2023-05-04 19:08 UTC
-
Our Engineering team is investigating reports of an issue with uploading and downloading objects to and from Spaces buckets in the Cloud Control Panel, for all regions. User reports indicate this issue began around 18:00 UTC. 
At this time, functionality via the Spaces API does not appear to be impacted. 
We are working to confirm details of this issue and will provide an update as soon as possible.
Affected Components: Services - Spaces Regions - Global 
Status: investigating
2023-05-04 18:48 UTC
-
Incident Created at: 2023-05-04 18:48 UTC Resolved at: 2023-05-04 19:49 UTC]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[website maintenance]]></title>
        <id>https://status.make.com/incidents/10hhnwxqyw68</id>
        <link href="https://status.make.com/incidents/10hhnwxqyw68"/>
        <updated>2023-05-04T16:58:49.000Z</updated>
        <summary type="html"><![CDATA[May  4, 18:58 CEST
Completed - Maintenance is completed
May  4, 18:40 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
May  4, 18:39 CEST
Scheduled - Some users might experience unexpected logout. We expect no impact on primary functionality of the platform.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Codespaces, Copilot, Git Operations, Issues, Pages, Pull Requests and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/c2jg911dtkjb</id>
        <link href="https://www.githubstatus.com/incidents/c2jg911dtkjb"/>
        <updated>2023-05-04T16:23:01.000Z</updated>
        <summary type="html"><![CDATA[May  4, 16:23 UTC
Resolved - This incident has been resolved.
May  4, 16:22 UTC
Update - Actions is operating normally.
May  4, 16:22 UTC
Update - Webhooks is operating normally.
May  4, 16:22 UTC
Update - Pages is operating normally.
May  4, 16:21 UTC
Update - Pull Requests is operating normally.
May  4, 16:21 UTC
Update - Codespaces is operating normally.
May  4, 16:21 UTC
Update - Issues is operating normally.
May  4, 16:19 UTC
Update - Codespaces is now experiencing degraded performance. We are still investigating and will provide an update when we have one.
May  4, 16:19 UTC
Update - API Requests is operating normally.
May  4, 16:18 UTC
Update - Pages is operating normally.
May  4, 16:18 UTC
Update - Copilot is operating normally.
May  4, 16:06 UTC
Update - Pages is experiencing degraded performance. We are continuing to investigate.
May  4, 16:06 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
May  4, 16:06 UTC
Update - Copilot is experiencing degraded performance. We are continuing to investigate.
May  4, 16:06 UTC
Update - API Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May  4, 16:04 UTC
Update - Webhooks is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May  4, 16:04 UTC
Update - Codespaces is experiencing degraded availability. We are continuing to investigate.
May  4, 16:03 UTC
Update - Git Operations is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May  4, 16:03 UTC
Update - Pull Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
May  4, 15:55 UTC
Investigating - We are investigating reports of degraded performance for Issues.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN Certificate and Load Balancer creates in AMS3 and SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/rkdmkrqwtght</id>
        <link href="https://status.digitalocean.com/incidents/rkdmkrqwtght"/>
        <updated>2023-05-03T02:26:27.000Z</updated>
        <summary type="html"><![CDATA[May  3, 02:26 UTC
Resolved - Impact: minor;
As of 03:45 UTC, Our Engineering team has confirmed the full resolution of the issue which impacted Spaces CDN certificates in AMS3 and SGP1 regions.
If you should experience any further issues, please open a ticket with our support team right away. Thank you for your patience and we apologize for the inconvenience.
Affected Components: Services - Spaces CDN Regions - AMS3 Regions - SGP1 
Status: resolved
2023-05-03 03:55 UTC
-
Our engineering team has renewed the expired SSL certificates on the Spaces CDN in AMS3 and SGP1 regions. Users should no longer experience any errors when accessing the objects using the CDN endpoint. 
We are now monitoring Spaces traffic to ensure this fix has mitigated all impacts and will post an update as soon as we c…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI down]]></title>
        <id>https://status.notion.so/incidents/pyrhq77wmg6p</id>
        <link href="https://status.notion.so/incidents/pyrhq77wmg6p"/>
        <updated>2023-05-02T16:32:19.000Z</updated>
        <summary type="html"><![CDATA[May  2, 09:32 PDT
Resolved - We have resolved this issue and Notion AI should be working again for users.
May  2, 09:14 PDT
Investigating - We are currently experiencing an issue with Notion AI. Engineering is investigating the root cause of this issue.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/lhw4ny5js29b</id>
        <link href="https://www.githubstatus.com/incidents/lhw4ny5js29b"/>
        <updated>2023-05-02T15:49:29.000Z</updated>
        <summary type="html"><![CDATA[May  2, 15:49 UTC
Resolved - This incident has been resolved.
May  2, 15:34 UTC
Update - We are investigating reports of issues with service(s): Actions. A subset of customers are experiencing run start delays and job failures. We are deploying a fix to mitigate the issue.
May  2, 14:58 UTC
Investigating - We are investigating reports of degraded performance for Actions.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Migration Enforcement - Phase 3]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/xgzsxbpp8jy3</id>
        <link href="https://airbnbapi.statuspage.io/incidents/xgzsxbpp8jy3"/>
        <updated>2023-05-02T07:00:15.000Z</updated>
        <summary type="html"><![CDATA[May  2, 00:00 PDT
Completed - The scheduled maintenance has been completed.
May  1, 00:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Apr 26, 10:06 PDT
Scheduled - The deprecation date for API Version 2021.12.31 was March 31, 2023. Any applications that have not fully migrated will be automatically migrated to a newer version, which can result in application breakage.
Automatic migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 3 begins May 1, 2023 for the following endpoints: Descriptions, Photos, Rooms, Booking Settings]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Enterprise Grid customers experiencing errors when starting a workflow]]></title>
        <id>https://status.slack.com//2023-05/c2ae05d7865dc16b</id>
        <link href="https://status.slack.com//2023-05/c2ae05d7865dc16b"/>
        <updated>2023-05-02T03:25:35.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On May 1, 2023, from 3:53 PM PDT to 6:45 PM PDT, some Enterprise Grid customers may have experienced issues launching workflows in Slack. 


A recent change to workflows inadvertently introduced an issue that resulted in the error codes REBBE72D5DA6 and REBA7F7D9186 when launching workflows. We rolled back the change, which resolved the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subsea Fiber Faults in the APAC region]]></title>
        <id>https://status.digitalocean.com/incidents/4w1yx5p58p1t</id>
        <link href="https://status.digitalocean.com/incidents/4w1yx5p58p1t"/>
        <updated>2023-05-01T20:00:00.000Z</updated>
        <summary type="html"><![CDATA[May  1, 20:00 UTC
Resolved - Our Engineering team has continued to actively monitor the situation resulting from multiple subsea fiber faults in the APAC region. Over the last ten days, network connectivity on routes from Singapore to San Francisco and Singapore to Europe has not experienced any major packet loss or latency. Network routes to and from Bangalore also have not experienced any major packet loss or latency. Crews have been able to complete some cable repairs, leading to the stability for the routes mentioned. 
Our team is still observing daily packet loss and latency on network routes between Singapore and New York City, as well as Singapore and Toronto. These are normally short-lived and happen during Singapore business hours when traffic is heavy. 
Repair of all cables from …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TOR1 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/c0dm614gsthg</id>
        <link href="https://status.digitalocean.com/incidents/c0dm614gsthg"/>
        <updated>2023-04-30T19:12:14.000Z</updated>
        <summary type="html"><![CDATA[Apr 30, 19:12 UTC
Resolved - Impact: maintenance;
Scheduled maintenance has completed ahead of schedule, with no known impact to customer traffic. If you experienced any issues or have any questions, please send us a ticket from your cloud support page. Thank you!
Affected Components: Services - Networking Regions - TOR1 
Status: completed
2023-05-03 20:17 UTC
-
Scheduled maintenance is currently in progress. We will provide updates as necessary.
Affected Components: Services - Networking Regions - TOR1 
Status: in_progress
2023-05-03 19:00 UTC
-
Start: 2023-05-03 19:00 UTC 
End: 2023-05-03 22:00 UTC
During the above window, we will be performing maintenance on core network equipment in our TOR1 datacenter as part of network upgrade.
Expected impact:
This maintenance is designed and tested to be seamless and we do not expect any impact to customer traffic during the maintenance period. If an unexpected issue arises, affected Droplets and Droplet-based services would experience a temporary loss of connectivity over the public network. Services in the TOR1 region could also experience issues with management of events such as power on/off, snapshots, and backups etc. We will endeavor to keep any such impact to a minimum.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket
Affected Components: Services - Networking Regions - TOR1 
Status: scheduled
2023-04-30 19:12 UTC
-
Incident Created at: 2023-04-30 19:12 UTC Resolved at: 2023-05-03 20:17 UTC]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack.com links are broken for some users on Enterprise Grid]]></title>
        <id>https://status.slack.com//2023-04/aacc14003e92c232</id>
        <link href="https://status.slack.com//2023-04/aacc14003e92c232"/>
        <updated>2023-04-29T05:09:26.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From April 19, 2023 at 3:05 PM PST until April 28, 2023 at 7:51 PM PST, customers signed in with an Enterprise Grid workspace may have experienced unexpected redirect behaviour when navigating https://slack.com/ links.


We appreciate your patience as we investigated the issue. Our engineering team has implemented a fix to resolve the routing behaviour; however, please reach out to feedback@slack.com if you continue to experience any trouble.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/5h6hwm15zt11</id>
        <link href="https://www.githubstatus.com/incidents/5h6hwm15zt11"/>
        <updated>2023-04-28T12:45:06.000Z</updated>
        <summary type="html"><![CDATA[Apr 28, 12:45 UTC
Resolved - This incident has been resolved.
Apr 28, 12:26 UTC
Investigating - We are investigating reports of degraded availability for Codespaces.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-005]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-005/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-005/"/>
        <updated>2023-04-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: An issue in the Tailscale coordination server in device
reauthentication logic caused previously authenticated and tagged devices to
lose their ACL tags upon reauthentication.
What happened?
The logic that handles the reauthentication to a new identity on an
already-authenticated device with tags had a bug: instead of updating the
device’s logged-in identity to the newly authenticated user, the device’s
identity became that of the user who originally added it to the tailnet, without
any tags.
The bug was introduced on 2022-10-26, and discovered and remediated on
2023-04-21. The bug was discovered when troubleshooting a user-reported issue.
Who is affected?
189 tailnets triggered this bug in the course of normal use of Tailscale, either
directly by explicitly re-authenticating a device, or indirectly by using fast
user switching to switch between multiple
tailnets.
We have notified affected organizations where we have security
contacts.
What is the impact?
Devices that encountered the bug had their tags removed, which reverted the
device’s identity to that of the user who originally authenticated the device,
or the owner of the auth key that was originally used to authenticate the
device. In either case, this is the user listed as “Creator” in the Machines tab
of the admin panel. Depending on access rules in the tailnet policy file, this
could change the device’s network permissions.
We have analyzed the audit logs for affected tailnets, and found no evidence of
deliberate exploitation. In most instances, device owners noticed the incorrect
outcome of reauthentication, and corrected the device’s state themselves.
What do I need to do?
If you were not contacted by Tailscale, no action is required.  If you were
contacted by Tailscale, reapply the desired tags to affected devices in the
admin console, or by reauthenticating the devices. Tailscale has deployed a fix
to the coordination server as of 2023-04-21, and notified affected
organizations.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform and Container Registry]]></title>
        <id>https://status.digitalocean.com/incidents/yhlz9y8mzcb3</id>
        <link href="https://status.digitalocean.com/incidents/yhlz9y8mzcb3"/>
        <updated>2023-04-27T19:42:45.000Z</updated>
        <summary type="html"><![CDATA[Apr 27, 19:42 UTC
Resolved - Impact: minor;
As of 21:34 UTC, Our Engineering team has confirmed that the issue with our App Platform and Container Registry services has been fully resolved. All operations should now be operating normally with these services. If you continue to experience any trouble with these services please open a ticket with our support team. Thank you for your patience and we apologize for the inconvenience.
Affected Components: Services - App Platform Services - Container Registry Regions - Global 
Status: resolved
2023-04-27 21:57 UTC
-
As of 20:44 UTC, Our engineering team has implemented a fix to resolve the issue with our App Platform and Container Registry services and is monitoring the situation closely. 
Users should no longer see any issues while pushing their images to their container registries and App Platform users shouldn't experience any build failures.
We are going to continue to monitor the situation and will post an update once we are confident this issue will not recur.
Affected Components: Services - App Platform Services - Container Registry Regions - Global 
Status: monitoring
2023-04-27 21:08 UTC
-
As of 19:09 UTC, our Engineering team has identified an issue with DigitalOcean Container Registry and App Platform. At this time, users may experience slow responses or errors while interacting and authenticating with the DigitalOcean Container Registry, creating new container registries, and pushing/deleting images to/from registries. Users may also experience issues with App Platform deploys and creates failing. 
We apologize for the inconvenience and will share an update once we have more information.
Affected Components: Services - App Platform Services - Container Registry Regions - Global 
Status: identified
2023-04-27 19:42 UTC
-
Incident Created at: 2023-04-27 19:42 UTC Resolved at: 2023-04-27 21:57 UTC]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
</feed>