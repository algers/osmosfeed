<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-10-17T00:20:59.036Z</id>
    <title>osmos::feed</title>
    <updated>2023-10-17T00:20:59.036Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Mexico SMS Carrier Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/5t8v5hdr7tk2</id>
        <link href="https://status.twilio.com/incidents/5t8v5hdr7tk2"/>
        <updated>2023-10-17T10:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Oct 17, 03:00 - 07:00 PDT
Oct 10, 17:28 PDT
Scheduled - The AT&T network in Mexico is conducting an emergency maintenance from 17 October 2023 at 03:00 PDT until 17 October 2023 at 07:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T Mexico handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/q87dhhqsr38q</id>
        <link href="https://status.twilio.com/incidents/q87dhhqsr38q"/>
        <updated>2023-10-17T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Oct 16, 21:00 - 21:30 PDT
Oct 12, 09:24 PDT
Scheduled - Our SMS carrier partner in Italy and Greece is conducting a planned maintenance from 16 October 2023 at 21:00 PDT until 16 October 2023 at 21:30 PDT. During the maintenance window, there could be intermittent delays delivering SMS to Italy and Greece handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/1fpk2lxzbvtz</id>
        <link href="https://status.twilio.com/incidents/1fpk2lxzbvtz"/>
        <updated>2023-10-17T02:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Oct 16, 19:00 PDT  -  Oct 17, 03:00 PDT
Oct 12, 18:34 PDT
Scheduled - The AT&T network in the US is conducting a planned maintenance from 16 October at 19:00 PDT until 17 October at 03:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T US handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to One Nz Vodafone Network in New Zealand]]></title>
        <id>https://status.twilio.com/incidents/cdc2jz2n6ysv</id>
        <link href="https://status.twilio.com/incidents/cdc2jz2n6ysv"/>
        <updated>2023-10-17T00:18:42.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 17:18 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to One Nz Vodafone Network in New Zealand. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 16:52 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to One Nz Vodafone Network in New Zealand. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sms Delivery Delays to Orange Network in Cameroon]]></title>
        <id>https://status.twilio.com/incidents/h5ghwmx5hv29</id>
        <link href="https://status.twilio.com/incidents/h5ghwmx5hv29"/>
        <updated>2023-10-16T23:29:04.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 16:29 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Orange Network in Cameroon. We will continue monitoring the service to ensure a full recovery. We will provide another update in 4 hours or as soon as more information becomes available.
Oct 16, 14:38 PDT
Update - We are experiencing SMS delivery delays when sending messages to Orange Network in Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 13:40 PDT
Update - We are experiencing SMS delivery delays when sending messages to Orange Network in Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Oct 16, 13:28 PDT
Investigating - Our monitoring systems have detected a potential issue Orange Network in Cameroon. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Telekom Network in Romania]]></title>
        <id>https://status.twilio.com/incidents/pcs2727d851k</id>
        <link href="https://status.twilio.com/incidents/pcs2727d851k"/>
        <updated>2023-10-16T23:22:15.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 16:22 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to Telekom Network in Romania. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DigitalOcean Control Panel and API]]></title>
        <id>https://status.digitalocean.com/incidents/yxfmyjl0kmpr</id>
        <link href="https://status.digitalocean.com/incidents/yxfmyjl0kmpr"/>
        <updated>2023-10-16T23:14:32.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 23:14 UTC
Resolved - From 22:45 to 23:00 UTC, our Engineering team has reported an issue with DigitalOcean Control Panel and API.
 During that time, Customers may have experienced intermittent timeout errors while using DigitalOcean Control Panel and API. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kazakhstan SMS Carrier Maintenance - K-Cell]]></title>
        <id>https://status.twilio.com/incidents/jqggn9nfwnff</id>
        <link href="https://status.twilio.com/incidents/jqggn9nfwnff"/>
        <updated>2023-10-16T23:02:26.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 16:02 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 16, 02:27 PDT
Scheduled - The K-Cell network in Kazakhstan is conducting an emergency maintenance from 16 October 2023 at 16:00 PDT until 16 October 2023 at 19:00 PDT.During the maintenance window, there could be intermittent delays delivering SMS to K-Cell Kazakhstan handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays To Algar Telecom Network in Brazil]]></title>
        <id>https://status.twilio.com/incidents/rnqg1jsdy4lg</id>
        <link href="https://status.twilio.com/incidents/rnqg1jsdy4lg"/>
        <updated>2023-10-16T22:40:06.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 15:40 PDT
Update - We are experiencing SMS delivery delays when sending messages to Algar Telecom Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 14:23 PDT
Update - We are experiencing SMS delivery delays when sending messages to Algar Telecom Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Oct 16, 14:11 PDT
Investigating - Our monitoring systems have detected a potential issue Telecom Network Brazil. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UK SMS Carrier Partner Maintenance- O2 UK]]></title>
        <id>https://status.twilio.com/incidents/w977sylyz6n5</id>
        <link href="https://status.twilio.com/incidents/w977sylyz6n5"/>
        <updated>2023-10-16T22:00:28.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 15:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 13, 09:43 PDT
Scheduled - Our SMS carrier partner in UK is conducting a series of planned maintenances from 16 October 2023 at 15:00 PDT until 19 October 2023 at 17:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from multiple network's UK handsets. 

Note, the maintenance will be carried out on each of the following dates and times:

16 October 2023 at 15:00 PDT until 16 October 2023 at 17:00 PDT.

17 October 2023 at 15:00 PDT until 17 October 2023 at 17:00 PDT.

19 October 2023 at 15:00 PDT until 19 October 2023 at 17:00 PDT.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Moov Network in Ivory Coast]]></title>
        <id>https://status.twilio.com/incidents/8s4h08xwj2bv</id>
        <link href="https://status.twilio.com/incidents/8s4h08xwj2bv"/>
        <updated>2023-10-16T20:09:12.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 13:09 PDT
Resolved - We are observing recovery in SMS delivery delays when sending messages to Moov Network in Ivory Coast. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 11:01 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Moov Network in Ivory Coast. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 07:02 PDT
Update - We continue to experience SMS delivery delays when sending messages to Moov Network in Ivory Coast. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Oct 16, 05:02 PDT
Update - We continue to experience SMS delivery delays when sending messages to Moov Network in Ivory Coast. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 16, 04:08 PDT
Update - We are experiencing SMS delivery delays when sending messages to Moov Network in Ivory Coast. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 Hour or as soon as more information becomes available.
Oct 16, 04:03 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to Moov Network in Ivory Coast. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 Hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Japan SMS Carrier Maintenance - Rakuten Mobile]]></title>
        <id>https://status.twilio.com/incidents/x5dd2kpcx75q</id>
        <link href="https://status.twilio.com/incidents/x5dd2kpcx75q"/>
        <updated>2023-10-16T20:00:10.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 13:00 PDT
Completed - The scheduled maintenance has been completed.
Oct 16, 06:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct  5, 02:17 PDT
Scheduled - The Rakuten Mobile network in Japan is conducting a planned maintenance from 16 October 2023 at 06:00 PDT until 16 October 2023 at 13:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from Rakuten Mobile Japan handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Latency in SFO Region]]></title>
        <id>https://status.digitalocean.com/incidents/k58m2k4qcchf</id>
        <link href="https://status.digitalocean.com/incidents/k58m2k4qcchf"/>
        <updated>2023-10-15T01:04:23.000Z</updated>
        <summary type="html"><![CDATA[Oct 15, 01:04 UTC
Resolved - Our Engineering team has confirmed the full resolution of the networking issue in the SFO regions. If you continue to see issues with latency or packet loss in the SFO region please reach out directly to our support team for assistance.
Oct 14, 20:30 UTC
Update - Our Engineering team is continuing to monitor the networking issue in the SFO regions. So far, we haven't observed any major spike in latency or packet loss with network connections going in or out of the SFO regions. However, we will post an update as soon as the issue is fully resolved.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 14, 15:25 UTC
Monitoring - Our Engineering team has detected a recurrence of the networking issue identified in a previous incident today: 
https://status.digitalocean.com/incidents/bhpslzd37517
Our team is actively monitoring the situation and has implemented traffic routing changes where applicable to alleviate the latency. Some users may still experience packet loss or increased latency accessing the resources in the SFO region from certain ISPs.
We apologize for any inconvenience caused and will provide updates as the situation progresses.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Latency in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/bhpslzd37517</id>
        <link href="https://status.digitalocean.com/incidents/bhpslzd37517"/>
        <updated>2023-10-14T11:46:50.000Z</updated>
        <summary type="html"><![CDATA[Oct 14, 11:46 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with networking in affected regions. Users should no longer experience timeouts or delays when connecting to or from these regions. 
If you continue to experience problems, please open a ticket with our support team.
Thank you for your patience and we apologize for any inconvenience.
Oct 14, 11:10 UTC
Monitoring - As of 10:55 UTC, our Engineering team has implemented a fix to address the networking problem in the affected regions and is currently monitoring the situation. Users should no longer face timeouts or encounter delays when connecting to or from these regions. We will post an update as soon as the issue is fully resolved.
Oct 14, 10:30 UTC
Update - As of 10:27 UTC, our Engineering team is continuing to investigate an issue with networking in our SFO regions. Additionally, it has come to our attention that this issue has affected other regions, specifically SGP1, SYD1, and NYC3. Users may encounter timeouts or experience delays in network connections going in and out of these regions. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 14, 09:47 UTC
Update - We are continuing to investigate this issue.
Oct 14, 09:12 UTC
Investigating - As of 8:30 UTC, our Engineering team is investigating an issue with networking in our SFO regions. During this time, users may experience timeouts or latency with network connections going in or out of the SFO regions. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/lh3j71zp57hx</id>
        <link href="https://status.digitalocean.com/incidents/lh3j71zp57hx"/>
        <updated>2023-10-12T19:17:29.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 19:17 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue that impacted the Spaces CDN. Objects should be accessible over the CDN endpoint without any issues. However, HTTP/2 is temporarily unavailable due to upstream issues, and due to this HTTP/2 requests should automatically be re-negotiated to 1.1, but in case you experience failures please open a ticket with our support team.
Thank you for your patience and we apologize for any inconvenience.
Oct 12, 16:10 UTC
Update - After our upstream provider implemented a remediation step to resolve the issue with the Spaces CDN, the Spaces CDN is serving the objects stored in the Spaces bucket without any errors or performance issues. However, we are still monitoring the situation closely and we'll share more in…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing an issue with SAML SSO login]]></title>
        <id>https://status.notion.so/incidents/55frj2c42yvq</id>
        <link href="https://status.notion.so/incidents/55frj2c42yvq"/>
        <updated>2023-10-12T18:14:45.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 11:14 PDT
Resolved - Users were unable to log in using the Notion mobile and desktop apps to enterprise workspaces requiring SAML SSO login. This issue is resolved now.
Oct 12, 11:10 PDT
Update - We are experiencing an issue with SAML SSO login in the Notion mobile and desktop apps, and we are investigating the cause.
Oct 12, 11:05 PDT
Investigating - We are experiencing an issue with SAML SSO and we are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Users cannot clone nor move scenarios]]></title>
        <id>https://status.make.com/incidents/2c52p236t4rp</id>
        <link href="https://status.make.com/incidents/2c52p236t4rp"/>
        <updated>2023-10-12T15:40:47.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 17:40 CEST
Resolved - This incident has been resolved.
Oct 12, 16:21 CEST
Monitoring - We implemented workaround and we are currently monitoring the issue.
Oct 12, 16:09 CEST
Investigating - We are experiencing issues within our scenario features i.e. users cannot clone scenario nor move it to a folder inside list of scenarios. This is still possible when user open particular scenario.
We are currently working on implementing a quick workaround, we should publish the relevant steps within the next 30 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes Service]]></title>
        <id>https://status.digitalocean.com/incidents/fsfsv9fj43w7</id>
        <link href="https://status.digitalocean.com/incidents/fsfsv9fj43w7"/>
        <updated>2023-10-11T07:26:53.000Z</updated>
        <summary type="html"><![CDATA[Oct 11, 07:26 UTC
Resolved - Our Engineering team has resolved the issue with the Managed Kubernetes Service. A daemonset has been released to all existing clusters eliminating the auto-update process and it will be removed again going forward. If you find worker nodes that could still be affected by a prior occurrence of this incident, please replace them for permanent mitigation.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Oct 11, 04:06 UTC
Monitoring - Our Engineering team has completed the work for both items. New images have been released that eliminate the auto-update process and the daemonset has been applied to all existing clusters. 
Given this, the team is not expecting to see a recurrence of this inciden…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/gtsz1l2jc96n</id>
        <link href="https://www.githubstatus.com/incidents/gtsz1l2jc96n"/>
        <updated>2023-10-09T15:18:49.000Z</updated>
        <summary type="html"><![CDATA[Oct  9, 15:18 UTC
Resolved - This incident has been resolved.
Oct  9, 14:52 UTC
Update - We are investigating delays for commits showing up on Pull Requests page loads in the web UI. As a result of this,  about 20% of pull requests are currently showing stale data of up-to 7m. We are currently investigating contributing factors right now.
Oct  9, 14:51 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: **Summary**
Google Keep users are experiencing issues with notification emails when a note is shared.
**Description:**
We are experiencing an issue with Google Keep beginning on Wednesday, 2023-09-27. Mitigation work is underway by our engineering team. We do not have an ETA for mitigation at this point. We will provide more information by Wednesday, 2023-10-04 10:00 US/Pacific.
**Diagnosis**
Google Keep users are experiencing an issue where notification emails about note sharing are ending up in receiver's Spam folder
**Workaround**
None at this time]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM"/>
        <updated>2023-10-09T11:49:19.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-09-28 07:00</strong> and ended at <strong>2023-10-04 18:22</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced an impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 28 Sept 2023 00:00</p>
<p><strong>Incident End:</strong> 4 Oct 2023 11:22</p>
<p><strong>Duration:</strong>  6 days, 11 hours, 22 minutes</p>
<p><strong>Affected Services and Features:</strong></p>
<p>Google Keep - Email notifications</p>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Google Keep experienced an issue where notification emails about note sharing ended up in the receiver&#39;s Spam folder for a duration of 6 days, 11 hours and 22 minutes. From preliminary analysis, the root cause is an unexpected increase in notifications.</p>
<p><strong>Customer Impact:</strong></p>
<p>Google Keep users incorrectly received email notifications in their spam folder.</p>
</div><hr><p>Affected products: Google Keep</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces listing in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/08q0d3g8dtp3</id>
        <link href="https://status.digitalocean.com/incidents/08q0d3g8dtp3"/>
        <updated>2023-10-07T20:01:58.000Z</updated>
        <summary type="html"><![CDATA[Oct  7, 20:01 UTC
Resolved - As of 19:52 UTC, our Engineering team has confirmed the full resolution of the issue impacting the listing of Spaces buckets in the FRA1 region via the Cloud Control Panel. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Oct  7, 19:38 UTC
Monitoring - Our engineering team has implemented a fix to resolve the issue with listing Spaces in our FRA1 region via the Cloud Panel and is monitoring the situation. We will post an update as soon as the issue is fully resolved.
Oct  7, 19:15 UTC
Investigating - Our Engineering team is investigating an issue with Spaces in our FRA1 region. As of 13:40 UTC, users may experience issues listing the Spaces created in the FRA1 region via the Cloud Panel. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Users are having trouble connecting to Slack]]></title>
        <id>https://status.slack.com//2023-10/ad8f0e62516e8812</id>
        <link href="https://status.slack.com//2023-10/ad8f0e62516e8812"/>
        <updated>2023-10-06T15:28:47.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:


On Friday, October 6 2023, from 1:52 AM PDT to 2:12 AM PDT, some users were unable to load Slack. This was caused by an issue where certain backend Slack processes were pulling data from our database rather than their cache which caused strain on our servers.


We rolled out a change to how these processes load data, and the issue was resolved for all users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests]]></title>
        <id>https://www.githubstatus.com/incidents/w554d74dv18j</id>
        <link href="https://www.githubstatus.com/incidents/w554d74dv18j"/>
        <updated>2023-10-06T02:59:36.000Z</updated>
        <summary type="html"><![CDATA[Oct  6, 02:59 UTC
Resolved - We deployed a new configuration to improve our network availability. This resulted in a small percentage of user traffic getting incorrectly blocked, but missed by our automated detections. We mitigated this with a change to the configuration, rolled out slowly over the last hour of this incident time for safe deployment. Beyond the learnings related to the config, we are analyzing how we can more quickly detect this kind of impact as part of future configuration rollouts.
Oct  6, 02:42 UTC
Update - We have confirmed that the fix has resolved the issue in the subset of regions where it has been deployed. We are now continuing the deployment to the remaining regions.
Oct  6, 02:03 UTC
Update - We are monitoring the rollout of the fix and are beginning to see signs of improvement. We will send another update shortly.
Oct  6, 01:31 UTC
Update - A small number of customers are experiencing 403 errors when attempting to access repository data via the API. We have found what we believe to be the cause of the issue and are deploying a fix.
Oct  6, 01:12 UTC
Investigating - We are investigating reports of degraded performance for API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session expiration issues]]></title>
        <id>https://status.make.com/incidents/jjzrxzv3vfj5</id>
        <link href="https://status.make.com/incidents/jjzrxzv3vfj5"/>
        <updated>2023-10-05T16:58:09.000Z</updated>
        <summary type="html"><![CDATA[Oct  5, 18:58 CEST
Resolved - This incident has been resolved.
Oct  5, 13:23 CEST
Monitoring - We have identified that cause of the issue, users should no longer experience unexpected session drops. We will be monitoring the situation for the next few hours.
Oct  5, 12:05 CEST
Investigating - We encountered issues related to user sessions, so users may experience that their session might be disconnected after a few minutes. We are currently investigating the issue and we will provide a new update in the next hour.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/9fk4c2w43clz</id>
        <link href="https://www.githubstatus.com/incidents/9fk4c2w43clz"/>
        <updated>2023-10-05T16:42:15.000Z</updated>
        <summary type="html"><![CDATA[Oct  5, 16:42 UTC
Resolved - This incident has been resolved.
Oct  5, 16:12 UTC
Update - We've implemented a fix for Pull Request web UI delays and are seeing recovery. We are monitoring and will send another update in a few minutes.
Oct  5, 16:01 UTC
Update - We are still investigating delays of up to 15 minutes in commits showing up on Pull Requests in the Web UI for all customers.
Oct  5, 15:20 UTC
Update - We are continuing to investigate the delay of commits showing up on Pull Requests in the Web UI. Pull Requests will still function normally otherwise.
Oct  5, 14:34 UTC
Update - We are investigating delays for commits showing up on Pull Requests page loads in the web UI. As a result of this,  about 15% of pull requests are currently showing stale data. We are currently investigating contributing factors right now.
Oct  5, 14:18 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble connecting to Slack]]></title>
        <id>https://status.slack.com//2023-10/6d592537eb7bd98e</id>
        <link href="https://status.slack.com//2023-10/6d592537eb7bd98e"/>
        <updated>2023-10-05T13:53:12.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On Oct 4, 2023 from 11:15 PM PDT to around 11:58 PM PDT a small subset of users experienced trouble connecting to Slack and sending and receiving messages.


A code change optimizing resource usage caused a core service to restart causing brief connectivity issues for a few users.


The service recovered automatically and we’ve put measures in place to prevent this from happening in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with loading some items on Slack]]></title>
        <id>https://status.slack.com//2023-10/ee32d13012fb29d5</id>
        <link href="https://status.slack.com//2023-10/ee32d13012fb29d5"/>
        <updated>2023-10-04T18:53:01.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On October 3, 2023 from 4:00 PM PDT to 4:20 PM PDT, some customers had trouble loading threads, mentions, reactions and search results in Slack.


We determined that this was caused during the process of changing our database hosts on our backend. To mitigate the issue, we stopped the change which fixed the issue for affected customers.


We're sorry for any disruption this may have caused and thank you for your patience while we looked into this]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/18rxsrdk6zkz</id>
        <link href="https://status.notion.so/incidents/18rxsrdk6zkz"/>
        <updated>2023-10-04T17:52:33.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 10:52 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  4, 09:28 PDT
Monitoring - We've rolling out a fix and will continue monitoring.
Oct  4, 09:04 PDT
Identified - We have implemented a fix and rolling out.
Oct  4, 08:32 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Error on GET Opportunities API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/kkvp54r30jdn</id>
        <link href="https://airbnbapi.statuspage.io/incidents/kkvp54r30jdn"/>
        <updated>2023-10-04T16:46:35.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 09:46 PDT
Resolved - This incident has been resolved.
Oct  3, 22:04 PDT
Monitoring - There was an increased number of 500 errors on the GET Opportunities API endpoint. We've mitigated the issue and will continue to monitor it. The time window for these errors was between 9:00 PM and 9:50 PM (PDT) today (Oct 3, 2023)]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/dknkp34qzxg0</id>
        <link href="https://status.notion.so/incidents/dknkp34qzxg0"/>
        <updated>2023-10-04T13:49:09.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 06:49 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  4, 05:31 PDT
Monitoring - We're rolling out a fix and will continue monitoring.
Oct  4, 05:12 PDT
Update - We have implemented a fix and rolling out soon.
Oct  4, 04:58 PDT
Identified - We have identified the issue and are working on a fix.
Oct  4, 04:46 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue Delaying Webhook Deliveries]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/rhkqlr25pfcs</id>
        <link href="https://airbnbapi.statuspage.io/incidents/rhkqlr25pfcs"/>
        <updated>2023-10-03T17:21:45.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:21 PDT
Resolved - This incident has been resolved.
Oct  2, 10:44 PDT
Monitoring - We're currently investigating an issue that's causing spikes in our delivery timelines for asynchronous webhook notifications. It began around 5PM PST on Friday, September 29, and briefly reoccured on both Saturday and Sunday afternoon. We've mitigated the issue for now, but will continue to monitor this over the next few days.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basic Auth Errors for Synchronous Availability Checks and Webhook Notifications]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/6zcksbcfmvsb</id>
        <link href="https://airbnbapi.statuspage.io/incidents/6zcksbcfmvsb"/>
        <updated>2023-10-03T17:21:27.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:21 PDT
Resolved - This incident has been resolved.
Oct  2, 14:30 PDT
Monitoring - There was an issue with the generation of Basic Auth headers for synchronous availability checks and asynchronous webhook notifications between 11:45 AM and 2:00 PM PDT today. This resulted in errors for both availability checks and webhooks.
The issue has been resolved, and our team will be monitoring any further errors.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevated Server Error Rates on Async Calendars API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/7d87njvtk91g</id>
        <link href="https://airbnbapi.statuspage.io/incidents/7d87njvtk91g"/>
        <updated>2023-10-03T17:20:56.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:20 PDT
Resolved - This incident has been resolved. Error rates have returned to normal levels. Please retry any failed requests.
Oct  2, 19:51 PDT
Monitoring - We have mitigated the issue and will continue monitoring it. The time window for the errors was between 5:45 PM and 7:00 PM PDT.
Oct  2, 18:34 PDT
Investigating - We are currently investigating elevated error rates when processing the Async Calendars API update requests.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/gxjt1jbwqs7m</id>
        <link href="https://status.notion.so/incidents/gxjt1jbwqs7m"/>
        <updated>2023-10-03T00:13:08.000Z</updated>
        <summary type="html"><![CDATA[Oct  2, 17:13 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  2, 16:54 PDT
Identified - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/b3g5shlj6hhl</id>
        <link href="https://status.notion.so/incidents/b3g5shlj6hhl"/>
        <updated>2023-10-02T23:03:47.000Z</updated>
        <summary type="html"><![CDATA[Oct  2, 16:03 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  2, 14:28 PDT
Update - We're continuing to monitor the impact of the fix we implemented. We expect it may take a few hours to fully complete.
Oct  2, 13:54 PDT
Monitoring - We're rolling out a fix and will continue monitoring.
Oct  2, 13:02 PDT
Update - We have developed a fix and will be implementing soon.
Oct  2, 06:40 PDT
Identified - We have identified the issue and are working on a fix.
Oct  2, 06:24 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Threads were not loading for some users]]></title>
        <id>https://status.slack.com//2023-10/101eb09455dcaa9a</id>
        <link href="https://status.slack.com//2023-10/101eb09455dcaa9a"/>
        <updated>2023-10-02T20:16:28.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On October 2, 2023 from 12:18PM PDT to 12:26PM PDT, some users noticed that threads were not loading correctly.


We identified a database issue on our backend that was causing the threads to not load. The issue was mitigated and users should no longer experience trouble loading threads.


We are investigating the cause of the database issue so we can ensure this does not happen again.


We apologize for any interruption to your work day!]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CDN Insights for App Platform]]></title>
        <id>https://status.digitalocean.com/incidents/bm43bydmrmbl</id>
        <link href="https://status.digitalocean.com/incidents/bm43bydmrmbl"/>
        <updated>2023-10-01T18:39:16.000Z</updated>
        <summary type="html"><![CDATA[Oct  1, 18:39 UTC
Resolved - From 14:44 UTC to 18:04 UTC we experienced another issue with our App Platform service related to CDN Insights for Apps in all regions. During this window, users might have observed blank graphs or no data for the CDN metrics including request counts, cache hit rates etc. Our engineering team was able to identify the problem and resolve it fully and as of 18:04 UTC all the metrics are displaying properly. 
The connectivity issue with the Apps for which we updated the status page earlier was only faced by a subset of users in FRA1 region, only between 12:45 UTC to 14:15 UTC. The normal functionality of apps was entirely restored around 14:15 UTC.
Thank you for your patience on this and if you continue to face issues then please open a support ticket from within the cloud control panel.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/hzsrymxjyvys</id>
        <link href="https://status.digitalocean.com/incidents/hzsrymxjyvys"/>
        <updated>2023-10-01T15:24:05.000Z</updated>
        <summary type="html"><![CDATA[Oct  1, 15:24 UTC
Resolved - Between 12:45 UTC and 14:15 UTC, we experienced an issue impacting the App platform apps in the FRA1 region. During this time, a subset of users might have experienced degraded performance or errors while accessing their apps in the region.
Our engineering team was able to take quick action to mitigate the impact and resolve the issue, and as of 14:15 UTC all services are now functioning normally. Thank you for your patience, and we apologize for any inconvenience. 
If you continue to experience any issues, please open a Support ticket right away for further review.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/vjypl8576bp3</id>
        <link href="https://status.digitalocean.com/incidents/vjypl8576bp3"/>
        <updated>2023-09-30T14:08:21.000Z</updated>
        <summary type="html"><![CDATA[Sep 30, 14:08 UTC
Resolved - Our engineering team has resolved the issue impacting Spaces in our SGP1 region. From approximately 11:47 UTC - 13:35 UTC, users may have encountered errors with API or object requests, experienced difficulties in creating new buckets in SGP1, or faced issues with loading Spaces in the Cloud Control Panel. Spaces should now be operating normally. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Sep 30, 13:51 UTC
Monitoring - Our engineering team has implemented a fix to resolve the issue with Spaces in our SGP1 region and is monitoring the situation. We will post an update as soon as the issue is fully resolved.
Sep 30, 13:43 UTC
Identified - As of 13:31 UTC, our engineering team has identified the cause of the issue with Spaces availability in our SGP1 region and is actively working on a fix. We will post an update as soon as additional information is available.
Sep 30, 12:29 UTC
Investigating - Our Engineering team is investigating an issue with Spaces in our SGP1 region. As of 11:47 UTC, users may experience issues with accessing Spaces and see request errors. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with receiving 2FA code]]></title>
        <id>https://status.slack.com//2023-09/7e84b360042dd07b</id>
        <link href="https://status.slack.com//2023-09/7e84b360042dd07b"/>
        <updated>2023-09-29T22:55:29.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On September 28, 2023 from approximately 11:54 AM PDT to 3:15 PM PDT, some Slack users experienced trouble receiving two-factor authentication (2FA) codes and therefore were unable to sign into Slack.


We determined this was caused by an upstream 2FA provider issue impacting code deliveries to US-based AT&T and Verizon devices. We monitored communications from the provider and confirmed Slack users could successfully sign in once the upstream issue was resolved.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degraded performance for public API]]></title>
        <id>https://status.notion.so/incidents/b782prxr5jw8</id>
        <link href="https://status.notion.so/incidents/b782prxr5jw8"/>
        <updated>2023-09-29T22:23:14.000Z</updated>
        <summary type="html"><![CDATA[Sep 29, 15:23 PDT
Resolved - The issue has been resolved.
Sep 29, 14:53 PDT
Monitoring - We have rolled out a fix and are seeing error rates and latencies back to normal levels. We will continue to monitor before resolving.
Sep 29, 13:08 PDT
Update - We are seeing reduced latencies and error rates, and continue our work to bring them down to normal levels.
Sep 29, 11:36 PDT
Identified - We have identified the issue and are working towards a fix.
Sep 29, 10:59 PDT
Investigating - We're seeing increased latency and some degraded performance with the public API. We've partially identified the issue, and are still investigating.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slow App Platform Builds]]></title>
        <id>https://status.digitalocean.com/incidents/5khjfm2dbz08</id>
        <link href="https://status.digitalocean.com/incidents/5khjfm2dbz08"/>
        <updated>2023-09-29T15:45:14.000Z</updated>
        <summary type="html"><![CDATA[Sep 29, 15:45 UTC
Resolved - As of 03:24 PM UTC, Our engineering team has resolved the issue impacting our App Platform builds. Everything should be functioning normally. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Sep 29, 14:21 UTC
Monitoring - As of 02:18 PM UTC, our Engineering team has implemented a fix to resolve the issue impacting App platform builds. We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. Thank you for your patience and we apologize for the inconvenience.
Sep 29, 12:12 UTC
Investigating - As of 9:00UTC, our Engineering team is investigating an issue with slow App Platform builds across all regions. During this time users may encounter delays in app builds and could potentially experience timeout errors in builds as a result. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: A small number of users experience latency with Slack]]></title>
        <id>https://status.slack.com//2023-09/94046787b0ca9008</id>
        <link href="https://status.slack.com//2023-09/94046787b0ca9008"/>
        <updated>2023-09-29T00:49:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 8:00 AM PDT to 11:00 AM PDT on Thursday, September 28, 2023, a small number of users experienced delays in Slack, including sending and editing messages, and loading channels.


Upon investigation, we determined that a specific task in our backend was producing too many file deletion events and overloading our servers, resulting in these delays.


We implemented a fix to reduce the load on our servers and this resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Create app on api.slack.com/apps not working]]></title>
        <id>https://status.slack.com//2023-09/054c6ac24cb629dd</id>
        <link href="https://status.slack.com//2023-09/054c6ac24cb629dd"/>
        <updated>2023-09-28T21:54:37.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On September 27, 2023 from 9:01 AM PDT to 12:32 PM PDT, some users experienced issues when attempting to create or edit custom apps on https://api.slack.com/apps, including seeing a "404 error" on this website.


We traced this back to a recent change that removed a function necessary for rendering a documentation page associated with the website. 


We reverted the change, restoring the documentation page rendering component on https://api.slack.com/apps. Users should no longer have issues creating or editing custom apps and accessing this website.


Note: Due to an internal tooling error, we initially reported the times in this summary in UTC, but labeled with PDT. We have corrected this mistake and apologize for any confusion caused.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Static Site components for the App Platform in NYC3 region]]></title>
        <id>https://status.digitalocean.com/incidents/7bjzfp9n2df6</id>
        <link href="https://status.digitalocean.com/incidents/7bjzfp9n2df6"/>
        <updated>2023-09-28T19:29:46.000Z</updated>
        <summary type="html"><![CDATA[Sep 28, 19:29 UTC
Resolved - Our Engineering team has confirmed that the issue with Static Site components for the App Platform in our NYC3 region has been fully resolved.
We apologize for the inconvenience. If you continue to experience any issues related to this incident, please open a ticket with our support team from within your Cloud Control Panel.
Sep 28, 19:00 UTC
Monitoring - Our Engineering team identified an issue with Static Site components for the App Platform in our NYC3 region. From 17:28 UTC to 18:45 UTC, some users experienced 404 errors for their static sites after deploying a new app, or after redeploying an existing app. 
A fix has now been implemented and all Apps should be functioning as expected. We're monitoring the situation and will post a final update once we confirm this is fully resolved.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion app & API are down]]></title>
        <id>https://status.notion.so/incidents/gg738l5v2yyn</id>
        <link href="https://status.notion.so/incidents/gg738l5v2yyn"/>
        <updated>2023-09-27T23:20:28.000Z</updated>
        <summary type="html"><![CDATA[Sep 27, 16:20 PDT
Resolved - The issue has been resolved, and Notion is available for all users.
Sep 27, 15:39 PDT
Update - Notion should be back online for most users; we are confirming.
Sep 27, 15:35 PDT
Update - We are continuing to investigate this issue.
Sep 27, 15:14 PDT
Investigating - The Notion App and API are experiencing some degraded performance. We are currently investigating.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[macOS Sonoma update is shown as a minor update in Rippling]]></title>
        <id>https://status.rippling.com/incidents/hqbr4jpwf48y</id>
        <link href="https://status.rippling.com/incidents/hqbr4jpwf48y"/>
        <updated>2023-09-27T19:48:47.000Z</updated>
        <summary type="html"><![CDATA[Sep 27, 19:48 UTC
Resolved - This incident has been resolved.
Sep 27, 19:29 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Sep 27, 18:31 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Creation in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/ygpk1jjf2pb7</id>
        <link href="https://status.digitalocean.com/incidents/ygpk1jjf2pb7"/>
        <updated>2023-09-27T19:42:00.000Z</updated>
        <summary type="html"><![CDATA[Sep 27, 19:42 UTC
Resolved - From 17:37 - 18:30 UTC, Spaces bucket creation in our NYC3 region failed. 
Our Engineering team has confirmed full resolution of the issue and all services are now operating normally. If you continue to experience any issues, please open a Support ticket from within your account. Thank you.
Sep 27, 19:02 UTC
Monitoring - Our Engineering team has completed a rollback of a recent change to resolve the issue with Spaces bucket creations in NYC3. Users should now be able to create buckets normally. 
We are monitoring the situation and will post an update as soon as we confirm the issue is fully resolved.
Sep 27, 18:49 UTC
Identified - Our Engineering team has identified an issue with creating Spaces buckets in our NYC3 region and is currently working on a fix to mitigate the issue. At this time, users will see 500 errors when attempting to create buckets in NYC3. 
We'll provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Can't dismiss Now you've got Later]]></title>
        <id>https://status.slack.com//2023-09/badc5543a21e1fa7</id>
        <link href="https://status.slack.com//2023-09/badc5543a21e1fa7"/>
        <updated>2023-09-27T17:24:03.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On September 26, 2023 from 10:00 AM PDT to 12:00 PM PDT, some users on our new interface design couldn't dismiss an information prompt that read "You now have Later". 


These users also experienced their UI locking, and were unable to click anything in the Slack app. For some, the sparkle animation fixed over the Later icon in the sidebar also persisted longer than intended. 


We identified the root cause as a missing detail in a recent code change. We reverted the change to mitigate the immediate issue, then deployed a fix to fully resolve the problem for all impacted users.


Note: Due to an internal tooling error, we initially reported the times in this summary in UTC, but labeled with PDT. We have corrected this mistake and apologize for any confusion caused.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creation/Deletion of Uptime Checks (Monitoring)]]></title>
        <id>https://status.digitalocean.com/incidents/s9wqn2fztjly</id>
        <link href="https://status.digitalocean.com/incidents/s9wqn2fztjly"/>
        <updated>2023-09-27T16:05:43.000Z</updated>
        <summary type="html"><![CDATA[Sep 27, 16:05 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with creating and deleting uptime checks. If you continue to face any errors, please open a support ticket from within your account. Thank you!
Sep 27, 15:33 UTC
Monitoring - Our Engineering team identified the root cause of the issue with creating and deleting uptime checks and has implemented a fix. At this time, users should be able to create and delete checks normally. 
We're monitoring the implemented fix and will post a final update once we confirm this issue is fully resolved.
Sep 27, 14:55 UTC
Investigating - Our Engineering team is investigating an issue with creating uptime checks. At this time, users may experience failures when creating or deleting checks. 
We'll provide an update with further information as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/tpykx175y6jt</id>
        <link href="https://www.githubstatus.com/incidents/tpykx175y6jt"/>
        <updated>2023-09-27T00:58:42.000Z</updated>
        <summary type="html"><![CDATA[Sep 27, 00:58 UTC
Resolved - On September 27, 2023 at 00:12 UTC, our alerting systems detected an increase in the time it took GitHub Actions workflow runs to start. During the incident, some customers experienced delays in starting Github Actions workflow runs and receiving status updates for in-progress runs. The root cause was identified to be a change that was deployed to an internal distributed event streaming platform which resulted in several worker nodes to go over a misconfigured memory limit. This caused these nodes to restart leading to a reduced job processing throughput. Github Actions relies on events delivered through this event streaming platform to start workflow runs and update their status. Delays in receiving these events led to run delays for about 40% of the Actions workflows.
	We mitigated this through a rollback of the offending change at 00:18 UTC. This allowed our event streaming platform to catch up with the backlog of workflow runs that were queued during the incident. The backlog was processed by 00:44 UTC. We have additional repair items in place to prevent a recurrence in the future.
Sep 27, 00:23 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/4wyj77ll2bxl</id>
        <link href="https://status.digitalocean.com/incidents/4wyj77ll2bxl"/>
        <updated>2023-09-26T23:45:29.000Z</updated>
        <summary type="html"><![CDATA[Sep 26, 23:45 UTC
Resolved - Our Engineering team has confirmed that the issue with our Container Registry services in the NYC3 region has been fully resolved. 
All operations should now be operating normally with our Container Registry services. If you continue to experience any trouble with these services please open a ticket with our support team. 
Thank you for your patience and we apologize for the inconvenience.
Sep 26, 17:15 UTC
Identified - While monitoring the implemented fix, our Engineering team identified additional customer impact and are working on an additional fix. At this time, users may still experience authorization issues and errors when interacting with registries.
We will provide an additional update as soon as we have more information.
Sep 26, 16:13 UTC
Monitoring - …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with custom emoji]]></title>
        <id>https://status.slack.com//2023-09/8315491ac04ad325</id>
        <link href="https://status.slack.com//2023-09/8315491ac04ad325"/>
        <updated>2023-09-26T20:28:49.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

From 12:54 PM PDT on September 22, 2023 to 09:13 AM PDT on September 25, 2023, custom emoji rendered as text for a small number of Enterprise Grid users.


We made a code change that inadvertently introduced unnecessary logging for routine emoji events. This unexpected logging increased traffic to the custom emoji infrastructure, impacting its performance. 


We deployed a fix to remove the unnecessary logging, which reduced traffic to the custom emoji systems and restored normal emoji rendering. 


A small fraction of the affected customers may have experienced lingering issues as the fix was gradually rolled out to everyone.


Note: Due to an internal tooling error, we initially reported the times in this summary in UTC, but labeled with PDT. We have corrected this mistake and apologize for any confusion caused.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Store application Issue]]></title>
        <id>https://status.make.com/incidents/5r55sspmpx14</id>
        <link href="https://status.make.com/incidents/5r55sspmpx14"/>
        <updated>2023-09-25T12:26:20.000Z</updated>
        <summary type="html"><![CDATA[Sep 25, 14:26 CEST
Resolved - This incident has been resolved.
Sep 21, 18:13 CEST
Monitoring - We have now contacted all customers that were affected by the Data Store issue. We will continue working with them directly to fix any discrepancies that have arisen in their Data Stores. If we have not contacted you, you were not impacted by the issue.
Sep 20, 18:40 CEST
Update - We have now completed our investigation and identified that the issue affected a small percentage of Make users and their organizations. We are preparing to update all impacted customers with information about the scenarios and particular executions that may have produced incorrect data in the Data Store. We expect to update all impacted customers in the next 24 hours.
Sep 20, 14:50 CEST
Update - We have confirmed that …]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/lpxzhxccln48</id>
        <link href="https://airbnbapi.statuspage.io/incidents/lpxzhxccln48"/>
        <updated>2023-09-25T03:22:05.000Z</updated>
        <summary type="html"><![CDATA[Sep 24, 20:22 PDT
Resolved - This incident has been resolved.
Error rates have returned to normal levels. Please retry any failed requests.
We apologize for the inconvenience caused and thank you for your patience and understanding.
Sep 22, 10:43 PDT
Monitoring - The issue has been mitigated at 10:30AM PDT. Please apply any failed requests during the incident.
Sep 22, 10:10 PDT
Investigating - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started today (Sept 22, 2023) around 09:45 AM PDT. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sep 24 2023 Notion is down]]></title>
        <id>https://status.notion.so/incidents/lmy95r4d97sq</id>
        <link href="https://status.notion.so/incidents/lmy95r4d97sq"/>
        <updated>2023-09-25T01:04:53.000Z</updated>
        <summary type="html"><![CDATA[Sep 24, 18:04 PDT
Resolved - The issue has been resolved, and notion.so is available to all users.
Sep 24, 18:03 PDT
Update - The issue has been resolved, and notion.so is available to all users.
Sep 24, 17:56 PDT
Investigating - Notion.so is currently down for some users. We are actively investigating.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New customer signups]]></title>
        <id>https://status.digitalocean.com/incidents/nf42p0nw61c4</id>
        <link href="https://status.digitalocean.com/incidents/nf42p0nw61c4"/>
        <updated>2023-09-23T13:45:35.000Z</updated>
        <summary type="html"><![CDATA[Sep 23, 13:45 UTC
Resolved - Our engineering team has confirmed that the issues affecting the new signups have been fully resolved. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Sep 23, 09:53 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting new account signups and monitoring the situation. We will post an update as soon as the issue is fully resolved.
Sep 23, 09:40 UTC
Identified - Our Engineering team has identified the issue impacting new account signups and is actively working on a fix. We will post an update as soon as additional information is available.
Sep 23, 09:13 UTC
Investigating - As of 08.45 UTC, our Engineering team is investigating an issue with new customer signups. During this time, some new customers are unable to finish the signup process, as the submit button for the initial questionnaire does not proceed further.
We apologize for the inconvenience and will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Databases Creation]]></title>
        <id>https://status.digitalocean.com/incidents/99c30sdym613</id>
        <link href="https://status.digitalocean.com/incidents/99c30sdym613"/>
        <updated>2023-09-22T18:32:09.000Z</updated>
        <summary type="html"><![CDATA[Sep 22, 18:32 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with creation of Managed Database clusters.
If you continue to experience problems, please open a ticket with our support team.
Sep 22, 17:58 UTC
Monitoring - Our Engineering team is continuing to investigate the root cause of this incident, but creation of Managed Database clusters has remained stable. 
We will continue to monitor this incident to ensure the issue does not recur and will provide an update when we have more information, or we are confident this is resolved. Thank you.
Sep 22, 17:04 UTC
Update - Our Engineering team continues to investigate the root cause of this issue, but we are now seeing previously created clusters coming online and creations going through correctly. At this time, users should be able to access clusters previously stuck in the creation state and create new clusters.
Sep 22, 16:40 UTC
Investigating - Our Engineering team is investigating an issue with creation of Managed Database clusters not completing. At this time, users may see cluster creation for Postgres, MySQL, Redis, and Kafka clusters taking a long time and/or not completing. 
We will provide another update as soon as we have further information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack Connect invitations page not loading]]></title>
        <id>https://status.slack.com//2023-09/798ffab066d9a5d6</id>
        <link href="https://status.slack.com//2023-09/798ffab066d9a5d6"/>
        <updated>2023-09-22T17:48:18.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On September 21, 2023 from 8:00 PM PDT to 1:30 PM PDT, some users encountered a message stating "Something went wrong" when accessing the Slack Connect invites page.


We determined that rate limits set for querying invites were the cause of the "Something went wrong" message. These limits were set prior to the  product decision to include these in the Activity view. Because use of this view has increased, the limits are reached much sooner than originally expected.


These limits were lifted for the invites page, resolving the issue for users. We appreciate your patience while we sorted this out and apologize for any inconvenience this may have caused.


Note: Due to an internal tooling error, we initially reported the times in this summary in UTC, but labeled with PDT. We have corrected this mistake and apologize for any confusion caused.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pages]]></title>
        <id>https://www.githubstatus.com/incidents/4ypqyv8zbrck</id>
        <link href="https://www.githubstatus.com/incidents/4ypqyv8zbrck"/>
        <updated>2023-09-22T17:39:54.000Z</updated>
        <summary type="html"><![CDATA[Sep 22, 17:39 UTC
Resolved - This incident has been resolved.
Sep 22, 17:27 UTC
Update - We have identified the issue with Pages deploys and are actively working to mitigate the issue.
Sep 22, 17:10 UTC
Investigating - We are investigating reports of degraded performance for Pages]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues completing an MFA authentication challenge]]></title>
        <id>https://status.rippling.com/incidents/262wf83cx2t3</id>
        <link href="https://status.rippling.com/incidents/262wf83cx2t3"/>
        <updated>2023-09-21T17:04:12.000Z</updated>
        <summary type="html"><![CDATA[Sep 21, 17:04 UTC
Resolved - This incident has been resolved.
Sep 21, 14:11 UTC
Update - We are continuing to monitor for any further issues.
Sep 21, 13:54 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Sep 21, 13:26 UTC
Identified - The issue has been identified and a fix is being implemented.
Sep 21, 13:22 UTC
Investigating - Users asked to complete an MFA challenge are unable to advance past the page. We are investigating this issue]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTTP traffic to App Platform in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/wnx731vkk630</id>
        <link href="https://status.digitalocean.com/incidents/wnx731vkk630"/>
        <updated>2023-09-21T12:59:45.000Z</updated>
        <summary type="html"><![CDATA[Sep 21, 12:59 UTC
Resolved - As of 12:46 UTC, our Engineering team confirmed that the mitigation applied has successfully resolved the issue impacting the App Platform in the FRA1 region. If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
Thank you for your patience and we apologize for the inconvenience.
Sep 21, 12:52 UTC
Monitoring - As of 12:30 UTC, our Engineering team has identified the issue impacting the App Platform in the FRA1 region and has put mitigations in place to fix the issue. We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. In the meantime, if you experience any issues with your APP, please redeploy the App.
Thank you for your patience and we apologize for the inconvenience.
Sep 21, 12:29 UTC
Investigating - As of 12:05 UTC, our Engineering team is investigating reports of an issue with our App Platform service in the FRA1 region. At this time the HTTP traffic to the existing Apps might be affected. Currently, traffic to two clusters in this region is closed. If your App resides in one of these clusters, we request you to redeploy the App so that it will land on a new cluster.
We will provide an update as soon as we have additional information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in NYC region]]></title>
        <id>https://status.digitalocean.com/incidents/p931kzltztmb</id>
        <link href="https://status.digitalocean.com/incidents/p931kzltztmb"/>
        <updated>2023-09-21T07:14:44.000Z</updated>
        <summary type="html"><![CDATA[Sep 21, 07:14 UTC
Resolved - From 04:30 - 05:10 UTC, our Engineering team observed an issue with network connectivity in the NYC region. During this time, users might have experienced high latency or errors while connecting to services, including Droplets and Droplet-based products like Managed Kubernetes, Managed Database, and App Platform Apps. 
As of 05:10 UTC, the impact has been subsided and users should no longer be facing network connectivity issues. We apologize for the inconvenience and if you are still experiencing issues or have any additional questions then please open a support ticket from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/c85syxc7gnbx</id>
        <link href="https://www.githubstatus.com/incidents/c85syxc7gnbx"/>
        <updated>2023-09-20T21:05:57.000Z</updated>
        <summary type="html"><![CDATA[Sep 20, 21:05 UTC
Resolved - This incident has been resolved.
Sep 20, 21:05 UTC
Update - Creates and resumes in US West are working normally
Sep 20, 20:47 UTC
Update - Codespace resumes in US West are currently impacted. New Codespaces are working normally.
Sep 20, 20:21 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Users are unable to change their payment method]]></title>
        <id>https://status.notion.so/incidents/sj01vbq7vgb4</id>
        <link href="https://status.notion.so/incidents/sj01vbq7vgb4"/>
        <updated>2023-09-20T21:03:28.000Z</updated>
        <summary type="html"><![CDATA[Sep 20, 14:03 PDT
Resolved - This incident has been resolved. Users are not able to change their payment method.
Sep 20, 13:52 PDT
Identified - The issue has been identified and a fix is being implemented]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/ctr28sphxc4c</id>
        <link href="https://www.githubstatus.com/incidents/ctr28sphxc4c"/>
        <updated>2023-09-20T09:33:57.000Z</updated>
        <summary type="html"><![CDATA[Sep 20, 09:33 UTC
Resolved - This incident has been resolved.
Sep 20, 09:31 UTC
Update - We have mitigated the Actions notifications and the service is recovering.
Sep 20, 09:21 UTC
Update - Actions notifications are experiencing degraded performance. We are currently investigating.
Sep 20, 09:20 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, API Requests and Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/zjchv3zvfg50</id>
        <link href="https://www.githubstatus.com/incidents/zjchv3zvfg50"/>
        <updated>2023-09-20T04:28:09.000Z</updated>
        <summary type="html"><![CDATA[Sep 20, 04:28 UTC
Resolved - On 2023-09-19 at 20:36 UTC, while migrating the primary datastore for Projects, we caused an outage that led to Projects data becoming unavailable for approximately four hours. 
While working to restore Projects data, we also experienced a data replication interruption which affected Git Operations, APIs, and Issues. We first resolved the data replication issue, which returned Git Operations, APIs, and Issues to normal operation. We then restored Projects data to its pre-migration state, and re-inserted data that was added during the period of partial availability. The incident was resolved on 2023-09-20 at 00:06 UTC.
As a result of this incident, we have improved validation of data migrations in test and during rollout.  We have also identified improvements to…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/00lscqwb6ht5</id>
        <link href="https://www.githubstatus.com/incidents/00lscqwb6ht5"/>
        <updated>2023-09-19T14:04:05.000Z</updated>
        <summary type="html"><![CDATA[Sep 19, 14:04 UTC
Resolved - This incident has been resolved.
Sep 19, 13:53 UTC
Update - Pull Requests is operating normally.
Sep 19, 13:52 UTC
Update - Push event processing has caught up and there should be no further delays experienced for pull request updates. There will be a tail of delayed notifications being delivered as part of the recovery, and we expect this to be fully caught up in the near future.
Sep 19, 13:29 UTC
Update - We are experiencing a delay in processing for repository push events, which may result in delayed updates to pull requests. We have identified the issue and are seeing recovery, and will provide another update shortly.
Sep 19, 13:21 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/ndjt4zhsr7q9</id>
        <link href="https://www.githubstatus.com/incidents/ndjt4zhsr7q9"/>
        <updated>2023-09-18T21:33:29.000Z</updated>
        <summary type="html"><![CDATA[Sep 18, 21:33 UTC
Resolved - This incident has been resolved.
Sep 18, 21:28 UTC
Update - There was a regional issue with Codespaces. Instance creation was failing for some customers in West US. The issue has since been mitigated.
Sep 18, 21:28 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/frp2gyhj1mjc</id>
        <link href="https://www.githubstatus.com/incidents/frp2gyhj1mjc"/>
        <updated>2023-09-18T21:26:58.000Z</updated>
        <summary type="html"><![CDATA[Sep 18, 21:26 UTC
Resolved - This incident has been resolved.
Sep 18, 21:16 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>