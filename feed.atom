<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-03-04T00:22:00.200Z</id>
    <title>osmos::feed</title>
    <updated>2024-03-04T00:22:00.200Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests, Copilot, Git Operations, Actions and Pages]]></title>
        <id>https://www.githubstatus.com/incidents/7x5z8plb48t6</id>
        <link href="https://www.githubstatus.com/incidents/7x5z8plb48t6"/>
        <updated>2024-03-01T17:42:41.000Z</updated>
        <summary type="html"><![CDATA[Mar  1, 17:42 UTC
Resolved - This incident has been resolved.
Mar  1, 17:42 UTC
Update - Git Operations is operating normally.
Mar  1, 17:41 UTC
Update - Actions and Pages are operating normally.
Mar  1, 17:36 UTC
Update - Copilot is operating normally.
Mar  1, 17:34 UTC
Update - Pages is experiencing degraded performance. We are continuing to investigate.
Mar  1, 17:34 UTC
Update - One of our clusters is experiencing problems, and we are working on restoring the cluster at this time.
Mar  1, 17:30 UTC
Investigating - We are investigating reports of degraded performance for API Requests, Copilot, Git Operations and Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Help website is currently unreachable]]></title>
        <id>https://status.make.com/incidents/1vm1k0hxtbdb</id>
        <link href="https://status.make.com/incidents/1vm1k0hxtbdb"/>
        <updated>2024-03-01T16:44:34.000Z</updated>
        <summary type="html"><![CDATA[Mar  1, 17:44 CET
Monitoring - We have successfully implemented a workaround, and the content of the affected pages is now accessible. However, we are still awaiting a response from the third-party service and further insight into the issue. Currently, the entire webpage is operational. We will continue to monitor the situation closely until we receive additional information from the third party.
Mar  1, 16:51 CET
Update - We are still investigating the issue, and our initial suspicion is that it may be related to one of our third-party services. We anticipate providing the next update within the next 2 hours, or as soon as more information becomes available.
Mar  1, 16:02 CET
Investigating - Our Help and API documentation websites are currently unreachable due to unknown reasons. We are currently investigating the root cause. The rest of the platform's functionalities are unaffected. We will provide an update in the next 30 minutes. Sorry for the inconvenience.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests, Actions and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/wcl1sw4mzg60</id>
        <link href="https://www.githubstatus.com/incidents/wcl1sw4mzg60"/>
        <updated>2024-03-01T16:12:23.000Z</updated>
        <summary type="html"><![CDATA[Mar  1, 16:12 UTC
Resolved - This incident has been resolved.
Mar  1, 16:12 UTC
Update - Issues, Pull Requests and Actions are operating normally.
Mar  1, 15:48 UTC
Update - We're seeing our background job queue sizes trend down, and expect full recovery in the next 15 minutes.
Mar  1, 15:39 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Mar  1, 15:27 UTC
Update - We're continuing to investigate issues with background jobs that have impacted Actions and Pull Requests. We have a mitigation in place and are monitoring for recovery.
Mar  1, 14:51 UTC
Update - We're investigating issues with background jobs that are causing sporadic delays in pull request synchronization and reduced Actions throughput.
Mar  1, 14:39 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests and Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New improved payment description format]]></title>
        <id>287075</id>
        <link href="https://changelog.bookingsync.com/new-improved-payment-description-format-287075"/>
        <updated>2024-03-01T12:58:56.000Z</updated>
        <summary type="html"><![CDATA[Improvement
Â Â 
We've enhancing the payment descriptions that are displayed in your Stripe or BookingPay account on your transactions, to make them clearer and more useful for you, based on your valuable feedback.
Here's What's Changing:
The description will now include the booking reference:
Old Format: "Payment for Beautiful Example Rental from Monday 27 May 2024, 04:00 PM to Sunday 02 Jun 2024, 10:00 AM"
â€‹


New Format: "Payment for booking XYZ123 at Beautiful Example Rental from 27 May 2024 to 02 Jun 2024"


Why? To streamline tracking and management of your bookings, making reconciliation easier.
Thank you for being a valuable member of the Smily community. We're thrilled to keep enhancing your property management journey!]]></summary>
        <author>
            <name>Megan, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users unable to create or edit workflows]]></title>
        <id>https://slack-status.com/2024-02/35134eda119581f5</id>
        <link href="https://slack-status.com/2024-02/35134eda119581f5"/>
        <updated>2024-03-01T05:48:33.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 29, 2024 from 1:28 PM PST to around 4:36 PM PST, users were unable to create or update some workflows.

We determined that a recent code change had inadvertently introduced a logic problem, resulting in errors for workflows that used event triggers. We reverted this code change. However, rolling back reintroduced a separate issue that we'd resolved earlier in the day. 

We prepared a tailored revert to roll back the code causing the workflow problem while also including the fix for the previous issue. This second revert resolved both problems, restoring full functionality to Slack. 

Thank you for being patient with us.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We are experiencing an Apps Script issue impacting Google Calendar 3rd party video conferencing integrations, Google Forms, Google Docs, and Google Sheets beginning on Monday, 2024-02-26 00:11 US/Pacific.
For further details on product impact, please see: https://www.google.com/appsstatus/dashboard/incidents/DR1JYH2tW69g9BobX8e6]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/ipDHw6kUSngP2zux2KoP</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/ipDHw6kUSngP2zux2KoP"/>
        <updated>2024-02-29T23:46:33.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2024-02-26 09:42</strong> and ended at <strong>2024-02-26 21:02</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Incident Report</h1>
<h2>Summary</h2>
<p>On Monday, 26 February 2024 at 1:42 US/Pacific, Google Apps Script experienced a higher rate of failures while attempting various operational actions for a duration of 11 hours, 20 minutes. Apps Script integrations in Google Workspace products, such as those that integrate with Google Calendar and Google Docs, were partially unavailable. This includes 3rd party Google Workspace add-ons that were built using Apps Script. During the incident, the issue was intermittent for most users with overall error rates of up to 25% for the listed services, and customers retrying actions were eventually successful.</p>
<p>This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platformâ€™s performance and availability.</p>
<h2>Root Cause</h2>
<p>A third-party Google Workspace Add-on with a large pre-existing install base added a dependency on Google Apps Script, which triggered a significant increase in load for the Apps Script service. Due to this high QPS, latencies increased, followed by an increase in error rates. All server threads supporting Apps Script integrations were busy processing requests, causing our servers to start rejecting requests and resulting in errors. Ultimately this resulted in temporary service interruption for users who rely on Apps Script functionality within their Google Workspace environments.</p>
<h2>Remediation and Prevention</h2>
<p>Google engineering was alerted to the outage via internal monitoring on Monday 26 February 2024 at 00:16 US/Pacific and immediately started an investigation. Engineering identified the problematic deployment, rolled back the change, and temporarily blocked access to the add-on so the service could fulfill requests as expected. This resulted in a significant error rate decrease, services became available, and the issue was fully recovered on Monday 26 February, 2024 13:02 US/Pacific.</p>
<p>Google is committed preventing a repeat of this issue in the future and is completing the following actions:</p>
<ul>
<li>Adding additional system limits to prevent a single Apps Script project from creating overload scenarios</li>
<li>Improving the process for blocking problematic Apps Script projects</li>
</ul>
<h2>Detailed Description of Impact</h2>
<p>On Monday, 26 February 2024 from 01:42 to 13:02 US/Pacific customers utilizing products with Apps Script integrations may have experienced the following:</p>
<h3>Apps Script</h3>
<p>Apps Script encountered errors during the creation or loading of scripts.</p>
<h3>Google Calendar</h3>
<p>A subset of customers encountered failures while listing the third-party conferencing solutions. Additionally, failures were observed while attaching a conference to an event with the Google Calendar.</p>
<h3>Google Forms, Google sheets and Google docs</h3>
<p>Custom functions and Add-ons created with Apps Script would have experienced disruptions when used in Google Docs, Sheets, Slides, and Forms.</p>
</div><hr><p>Affected products: Google Calendar</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Workflows are unable to be viewed or triggered]]></title>
        <id>https://slack-status.com/2024-02/939919184ea7655b</id>
        <link href="https://slack-status.com/2024-02/939919184ea7655b"/>
        <updated>2024-02-29T20:47:52.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 29, 2024 at 8:00 AM PST to 9:55 AM PST, users saw errors when attempting to trigger and manage legacy workflows.

Upon investigation, we discovered a code change prevented part of the infrastructure that supports workflows from functioning correctly.

To resolve the issue, we reverted the code change and all workflows were restored.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/5lc3f39mjcq8</id>
        <link href="https://www.githubstatus.com/incidents/5lc3f39mjcq8"/>
        <updated>2024-02-29T12:27:17.000Z</updated>
        <summary type="html"><![CDATA[Feb 29, 12:27 UTC
Resolved - On February 29, 2024, between 9:32 and 11:54 UTC, queuing in our background job service caused processing delays to Webhooks, Actions, and Issues. Nearly 95% of delays occurred between 11:05 and 11:27 UTC, with 5% during the remainder of the incident. During this incident, the following customer impacts occurred: 50% of webhooks experienced delays of up to 5m, 1% of webhooks experienced delays of 17m at peak; Actions: on average, 7% of customers experienced delays, with a peak of 44%; and many Issues saw a delay in appearing in searches. At 9:32 UTC our automated failover successfully routed traffic to a secondary cluster. But an improper restoration to primary at 10:32 UTC caused a significant increase in queued jobs until 11:21 UTC, when a correction was made and healthy services began burning down the backlog until full resolution.
We have made improvements to the automation and reliability of our fallback process to prevent recurrence. We also have larger work already in progress to improve the overall reliability of our job processing platform.

Feb 29, 12:21 UTC
Update - We're seeing recovery and are going to take time to verify that all systems are back in a working state.
Feb 29, 12:19 UTC
Update - Issues is operating normally.
Feb 29, 12:18 UTC
Update - Webhooks is operating normally.
Feb 29, 11:05 UTC
Update - We're continuing to investigate delayed background jobs. We've seen partial recovery for Issues, and there is ongoing impact to actions, notifications and webhooks.
Feb 29, 10:58 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Feb 29, 10:36 UTC
Update - We're seeing issues related to background jobs, which are causing delays for webhook delivery and search indexing, and other updates.
Feb 29, 10:33 UTC
Investigating - We are investigating reports of degraded performance for Issues and Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Recovery Image]]></title>
        <id>https://status.digitalocean.com/incidents/dj2p47c4zss1</id>
        <link href="https://status.digitalocean.com/incidents/dj2p47c4zss1"/>
        <updated>2024-02-29T05:50:18.000Z</updated>
        <summary type="html"><![CDATA[Feb 29, 05:50 UTC
Resolved - Our Engineering team identified and resolved an issue that was affecting the booting of Droplets from the Recovery ISO.
From 00:20 UTC to 05:24 UTC, users might have experienced errors when attempting to boot Droplets from the Recovery ISO.
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CreateMessage API - WRITE downtime]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/qfy422ht70s1</id>
        <link href="https://airbnbapi.statuspage.io/incidents/qfy422ht70s1"/>
        <updated>2024-02-29T01:05:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 17:05 PST
Completed - The scheduled maintenance has been completed.
Feb 28, 17:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 22, 23:23 PST
Scheduled - Please note upcoming maintenance occurring on Wednesday, 2/28/2024 at 5:00 PM PT, that will result in ~30-60 second WRITE downtime to CreateMessage API. Reads will not be affected.
Alerts and errors pertaining to Messaging on Wednesday, 2/28/2024 around 5:00 PM PT, are to be expected. Please consider any corrective actions needed to mitigate potential data loss during this ~30-60 second period.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Databases creation - PostgresSQL v16]]></title>
        <id>https://status.digitalocean.com/incidents/cd70bby9qynm</id>
        <link href="https://status.digitalocean.com/incidents/cd70bby9qynm"/>
        <updated>2024-02-28T22:53:47.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 22:53 UTC
Resolved - Our Engineering team has confirmed that creation, forking, and restoration of PostgreSQL clusters on v16 is functioning correctly.
Upgrades for lower-versioned PostgreSQL clusters to v16 remain unavailable at this time and users will see errors if they attempt to perform that upgrade. Our Engineering team continues to work on making upgrades to v16 available again, but we expect this to take some time.
If you continue to experience issues or have any questions, please open a ticket with our support team.
Feb 28, 20:56 UTC
Monitoring - After testing, teams have determined that PostgreSQL v16 is safe for new creations, as well as forks and restores for existing clusters. At this time, v16 is re-enabled in our Cloud Control Panel and users creating, forking, or reâ€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SFO Networking]]></title>
        <id>https://status.digitalocean.com/incidents/g4sdvwy4z4zm</id>
        <link href="https://status.digitalocean.com/incidents/g4sdvwy4z4zm"/>
        <updated>2024-02-28T22:49:41.000Z</updated>
        <summary type="html"><![CDATA[Feb 28, 22:49 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with networking in our SFO2 region. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!
Feb 28, 22:23 UTC
Monitoring - Our Engineering team has confirmed that the faulty network hardware component was the cause of this issue. From 21:39 - 22:11 UTC, this component was not functioning correctly, causing networking issues for a subset of customers in our SFO2 region, as well as internal alerts in our SFO1/SFO3 regions. 
At this time, all services should now be operating normally. We will monitor this incident for a short period of time to confirm full resolution.
Feb 28, 22:17 UTC
Identified - Our Engineering team has identified the cause of the issue with networking in our SFO regions to be related to an issue with a network hardware component in SFO2. They have isolated that component and we're observing error rates returning to pre-incident levels at this time. 
We are continuing to look into this failure, but users should be seeing recovery on their services. We'll provide another update soon.
Feb 28, 22:06 UTC
Investigating - Our Engineering team is currently investigating internal alerts and customer reports for an increase in networking errors in our SFO regions for Droplets and Droplet-based services. We will provide an update as soon as we have further information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlock your rental's potential: Expert Vrbo Strategies, Smily Innovations & Upcomming Events! ðŸ—ï¸]]></title>
        <id>286877</id>
        <link href="https://changelog.bookingsync.com/unlock-your-rental's-potential-expert-vrbo-strategies-smily-innovations-upcomming-events!-286877"/>
        <updated>2024-02-28T10:21:46.000Z</updated>
        <summary type="html"><![CDATA[Communications
Â Â 
In todayâ€™s Smily newsletter, we directly address Vrbo performance optimization, spotlighting essential optimization tips shared by our Partner Vrbo. We also introduce our latest releases, designed to streamline your operations, and preview upcoming events where Smily will be featured. This edition delivers focused insights and recent developments to strengthen your property management strategies. ðŸ™Œ
ðŸ”Optimization tips: Enhance your listings on Vrbo
Our partner Vrbo, has generously provided a selection of their top optimization practices to elevate your overall listing's performance. Below are the main highlights, along with guidance on implementing these strategies through your Smily account:
Optimize listing content - What makes a Vrbo listing attractive ðŸ’™
Key amenitieâ€¦]]></summary>
        <author>
            <name>Ella, Chief Customer Officer (CCO) &amp; Cofounder</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble signing in on mobile for some Owners and Admins]]></title>
        <id>https://slack-status.com/2024-02/2ea2fda2874858b7</id>
        <link href="https://slack-status.com/2024-02/2ea2fda2874858b7"/>
        <updated>2024-02-28T05:55:13.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 
On February 27, 2024 from around 8:30 PM PST to 10:06 PM PST Workspace Owners and Admins were experiencing issues while setting up two-factor authentication during sign in on iOS devices.

The issue was caused by a two-factor authentication feature deploy to users on the Pro subscription. After identifying the root cause the feature was was rolled back and this allowed all users to log back into Slack.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMS3 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/7rzlkzs0ksqs</id>
        <link href="https://status.digitalocean.com/incidents/7rzlkzs0ksqs"/>
        <updated>2024-02-27T20:00:57.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 20:00 UTC
Completed - The scheduled maintenance has been completed.
Feb 27, 16:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 20, 15:19 UTC
Scheduled - Start: 2024-02-27 16:00 UTC
End:  2024-02-27  20:00 UTC

During the above window, our Networking team will be making changes to core networking infrastructure, to improve performance and scalability in the AMS3 region. 
Expected impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience increased latency or a brief disruption in network traffic. We will endeavor to keep any such impact to a minimum.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Close button for right pane not appearing for some users]]></title>
        <id>https://slack-status.com/2024-02/cedce965afd47ebb</id>
        <link href="https://slack-status.com/2024-02/cedce965afd47ebb"/>
        <updated>2024-02-27T15:41:56.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 
On February 26, 2024, from 9:21 AM PST to 9:16 AM PST on February 27, some users encountered the close button disappearing in the right pane where threads and profiles are viewed.

We discovered this was caused by a missing check in the change workspace process that prevented the right pane elements from rendering properly. Once the cause was identified, a change was made to the logic for drawing Slack's interface when switching workspaces. Once the change was rolled out, the issue was resolved.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users are not able to create workflows]]></title>
        <id>https://slack-status.com/2024-02/b1389c8cd318a7d1</id>
        <link href="https://slack-status.com/2024-02/b1389c8cd318a7d1"/>
        <updated>2024-02-27T02:50:48.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
From 12:30 PM PST to 1:05 PM PST on February 26, 2024, users were unable to create workflows.

We determined that the related API method needed updating in order to eliminate inconsistencies that were causing elevated replication lag.

We deployed an update to the API method correcting this issue, and all workflow creation will now work as expected.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues sending and loading messages]]></title>
        <id>https://slack-status.com/2024-02/4c0af99013428a7a</id>
        <link href="https://slack-status.com/2024-02/4c0af99013428a7a"/>
        <updated>2024-02-27T01:47:26.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 26, 2024 from 9:00 AM PST to 9:36 AM PST, some users experienced problems sending and loading messages in Slack. Others may have completed these actions successfully but more slowly than expected.

Earlier today, we deployed a routine update to part of our network infrastructure. During this work, we deprovisioned some of our proxy servers and provisioned a new set. Some of our webapp servers took longer than expected to sync with the new list of proxy servers, resulting in a spike in connection errors. This coincided with the regular 9:00 AM PST increase in traffic to our servers, which may have exacerbated impact.

By around 9:36 AM PST, error rates had decreased and user impact had completely subsided, most likely due to our automatic scaling functionality. However, we're continuing to analyze data from this event to fully understand the root cause and prevent similar issues in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues registering an account]]></title>
        <id>https://status.rippling.com/incidents/5pv7fqwgvtck</id>
        <link href="https://status.rippling.com/incidents/5pv7fqwgvtck"/>
        <updated>2024-02-27T01:21:53.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 01:21 UTC
Resolved - This incident has been resolved.
Feb 26, 23:24 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Feb 26, 22:06 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Problem with executing scenarios]]></title>
        <id>https://status.make.com/incidents/smcl14qd1qxw</id>
        <link href="https://status.make.com/incidents/smcl14qd1qxw"/>
        <updated>2024-02-26T23:06:42.000Z</updated>
        <summary type="html"><![CDATA[Feb 27, 00:06 CET
Resolved - This incident has been resolved.
Feb 26, 21:58 CET
Monitoring - A fix has been implemented and we are monitoring the results.
Feb 26, 21:34 CET
Investigating - We're encountering issues executing scenarios on eu2.make.com. Users may experience delays and errors when attempting to execute any scenarios. There are no other issues with scenario creation or general connectivity to Make. We're currently investigating this issue and will update this Statuspage within the next hour, or as more information comes to hand.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/f9ntcnd3fdvs</id>
        <link href="https://www.githubstatus.com/incidents/f9ntcnd3fdvs"/>
        <updated>2024-02-26T21:40:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 21:40 UTC
Resolved - On Monday, February 26th, from 20:45 UTC to 21:39 UTC, GitHub Packages reported an outage indicating a degradation in GitHub Container Registry and NPM package upload functionality. Upon investigation, we found a misconfigured observability metric which inadvertently pulled in data from a newly provisioned test environment. All failures being reported were traced back to this test environment. We confirmed that there was no real customer impact to GitHub Packages during this incident. We have since reconfigured our observability metrics to accurately report based on environment.
Feb 26, 21:20 UTC
Update - We are seeing some recovery in NPM and GitHub Container Registry functionality, but are maintaining red status until we are certain issues wonâ€™t recur.
Feb 26, 21:03 UTC
Update - NPM and GitHub Container Registry services are degraded, particularly the upload functionality. Investigation is underway.
Feb 26, 21:01 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks, Actions, Pull Requests and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/78qz8zwhx9tj</id>
        <link href="https://www.githubstatus.com/incidents/78qz8zwhx9tj"/>
        <updated>2024-02-26T19:37:32.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 19:37 UTC
Resolved - On February 26, 2024, between 18:34 UTC and 19:37 UTC our background job service was degraded and caused job start delays up to 15 minutes. Users experienced delays in Webhooks, Actions, and some UI updates (e.g. a delay in UI updates on pull requests). This was due to capacity problems with our job queueing service, and a failure of our automated failover system.
We mitigated the incident by manually failing over to our secondary cluster.   No data was lost - recovery began at 18:55 UTC, when the backlog of enqueued jobs began to process.
We are actively working to repair our failover automation and expand the capacity of our background job queuing service to prevent issues like this in the future.

Feb 26, 19:37 UTC
Update - Actions and Pull Requests are operating normally.
Feb 26, 19:37 UTC
Update - Webhooks and Issues are operating normally.
Feb 26, 19:05 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:57 UTC
Update - Pull Requests is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:55 UTC
Update - We have deployed a fix for issues affecting Webhooks, Actions, and some other services. We are beginning to see recovery and will continue to monitor and fix as needed.
Feb 26, 18:55 UTC
Update - Webhooks is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:48 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Feb 26, 18:47 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with the billing history page]]></title>
        <id>https://slack-status.com/2024-02/b4d41d5daac6431c</id>
        <link href="https://slack-status.com/2024-02/b4d41d5daac6431c"/>
        <updated>2024-02-26T14:30:45.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From 2:58 PM PST on February 22, 2024, until 9:47 PM PST on February 25, 2024, some users were unable to access their workspace billing history.

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users.

Thank you very much for your patience while we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manage your VRBO guest review from Smily interface]]></title>
        <id>286647</id>
        <link href="https://changelog.bookingsync.com/manage-your-vrbo-guest-review-from-smily-interface-286647"/>
        <updated>2024-02-26T10:54:27.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
Are you connected to VRBO OTA?
Starting today at 2pm UTC, we will be fetching guest reviews and ratings from VRBO, and you'll be able to respond to them directly from Smily interface :)
ðŸš§ Pain Points Addressed:
No need to visit VRBO to check and respond to guest reviews.
Previously, you couldn't rate a guest.
ðŸš€ Benefits for you:
Manage all your reviews in one place.
Easily read and respond to VRBO guest reviews from our Review section on the Smily interface.
Rate guests with ease.
â„¹ï¸ Information and next steps:
We will fetch all your past reviews.


You will be able to rate all past bookings matching those criteria:
--> Bookings without guest review that ended in the last 365 days;
--> Bookings for which a guest review is already present and 14 days havenâ€™t passed since the guest made the review;


Make use of this new feature and ensure to leave reviews for guests. This is an important criterion for VRBO and will impact your property ranking.


Lastly, you can use our "automated reviews" feature, released at the end of last year, for VRBO reviews too.


Have a great review day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/6hnyx0v9zjbh</id>
        <link href="https://status.make.com/incidents/6hnyx0v9zjbh"/>
        <updated>2024-02-26T07:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 26, 08:00 CET
Completed - The scheduled maintenance has been completed.
Feb 26, 06:00 CET
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 15, 16:40 CET
Scheduled - Hello,
Please note there is scheduled infrastructure maintenance on 26.02.2024, between 6:00-8:00 AM CET, when you can expect a slower scenario processing. Specifically, scenarios using a datastore module might see reduced performance which can cause its longer execution.
We will inform you once the maintenance is completed.
Thank you very much for understanding.
Kind regards,
Make team]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some template duplication may fail]]></title>
        <id>https://status.notion.so/incidents/gnzy4hx09dq6</id>
        <link href="https://status.notion.so/incidents/gnzy4hx09dq6"/>
        <updated>2024-02-24T00:52:27.000Z</updated>
        <summary type="html"><![CDATA[Feb 23, 16:52 PST
Resolved - Our engineering team identified the issue and released a fix to resolve it.
Feb 23, 16:42 PST
Investigating - Customers may experience an error when duplicating some templates.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Threads aren't loading for some users]]></title>
        <id>https://slack-status.com/2024-02/548b72776056c03b</id>
        <link href="https://slack-status.com/2024-02/548b72776056c03b"/>
        <updated>2024-02-23T03:05:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
From 2:54 PM to 2:59 PM PDT on February 22, 2024, some users were having issues loading Threads.

We carried out a thorough investigation and observed trends, but found no evidence of an issue on our side. By around 2:59 PM PDT, error rates had already subsided and we received confirmation from several users that they could load Threads again by reloading Slack.

Whilst no remedial action was taken on our end, we're analyzing data from this event to understand ways to mitigate similar issues that may occur in the future.

We're confident that there will be no further impact to users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Screen readers are not announcing unread messages when using the Ctrl + K command on Windows.]]></title>
        <id>https://slack-status.com/2024-02/83f4c18fc4d11c93</id>
        <link href="https://slack-status.com/2024-02/83f4c18fc4d11c93"/>
        <updated>2024-02-23T01:46:26.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 21, 2024 from 6:30 PM PST to 8:00 PM PST, customers using the JAWS or NVDA screen readers on Windows may have experienced issues with announcements for unread messages in the Quick Switcher (Ctrl + K). 

A recent code change inadvertently introduced a conflict with an HTML label for an interactive element.

We reverted the code change, fixing the issue and restoring normal announcement behaviour.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with password resets]]></title>
        <id>https://slack-status.com/2024-02/e04276624660caf9</id>
        <link href="https://slack-status.com/2024-02/e04276624660caf9"/>
        <updated>2024-02-22T14:32:08.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 

Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are unable to upload, download, and view files in Slack.]]></title>
        <id>https://slack-status.com/2024-01/f39851209d6c471a</id>
        <link href="https://slack-status.com/2024-01/f39851209d6c471a"/>
        <updated>2024-02-22T14:31:13.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 

We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 

Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry Latency in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/n9211k9rmml7</id>
        <link href="https://status.digitalocean.com/incidents/n9211k9rmml7"/>
        <updated>2024-02-21T20:43:48.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 20:43 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting the Container Registry in multiple regions. 
Everything involving the Container Registry should now be functioning normally. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Feb 21, 18:03 UTC
Monitoring - Our Engineering team has identified an internal operation within the Container Registry service which was placing load on the service, leading to latency and errors. The team has paused that operation in order to resolve the issue impacting the Container Registry in multiple regions. Users should not be facing any latency issues while interacting with their Container registries and also while building their Apps. 
We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. 
Thank you for your patience and we apologize for the inconvenience.
Feb 21, 15:41 UTC
Investigating - Our Engineering team is investigating an issue with the DigitalOcean Container Registry service. Beginning around 20:00 UTC on February 20, there has been an uptick in 401 errors for image pulls from the Container Registry service.
During this time, a subset of customers may experience latency or see 401 errors while interacting with Container Registries. This issue also impacts App Platform builds and users may encounter delays while building their Apps or experience timeout errors in builds as a result. Users utilizing Container Registry for images for deployment to Managed Kubernetes clusters may also see latency or failures to deploy.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/wn6s1w8vkk1y</id>
        <link href="https://www.githubstatus.com/incidents/wn6s1w8vkk1y"/>
        <updated>2024-02-21T17:30:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 17:30 UTC
Resolved - On Wednesday February 21, 2024, 17:07 UTC, we deployed a configuration change to one of our services inside of Actions. At 17:14 UTC we noticed an increase in exceptions that impacted approximately 85% of runs at that time. 
At 17:18 UTC, we reverted the deployment and our service immediately recovered. During this timeframe, customers may have noticed their workflows failed to trigger or workflows were queued but did not progress.
To prevent this issue in the future we are improving our deployment observability tooling to detect errors earlier in the deployment pipeline.
Feb 21, 17:20 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Availability in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/rccs80k3p2pb</id>
        <link href="https://status.digitalocean.com/incidents/rccs80k3p2pb"/>
        <updated>2024-02-20T07:07:10.000Z</updated>
        <summary type="html"><![CDATA[Feb 20, 07:07 UTC
Resolved - As of 06:25 am UTC, our Engineering team has confirmed the resolution of the issue impacting Spaces availability in the BLR1 region.
Users should no longer experience issues with their Spaces resources in the BLR1 region.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Feb 20, 06:39 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the Spaces availability issues in the BLR1 region and is monitoring the situation. 
Users should no longer encounter errors when accessing Spaces in the BLR1 region and should be able to create new Spaces buckets from the cloud control panel. 
We will post an update as soon as the issue is fully resolved.
Feb 20, 05:46 UTC
Investigating - Our Engineering team is investigating an issue with Spaces availability in the BLR1 region. During this time users may encounter errors when accessing Spaces objects and creating new buckets in the BLR1 region. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BLR1 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/5z0npmmmnc1h</id>
        <link href="https://status.digitalocean.com/incidents/5z0npmmmnc1h"/>
        <updated>2024-02-19T17:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 17:00 UTC
Completed - The scheduled maintenance has been completed.
Feb 19, 14:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 16, 14:52 UTC
Scheduled - Start: 2024-02-19 14:00 UTC
End:Â  2024-02-19 17:00 UTC
Hello,
During the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration.
Expected impact:
As part of this maintenance, event processing in BLR1 will be disabled for a period of up to 15 minutes during the three-hour window. During this period, users won't be able to create, destroy, or modify new or existing DO services in BLR1 (such as Droplets, DBaaS/DOKS clusters, etc.).
If you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket

Thank you,
Team DigitalOcean]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Products Impacted in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/q2sl05rc7mgp</id>
        <link href="https://status.digitalocean.com/incidents/q2sl05rc7mgp"/>
        <updated>2024-02-19T15:50:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 15:50 UTC
Resolved - From 15:50 - 16:46, our team received customer reports of issues impacting multiple products in our BLR1 region, including the accessibility of Managed Databases and Managed Kubernetes clusters and general network connectivity disruption. These issues may be related to a scheduled maintenance event in the region, per our status post linked below:
https://status.digitalocean.com/incidents/5z0npmmmnc1h
Our team continues to review customer reports and diagnose the impact related to this maintenance. In the meantime, we have rolled back the maintenance process and all services should now be responding normally. If you experience any further issues, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/yp30nqsv310k</id>
        <link href="https://status.digitalocean.com/incidents/yp30nqsv310k"/>
        <updated>2024-02-18T19:08:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 18, 19:08 UTC
Resolved - As of 17:47 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Managed Kubernetes service in our NYC3 region. The Cilium pods inside the clusters should be functioning normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb 18, 18:09 UTC
Monitoring - Our Engineering team has deployed the fix for the issue with Managed Kubernetes service where users were experiencing network connectivity issues with Cilium pods being restarted inside the clusters. Cilium pods should now be functioning normally. 
We are monitoring the situation and will post another update once we confirm the fix resolves this incident.
Feb 18, 16:57 UTC
Investigating - Our Engineering team is investigating an issue with our Managed Kubernetes service in NYC3 region. 
During this time users may experience network connectivity issues specifically with the Cilium pods inside their clusters.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Reports]]></title>
        <id>https://status.rippling.com/incidents/p5wbyw5s471p</id>
        <link href="https://status.rippling.com/incidents/p5wbyw5s471p"/>
        <updated>2024-02-15T22:38:53.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 22:38 UTC
Resolved - This issue has been resolved. Customers should no longer be impacted by this issue.
Feb 15, 19:50 UTC
Identified - We are aware of issues with Reports loading. We have identified a fix and are working to roll it out.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Errors updating Notion subscription]]></title>
        <id>https://status.notion.so/incidents/qpyywl1mc14q</id>
        <link href="https://status.notion.so/incidents/qpyywl1mc14q"/>
        <updated>2024-02-15T17:30:36.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 09:30 PST
Resolved - On February 15th 2023 between 01:39 PST - 08:43 PST, Notion was experiencing an issue with subscription updates, which may have resulted in errors when changing billing plans.
Our engineering team identified the issue and increased capacity to this service to prevent errors, and this is now working as normal. We appreciate your patience and will work to prevent similar errors in future.
Feb 15, 08:52 PST
Investigating - Customers may experience an error when attempting to change their Notion subscription.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Profile photos aren't displaying properly]]></title>
        <id>https://slack-status.com/2024-02/16aecfd53af7219b</id>
        <link href="https://slack-status.com/2024-02/16aecfd53af7219b"/>
        <updated>2024-02-15T14:40:11.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

Starting around 12:46 PM PST on February 6, 2024, until around 4:06 PM PST on February 9, 2024, some users experienced issues with their profile pictures loading, resulting in the default avatar image displaying instead of their custom image.

We identified a bug that caused missing image data from a database cache. We fixed the bug and reset the backend cache to pull new data, which initially fixed the issue around 4:00 PM PST on February 6. However, some users began experiencing the problem again once their Slack client refreshed.

We continued investigating the issue, tracing it back to a piece of code that would push older image data to a database that housed profile information, and promptly reverted the incorrect code which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are now supporting and managing Booking.com' Sponsored Benefit program at Smily]]></title>
        <id>285929</id>
        <link href="https://changelog.bookingsync.com/we-are-now-supporting-and-managing-booking-com'-sponsored-benefit-program-at-smily-285929"/>
        <updated>2024-02-15T13:30:51.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
We are thrilled to share with you some great news :)
We are now supporting and managing Booking.comÂ Sponsored Benefit program at Smily;
Benefits for you:
You were facing the obstacle whereÂ booking.comÂ **was paying a small amount of the booking.com reservation due to a discount offered to the guest.
1/ it was not blocking the synchronisation of your listing but was creating an issue for the payment;
2/ you had to send manually a payment linkÂ to collect the remaining amount to the guests. This wonâ€™t be necessary anymore!


Now, as we do support the Sponsored benefit program, you will still see and receive this small amount in advanceÂ paid byÂ Booking.comÂ and we will process with success the rest of the payment!


You can now join this program if you were waiting for this functionality ðŸ™‚
How does this program benefit your business?
When applied, it offers the following advantages for your business:
Boosts bookings from price-conscious customers
Lowers cancellations with customers paying in advance
Increases your revenue by covering the reduced amount to drive more bookings
Take the next step:
Take a look at what this program is about andÂ check out if you are eligible to enable itÂ and enjoy it;
Check our new manual
Your business deserves the best, and with Smilyâ€™s latest feature, weâ€™re here to ensure you get just that.]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbnb - enrich your listing thanks to the listing expectation types]]></title>
        <id>285927</id>
        <link href="https://changelog.bookingsync.com/airbnb-enrich-your-listing-thanks-to-the-listing-expectation-types-285927"/>
        <updated>2024-02-15T13:24:14.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
The Airbnb Listing expectation type feature is now available on Smily. 
This update lets you enrich your listings with additional information, directly from Smily's user-friendly interface.
Pain Points Addressed:
Gone are the days of toggling between platforms. Previously, you had to navigate Airbnb's website to update listing details, a time-consuming and inefficient process. Smily's new feature streamlines this, offering a seamless integration within the interface.
ðŸš€ Benefits for you:
Easily accessible under the "amenities" section in the Smily interface, the new "Property Info" category empowers you to select and customize expectation types, with an optional description of up to 100 characters.
This information is then flawlessly synced with Airbnb for those using full synchronization, ensuring a cohesive and accurate listing.
It will also be synchronised on your website if you have one with Smily!
Such information is key and valuable for the guests to avoid any negative surprises on check-in day!
â†’ Here is where to find this new section on the Smily interfaceðŸ‘‡
-> Then click on the pen to edit/add an optional description;

â„¹ï¸ Next steps and important information
For those using this via Airbnb, we've already fetched your existing listing expectations from Airbnb as of February 9th.


To ensure data alignment with this new feature, please update any changes made since then before our official release on February 21st via the Smily interface;
â†’ On February 21st, we will start the synchronisation of your data from Smily to the Airbnb application. Any values not reflected on the Smily interface will be removed from Airbnb.


Please note, that the Smily interface allows a description limit of 100 characters compared to Airbnb's 300. Should this affect your listings, we'll reach out individually.
â†’ if you cannot update your description before February 21st, let us know please;


Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble with canvas.]]></title>
        <id>https://slack-status.com/2024-02/804d3a52e234f2fa</id>
        <link href="https://slack-status.com/2024-02/804d3a52e234f2fa"/>
        <updated>2024-02-13T22:15:24.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 12, 2024 from 6:00 AM PST to 8:32 AM PST, some users experienced latency when loading canvases or errors related to editing and saving canvases.

Upon investigation we determined that this was caused by rate limits put in place by an upstream provider. We restarted the impacted endpoints to mitigate the issue immediately, which resolved the issue for all affected users. We're continuing to partner with our upstream provider to reduce the likelihood of this occurring in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion app experienced a brief outage due to a deployment that didn't function as expected]]></title>
        <id>https://status.notion.so/incidents/2b5f3nmpht76</id>
        <link href="https://status.notion.so/incidents/2b5f3nmpht76"/>
        <updated>2024-02-12T23:34:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 15:34 PST
Resolved - This incident has been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/k0qjh7s64fxk</id>
        <link href="https://www.githubstatus.com/incidents/k0qjh7s64fxk"/>
        <updated>2024-02-12T18:14:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 18:14 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to inciâ€¦]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/vv6z7v5w80yr</id>
        <link href="https://www.githubstatus.com/incidents/vv6z7v5w80yr"/>
        <updated>2024-02-12T12:39:20.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 12:39 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to incidents like this in the future.
Feb 12, 12:29 UTC
Update - We are starting to see recovery based on the signals that the team have been monitoring, following mitigation steps being taken. When confident that recovery is complete, we will resolve this incident.
Feb 12, 12:00 UTC
Update - We are continuing to investigate increased failure rates for Copilot code completion for some users in Europe.
Feb 12, 11:38 UTC
Update - We are investigating reports that GitHub Copilot code completions are not working for some users in Europe.
Feb 12, 11:38 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You can now publish your listings on HomeToGo! ðŸŽ‰]]></title>
        <id>285614</id>
        <link href="https://changelog.bookingsync.com/you-can-now-publish-your-listings-on-hometogo!-285614"/>
        <updated>2024-02-12T11:32:13.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 


We are excited to announce our new partnership with a popular booking platform among travelers: HomeToGo!
  

ðŸ’¡What is HomeToGo?
HomeToGo is a German group founded in 2014 that has become a leader in the vacation rental industry. 
Thanks to its exponential growth, HomeToGo is now an essential booking platform for property managers worldwide. ðŸŒ



ðŸ’¡ Why pushing your listings on HomeToGo?

Quality bookings: 7 days of average Length of Stay and a superior average transaction value.
Access to a broad audience: Particularly from the Germany-Switzerland-Austria region, along with other Northern European countries, guests with high purchasing power.
Flexibility: Hosts can apply their own cancellation policies and terms and conditions.
Direct communication channels with guests: Both during and post-stay.
Enhanced visibility: Through HomeToGo's extensive network of brands.
 
ðŸ‘‰ Install the HomeToGo app here
 
ðŸ’Œ Learn more on how to connect your properties in our dedicated manual page.
  
If you have any questions, please don't hesitate to contact channel.manager@hometogo.com]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/my7r3l9lsqnk</id>
        <link href="https://www.githubstatus.com/incidents/my7r3l9lsqnk"/>
        <updated>2024-02-09T11:28:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  9, 11:28 UTC
Resolved - On February 9, 2024 between 10:34 UTC and 11:24 UTC, the Webhooks service was degraded and 63% of webhooks were delayed by up to 16 minutes with an average delay of 5 minutes. No webhook deliveries were lost. This was due to an issue with an overloaded backend data store that was unable to process network requests fast enough.
We mitigated the incident by manually failing over traffic to healthy hosts.
We are expanding the capacity of the backing store as well as making the Webhooks service more resilient to this kind of issue. 

Feb  9, 11:25 UTC
Update - Webhooks is operating normally.
Feb  9, 11:11 UTC
Update - We are investigating latency in processing webhooks. Customers may see a delay of around 5 minutes at this time. We will continue to keep users updated on progress towards mitigation.
Feb  9, 11:09 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage on eu2.make.com]]></title>
        <id>https://status.make.com/incidents/ldktvddm239d</id>
        <link href="https://status.make.com/incidents/ldktvddm239d"/>
        <updated>2024-02-08T16:03:02.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 17:03 CET
Resolved - This incident has been resolved.
Feb  8, 14:41 CET
Update - We are continuing to monitor for any further issues.
Feb  8, 14:39 CET
Monitoring - We have identified and resolved the issue associated with the recent code change. Stability has been restored to eu2.make.com since 14:20 CET. We will continue to monitor the situation for the next several hours.
Feb  8, 14:21 CET
Investigating - We are currently experiencing issues with the eu2.make.com zone. Login to the webpage is unreachable, and scenario executions are affected. We are actively investigating the issue and will provide an update within the next 60 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unresponsive shared hooks]]></title>
        <id>https://status.make.com/incidents/wtq70l2b2g74</id>
        <link href="https://status.make.com/incidents/wtq70l2b2g74"/>
        <updated>2024-02-08T12:52:07.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 13:52 CET
Resolved - This incident has been resolved.
Feb  8, 10:04 CET
Update - The fix has been successfully rolled out. Webhooks and mailhooks should now be fully functional for eu1.make.com. We will continue to monitor the situation for the next few hours.
Feb  8, 09:33 CET
Update - While monitoring the issue, we observed certain problems with mailhooks and webhooks. We have identified the issue and are working on rolling out the fix. The issue pertains only to eu1.make.com, the remaining zones are functional.
Feb  8, 01:19 CET
Monitoring - A fix has been implemented and we are monitoring the current behavior.
Feb  8, 00:45 CET
Investigating - We are currently experiencing issues with some of our services responsible for shared hooks on our clusters.
Shared hooks might currently be unresponsive. We are actively investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be having trouble connecting to Slack.]]></title>
        <id>https://slack-status.com/2024-02/558e3bb8ce654659</id>
        <link href="https://slack-status.com/2024-02/558e3bb8ce654659"/>
        <updated>2024-02-08T02:06:20.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:
On February 7, 2024 from around 9:23 AM PST to 9:51 AM PST, a small percentage of users may have experienced issues connecting to Slack or searching within Slack. 

Our automated alerting systems detected an unusual spike in traffic to one of our servers. We carried out a thorough investigation, but the load subsided naturally as we worked, resolving the issue for all affected users.

While we did not take any remedial action in this case, we continued our investigation to better understand the problem and potential mitigation strategies for similar issues in future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Resize Events]]></title>
        <id>https://status.digitalocean.com/incidents/w1sspyd728kg</id>
        <link href="https://status.digitalocean.com/incidents/w1sspyd728kg"/>
        <updated>2024-02-07T20:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 20:54 UTC
Resolved - As of 19:37 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Droplet resize events in all regions. All the Droplet resize events should now be succeeding normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb  7, 19:41 UTC
Monitoring - Our Engineering team has fully deployed the fix for the issue with Droplet resizes and is now monitoring the situation. Users can now retry Droplet resizes and should see them succeed.
We'll post another update once we confirm the fix resolves this incident.
Feb  7, 15:49 UTC
Identified - Our Engineering team has identified the root cause of the issue with failed Droplet resizes and a fix is in the process of being deployed. 
Users attempting to resize Droplets where the image for the Droplet has been deleted or retired (e.g. a user created a Droplet from a Snapshot, but later deleted that Snapshot) will see failures. All other resizes are succeeding normally.
We'll post another update once the fix has completed deployment.
Feb  7, 15:32 UTC
Investigating - Our Engineering team is investigating an uptick in failed Droplet resizes, beginning Feb 6, 20:57 UTC. 
During this time, some users may experience failures when attempting to resize Droplets, in all regions. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Async Request Processing]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz"/>
        <updated>2024-02-07T17:48:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 09:48 PST
Resolved - This incident has been resolved.
Feb  7, 08:55 PST
Monitoring - Our Async Request Processing system had an issue this morning, which resulted in increased delays between the time a request was enqueued and when it was processed, and may have resulted in an elevated processing failure rate. This began around 8:10 AM PST, and was resolved by 9:15 AM PST.
We are still actively monitoring, but are not expecting any ongoing issues at this time.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in NYC Regions]]></title>
        <id>https://status.digitalocean.com/incidents/y2cm9wm1bvv7</id>
        <link href="https://status.digitalocean.com/incidents/y2cm9wm1bvv7"/>
        <updated>2024-02-07T07:21:30.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 07:21 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting network latency in our NYC regions.
The issues were a direct result of traffic congestion from our upstream providers, which has been repaired. Users should no longer experience packet loss or increased latency while interacting with their resources in the NYC regions.
We sincerely apologize and thank you for your patience as we worked through this issue. In case of any questions or concerns, please open a ticket with our Support team.
Feb  7, 04:03 UTC
Investigating - Our Engineering team is investigating multiple reports of network latency when connecting to services in our NYC regions. During this time, users may experience intermittent packet loss or increased latency while interacting with their resources in the NYC regions.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/1jwy0vy3hjfb</id>
        <link href="https://status.digitalocean.com/incidents/1jwy0vy3hjfb"/>
        <updated>2024-02-06T22:21:17.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 22:21 UTC
Completed - The scheduled maintenance has been completed.
Feb  6, 17:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  2, 20:23 UTC
Scheduled - Start Time: 17:00 UTC Feb 6, 2024
End Time: 00:00 UTC Feb 7, 2024
During the above time, our Engineering Team will be performing maintenance to failover some internal databases from one cluster to another.
Extensive testing has been conducted to ensure this maintenance will be successful and result in minimal impact to DigitalOcean users. The actual failover is estimated to take less than 3 seconds.
Existing infrastructure, including Droplets and Droplet-based services, should continue running without issue. There is no network disruption to existing services expected as part of this maintenance. However, there are dependencies on multiple services. During the failover, there may be customer impacts that should be brief and transitory. The following actions may experience increased latency or failure rates during the maintenance period:
- API calls to the DigitalOcean public API 
- Events for Droplets and Droplet-based services such as create, delete, power on/off, resize, etc 
- Control operations through the DigitalOcean Cloud Control Panel 
Multiple teams will be engaged to keep downtime to a minimum and mitigate any impact that does occur. Weâ€™ll post updates here for any unexpected changes to this scheduled maintenance, as well as progress updates during the maintenance itself.
If you have any questions or concerns, please reach out to the Support team from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automations failing]]></title>
        <id>https://status.notion.so/incidents/2wtslqc6bv54</id>
        <link href="https://status.notion.so/incidents/2wtslqc6bv54"/>
        <updated>2024-02-06T17:58:31.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 09:58 PST
Resolved - This incident has been resolved.
Feb  6, 09:16 PST
Update - We are working hard to resolve the issue for you. Thank you for your continuous patience.
Feb  6, 07:00 PST
Update - We are still working on fixing the issue and appreciate your patience.
Feb  6, 05:05 PST
Identified - We are experiencing an issue with the Notion's database automations service that cause automation actions to fail or experience delays. Our engineers have identified this & are working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A number of South Korean users inadvertently banned from using vital APIs in Notion, potentially affecting some functionalities within the app.]]></title>
        <id>https://status.notion.so/incidents/f88y9tn65kl1</id>
        <link href="https://status.notion.so/incidents/f88y9tn65kl1"/>
        <updated>2024-02-06T00:32:44.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:32 PST
Resolved - This incident has been resolved.
Feb  5, 16:03 PST
Update - We are continuing to investigate this issue.
Feb  5, 16:02 PST
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/dmybzf132ygz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/dmybzf132ygz"/>
        <updated>2024-02-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:00 PST
Resolved - We've identified spikes in errors across multiple endpoints that have occurred over the past day. The two distinct spikes were:
- February 5th, 4:10 PM to 4:30 PM (PST)
- February 6th, 6:30 AM to 7:30 AM (PST)
The issue is now resolved, and we don't expect any ongoing impact.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automation Failures]]></title>
        <id>https://status.notion.so/incidents/w7ppqptykrdz</id>
        <link href="https://status.notion.so/incidents/w7ppqptykrdz"/>
        <updated>2024-02-05T14:56:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 06:56 PST
Resolved - Between 3:00 UTC - 14:22 UTC, some users may have experienced database automation failures or delays in actions being triggered.
This is now resolved, and the affected automations have completed. Database automation services are now running as normal. 
Thank you for your patience while we worked through this issue.
Feb  5, 05:30 PST
Monitoring - Notion's database automation service began experiencing problems at approximately 3 AM UTC today, which caused a number of automation actions to fail or experience delays. 
Our engineers have identified a fix, and are now retrying automations that failed to trigger during this period. 
We will continue to monitor the situation and share an update when this is fully resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/3k40h28fkb2r</id>
        <link href="https://www.githubstatus.com/incidents/3k40h28fkb2r"/>
        <updated>2024-02-05T09:53:13.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 09:53 UTC
Resolved - On 2024-02-05, from 09:26 to 13:20 UTC some GitHub customers experienced errors when trying to download raw files. An overloaded server exposed a bug, causing us to return HTTP 500 error codes.
The issue was mitigated by disabling the server and re-routing traffic. We are implementing improvements to our routing logic to more quickly avoid troublesome hosts in the future. 

Feb  5, 09:40 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/1q1jhbr85zvv</id>
        <link href="https://status.digitalocean.com/incidents/1q1jhbr85zvv"/>
        <updated>2024-02-05T05:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 05:54 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting Spaces CDN in our SGP1 region.
From 03:02 UTC - 05:15 UTC, users were experiencing errors for objects served over the CDN.
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.
Feb  5, 05:10 UTC
Monitoring - Our Engineering team has applied a fix to mitigate the issue related to the Spaces CDN in the SGP1 region. Users should no longer experience errors for objects served over the CDN. 
We apologize for the inconvenience and will post another update once we're confident that the issue is fully resolved.
Feb  5, 04:52 UTC
Identified - From 03:02 UTC, our Engineering team has identified an issue with the Spaces CDN in our SGP1 region and is actively working on a fix. During this time, users may experience errors for objects served over the CDN. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
</feed>