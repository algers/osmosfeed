<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-01-22T00:23:51.231Z</id>
    <title>osmos::feed</title>
    <updated>2024-01-22T00:23:51.231Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/7ck5966p1073</id>
        <link href="https://www.githubstatus.com/incidents/7ck5966p1073"/>
        <updated>2024-01-21T09:34:34.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 09:34 UTC
Resolved - This incident has been resolved.
Jan 21, 09:32 UTC
Update - We continue to work on mitigating the issues with Codespace resumes in West Europe.
Jan 21, 08:58 UTC
Update - We continue to work on mitigating the issues with Codespace resumes in West Europe.
Jan 21, 08:27 UTC
Update - We continue to work on mitigating the issues with Codespace resumes in West Europe.
Jan 21, 07:55 UTC
Update - Codespace creation has fully recovered in all regions. We are still mitigating issues with Codespace resumes in West Europe.
Jan 21, 07:24 UTC
Update - We continue to see recovery in most regions. We are still working on mitigating the issue impacting customers in West Europe.
Jan 21, 06:50 UTC
Update - We are continuing to monitor recovery in the affected regions.
Jan 21, 06:15 UTC
Update - We are seeing signs of recovery in the affected regions.
Jan 21, 06:15 UTC
Update - Codespaces is experiencing degraded performance. We are continuing to investigate.
Jan 21, 06:00 UTC
Update - We continue to work on mitigating the underlying the issue impacting Codespaces customers.
Jan 21, 05:24 UTC
Update - We continue to work on mitigating the underlying the issue impacting Codespaces customers.
Jan 21, 04:50 UTC
Update - Around 15% of Codespaces customers are unable to create or resume their codespaces. We are continuing efforts to mitigate the issue.
Jan 21, 04:21 UTC
Update - Codespaces is experiencing degraded availability. We are continuing to investigate.
Jan 21, 04:15 UTC
Update - We have identified the issue impacting Codespaces customers in multiple regions and are working on mitigation.
Jan 21, 03:38 UTC
Update - We are experiencing elevated error rates in multiple regions and are currently investigating.
Jan 21, 03:38 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/hmvr5kpgzc45</id>
        <link href="https://www.githubstatus.com/incidents/hmvr5kpgzc45"/>
        <updated>2024-01-21T06:19:52.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 06:19 UTC
Resolved - This incident has been resolved.
Jan 21, 05:54 UTC
Update - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.
Jan 21, 05:26 UTC
Update - We have mitigated the issues impacting Actions Larger Runners. We are still experiencing delays starting normal jobs, and are continuing to investigate.
Jan 21, 04:53 UTC
Update - The team has identified the cause of the issues with Actions Larger Runners and has begun mitigation.
Jan 21, 04:16 UTC
Update - The team continues to investigate issues with some Actions jobs being queued for a long time and a percentage of jobs failing. We will continue providing updates on the progress towards mitigation.
Jan 21, 03:45 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet rebuild]]></title>
        <id>https://status.digitalocean.com/incidents/x0f8bvr688rs</id>
        <link href="https://status.digitalocean.com/incidents/x0f8bvr688rs"/>
        <updated>2024-01-17T09:34:03.000Z</updated>
        <summary type="html"><![CDATA[Jan 17, 09:34 UTC
Resolved - As of 09:17 UTC, our Engineering team has confirmed the full resolution of the issue impacting the Droplet rebuild via the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 17, 09:22 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue impacting the Droplet rebuild via Cloud Control Panel and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues when rebuilding Droplets from the Cloud Control Panel. We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 17, 08:26 UTC
Identified - Our Engineering team has identified the cause of the issue impacting the Droplet rebuild via the Cloud Control Panel and is actively working on a fix. During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel. We will post an update as soon as additional information is available.
Jan 17, 07:15 UTC
Investigating - As of 06:50 UTC, our Engineering team is investigating an issue impacting the Droplet rebuild via the Cloud Control Panel.
During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Functionality]]></title>
        <id>https://status.digitalocean.com/incidents/j4jwmwl3szxz</id>
        <link href="https://status.digitalocean.com/incidents/j4jwmwl3szxz"/>
        <updated>2024-01-16T13:34:24.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 13:34 UTC
Resolved - Our Engineering team has resolved the issue impacting multiple spaces-related functionalities. From approximately 10:30 UTC - 13:30 UTC, users may have experienced issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API. Spaces-related functionalities should now be operating normally.
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Jan 16, 12:51 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue affecting multiple Spaces-related functionalities and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues with Spaces-related functionalities. 
We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 16, 12:03 UTC
Identified - Our Engineering team has identified the issue affecting multiple Spaces-related functionalities and is actively working on a fix.
During this time, users may experience issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API.
Additionally, this may also impact Container Registry creation and issues with transferring images between regions. 
We apologize for the inconvenience and will share an update once we have more information.
Jan 16, 10:54 UTC
Investigating - As of 10:30 UTC, our Engineering team is investigating an issue with multiple Spaces functionalities via the Cloud Control Panel. 
During this time, users may experience errors when attempting to delete objects via the Cloud Control Panel. At this moment we are investigating the exact impact and will share more information as soon as we have it.
We apologize for the inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functions]]></title>
        <id>https://status.digitalocean.com/incidents/47dqh6pzqybt</id>
        <link href="https://status.digitalocean.com/incidents/47dqh6pzqybt"/>
        <updated>2024-01-16T09:30:15.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 09:30 UTC
Resolved - As of 08:50 UTC, our Engineering team has confirmed the full resolution of the issue impacting the ability to access and manage Functions through the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 16, 08:59 UTC
Monitoring - Our Engineering team has been able to mitigate the issue related to the access and operations with Functions through the Cloud Control Panel.
Users should no longer face any problems in accessing or operating Functions using the Cloud Control Panel. 
We apologize for the inconvenience. We are monitoring the situation and will post an update once we confirm this incident is fully resolved.
Jan 16, 08:17 UTC
Investigating - As of 03:00 AM UTC, our Engineering team is investigating an issue impacting Functions. 
During this time, users may experience issues with accessing Functions via the Cloud Control Panel. At this time, API operations shouldn't be impacted and should continue to function as intended. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🆕 New partnership announcement with KeyNest!]]></title>
        <id>283413</id>
        <link href="https://changelog.bookingsync.com/new-partnership-announcement-with-keynest!-283413"/>
        <updated>2024-01-16T06:58:43.000Z</updated>
        <summary type="html"><![CDATA[New!
  


We are thrilled to announce a new partnership that will revolutionise key management for vacation rental hosts and property managers like you. 🔑
  

💡What is KeyNest Points?
KeyNest Points is a global network of over 5,500 locations where you can securely store and exchange keys with ease. No more hassles of on-site visits or high installation costs.
Most KeyNest Points are open 24/7, ensuring flexibility for key exchanges at your convenience. Discover your nearest Point and its opening hours on the interactive map provided.

 


🏠 KeyNest Points - Your Trusted Partners
These points are typically local businesses such as convenience stores, cafes, hotels, or petrol stations, conveniently situated near your properties.
Each KeyNest Point is managed by trained staff, ensuring instant and secure key exchanges. Your peace of mind is paramount.

🔤 How Does it Work?

Here's a quick overview of how KeyNest Points operate:
Drop your keys at a local KeyNest Point.
Your keys are tagged and logged into the system for real-time tracking.
Staff at the Point securely stores your keys.
Send the location of the key and the code to your guests within your automated Smily messages.
Guests or cleaners visit the Point, show their safety code, and receive the key.
Upon return, they drop the key back at the same Point, and you are notified.
 
👉 Install the Keynest app here
 
💌 Learn more on our dedicated manual page.
  
If you have any questions, please don't hesitate to contact support@keynest.com.]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may not be able to configure 2FA]]></title>
        <id>https://status.slack.com//2024-01/9301a4fb7cc577ea</id>
        <link href="https://status.slack.com//2024-01/9301a4fb7cc577ea"/>
        <updated>2024-01-15T23:51:07.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:26 AM PST on January 12, 2024 to 10:04 AM PST on January 15, 2024, some users were unable to configure 2FA on their accounts. We were made aware of this after a spike in reports early in the morning of Monday, January 15.


Upon investigation, this issue was traced back to a recent code change which we discovered was preventing users from being redirected back to the 2FA configuration page after entering their password during the 2FA setup process. We immediately reverted this change which fully resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degraded behavior when moving pages in bulk]]></title>
        <id>https://status.notion.so/incidents/wn8zmpphxwr9</id>
        <link href="https://status.notion.so/incidents/wn8zmpphxwr9"/>
        <updated>2024-01-15T19:19:20.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 11:19 PST
Resolved - We've pushed a fix for this issue now and users can move pages in bulk again without errors. We apologize for the earlier disruption and thank you for bearing with us through this. 
Please open Notion and press Cmd/Ctrl + Shift + R to reload the latest changes before trying to move pages again.
Jan 15, 10:24 PST
Identified - Our team has identified the cause of problems when moving pages in bulk across Notion databases and is working on a fix. We will share further updates as soon as the problem is resolved.
Jan 15, 07:53 PST
Investigating - Users may be experiencing degraded behavior when moving pages in bulk across Notion databases. We are investigating this issue and will share an update as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monday app not working]]></title>
        <id>https://status.make.com/incidents/h3gvmlbldj1q</id>
        <link href="https://status.make.com/incidents/h3gvmlbldj1q"/>
        <updated>2024-01-15T15:18:40.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 16:18 CET
Resolved - We have reactivated the affected scenarios. Please note that this reactivation will not be visible in the scenario logs. Currently, the Monday app is fully operational.
Jan 15, 11:42 CET
Update - A fix has been rolled and we are investigating options to re-enable affected scenarios automatically.
Jan 15, 10:27 CET
Monitoring - A fix has been implemented and we are monitoring the results.
Jan 15, 09:47 CET
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue with MFA Login]]></title>
        <id>https://status.notion.so/incidents/qbkb93dwrm9r</id>
        <link href="https://status.notion.so/incidents/qbkb93dwrm9r"/>
        <updated>2024-01-12T06:56:42.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 22:56 PST
Resolved - This incident has been resolved.
Jan 11, 20:52 PST
Identified - We are experiencing issues with MFA  login method. The team has identified the cause and is working on the fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Networking]]></title>
        <id>https://status.digitalocean.com/incidents/33vqf05m8396</id>
        <link href="https://status.digitalocean.com/incidents/33vqf05m8396"/>
        <updated>2024-01-11T00:18:25.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 00:18 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
From approximately 20:15 - 21:45 UTC, DigitalOcean experienced a global networking issue that impacted multiple services and products. Users saw increased error rates and latency for event processing, accessing our Cloud Control Panel/API, applying Cloud Firewall policies, accessing www.digitalocean.com and our Community site, and DNS resolution. Additionally, users saw timeouts/increased latency for networking requests to Droplets and Droplet-based services, as well as connections to existing App Platform Apps. 
We sincerely apologize for the disruption. If you continue to experience issues or have questions, please reach out to our Support team by opening a ticket from within your account. …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New fix for your booking.com reservations]]></title>
        <id>283019</id>
        <link href="https://changelog.bookingsync.com/new-fix-for-your-booking-com-reservations-283019"/>
        <updated>2024-01-10T17:36:04.000Z</updated>
        <summary type="html"><![CDATA[New!
 
Improvement
  
We're thrilled to share a new feature we’ve just released for booking.com properties.
What's New?
The rate rule Booking at least 'x' day ahead is now synchronised on booking.com channel;
Why Does This Matter?
You were facing the obstacle where Guests were making a booking for the same day or 'x' days (based on your settings) even though the setting to prevent that was activated;
-> It won’t happen anymore.
What's Next?
If you face any obstacle with this setting, reach out to us; 
 
Improvement
  
We are and will be working on more improvements to make your day-to-day easier :)
--
💡 For more detailed information, simply click here;
Let's embark on this journey together to make your vacation rental dreams a reality 🚀
Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, API Requests, Pull Requests, Actions, Pages, Git Operations, Webhooks, Packages and Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/pxg3dz4yg7lp</id>
        <link href="https://www.githubstatus.com/incidents/pxg3dz4yg7lp"/>
        <updated>2024-01-09T14:40:45.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 14:40 UTC
Resolved - On January 9 between 12:45 and 13:56 UTC, services in one of our three sites experienced elevated latency for connections.  This led to a sustained period of timed out requests across a number of services, including but not limited to our git backend.  An average of 5% and max of 10% of requests failed with a 5xx response or timed out during this period.  This was caused by a combination of events that led to connection limits being hit in load balancer proxies in that site.  An upgrade of hosts was in flight, which meant a subset of proxy hosts were draining and coming offline as the upgrade rolled through the fleet.  A config change event also triggered a connection reset across all services in that site.  These events are commonplace, but led to a spike in c…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Database Operations]]></title>
        <id>https://status.digitalocean.com/incidents/8fm83lgfsdhm</id>
        <link href="https://status.digitalocean.com/incidents/8fm83lgfsdhm"/>
        <updated>2024-01-09T12:55:23.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 12:55 UTC
Resolved - As of 12:30  UTC, our Engineering team has resolved the issue impacting CRUD (create, read, update, delete) operations for Managed Database Clusters in all the regions.
Everything should now be functioning normally. We appreciate your patience throughout the process.
If you continue to experience problems, please open a ticket with our support team.
Jan  9, 10:17 UTC
Monitoring - Our Engineering team has confirmed that the action taken to mitigate the recurrence of the issue is successful. Users should no longer experience errors in CRUD (create, read, update, delete) operations for Managed Database Clusters in all regions, via both the Cloud Control Panel and API requests.
We are actively monitoring the situation to ensure stability and will provide an update …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/dxthh9653wkk</id>
        <link href="https://www.githubstatus.com/incidents/dxthh9653wkk"/>
        <updated>2024-01-09T05:44:37.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 05:44 UTC
Resolved - This incident has been resolved.
Jan  9, 05:40 UTC
Update - Audit logs streaming is currently unavailable due to a misconfiguration issue. The root cause is understood and our engineers will be updating the broken configuration shortly.
Jan  9, 04:59 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/35pgg0gc75nv</id>
        <link href="https://www.githubstatus.com/incidents/35pgg0gc75nv"/>
        <updated>2024-01-08T23:41:03.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 23:41 UTC
Resolved - This incident has been resolved.
Jan  8, 22:37 UTC
Update - The initial mitigation we attempted did not fully resolve this issue.  We are continuing to investigate and will provide an update as soon as possible.
Jan  8, 21:55 UTC
Update - We are engaged on the issue and continuing to work toward a mitigation. Please continue to fallback to VS Code desktop for port forwarding workflows.
Jan  8, 21:03 UTC
Update - Mitigation of degraded Codespaces port forwarding in is progress.  In the meantime, please continue to use VS Code Desktop for port forwarding.
Jan  8, 20:22 UTC
Update - We are actively mitigating degraded performance of Codespaces port forwarding in the web and GitHub CLI.  Codespaces port forwarding on VS Code Desktop is unaffected.
Jan  8, 20:22 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/crg2qtnbmb1t</id>
        <link href="https://status.rippling.com/incidents/crg2qtnbmb1t"/>
        <updated>2024-01-08T18:26:01.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 18:26 UTC
Resolved - The Rippling application was inaccessible from 9:39am to 10:06am PST; it has recovered and is now performing normally. The root cause is an incident with Cloudflare: https://www.cloudflarestatus.com/
Jan  8, 18:11 UTC
Monitoring - The issue accessing Rippling has been mitigated, and we are monitoring the results.
Jan  8, 17:56 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry and App Platform in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/88sl0dldq6dh</id>
        <link href="https://status.digitalocean.com/incidents/88sl0dldq6dh"/>
        <updated>2024-01-08T15:57:06.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 15:57 UTC
Resolved - As of 15:00 UTC, Our engineering team has resolved the issue impacting Container Registry  and App platform builds in various regions including AMS3, LON1 and FRA1 regions. 
Everything should now be functioning normally. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Jan  8, 15:13 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting Container Registry App platform builds in AMS3, LON1 and FRA1 regions. User should not be facing any issues while interacting with their Container registries and also while building their Apps. 
We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. 
Thank you for your patience and we apologize for the inconvenience.
Jan  8, 14:15 UTC
Investigating - Our Engineering team is investigating an issue with DigitalOcean Container Registry service in our AMS3 and LON1 regions. 
During this time a subset of customers may experience latency while interacting with the Container Registries. This is also impacting App Platform builds and users may encounter delays in while building their Apps and could potentially experience timeout errors in builds as a result. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2024, here we are! Happy New Year from all of us at Smily! 🎉]]></title>
        <id>282774</id>
        <link href="https://changelog.bookingsync.com/2024-here-we-are!-happy-new-year-from-all-of-us-at-smily!-282774"/>
        <updated>2024-01-08T09:34:22.000Z</updated>
        <summary type="html"><![CDATA[Communications
  

As we step into this new year, we want to take a moment to express our heartfelt gratitude for your trust and partnership. Your journey is at the heart of everything we do, and it's been an incredible experience growing alongside each of you.
This past year has been filled with challenges but also triumphs, and through it all, our Property Managers and Owners have always been our main inspiration. We're honored to be a part of your story, helping you manage and grow your vacation rental business.
Looking ahead, we're excited to continue our journey together. With every challenge comes an opportunity, and we're here to support you every step of the way. Let's make this year one of progress, success, and shared smiles!
Hereby some of Smily's milestones and snapshot of our 2023 key achievements:
Increase efficient communication: enhanced Inbox filters for efficient guest communication, enabling quick searches by conversation status, assignee, guest, booking stage, and more.
Powerful partnerships: Collaborations with Maeva and PlumGuide have opened new horizons and more OTA distribution channels!
Guest experience & security: From synchronized check-in types to robust website https updates, we're elevating the guest journey.
Sustainable visibility: Synchronization of our eco-amenities to Holidu to boost visibility and conversion rates!
Reviewed payment processes: With easier Stripe account onboarding and better integration but also by streamlining our billing system, managing finances should be smoother than ever.
Review management made easy: Our AI-powered Review Response and Automated Guest Reviews are setting new standards in reputation management.
Tech Marvels: Our focus on modernization and security has supercharged system stability and performance. Don't miss the insights on our new Tech Blog.
Once again, here's to a year filled with happiness, health, and prosperity. Let's keep building dreams and sharing smiles with every soul.
Happy New Year! 🌟
With love and warmth,
Your Smily Team]]></summary>
        <author>
            <name>Ella, Chief Customer Officer (CCO) &amp; Cofounder</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Clusters - Authentication Errors]]></title>
        <id>https://status.digitalocean.com/incidents/x0l7xgm3bc91</id>
        <link href="https://status.digitalocean.com/incidents/x0l7xgm3bc91"/>
        <updated>2024-01-08T00:32:13.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 00:32 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue with authentication failures on Managed Kubernetes Clusters.
From 19:26 - 23:29 UTC, users saw errors accessing and performing actions on Managed Kubernetes Clusters, due to authentication failures. All services are now operating normally.  
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!
Jan  8, 00:01 UTC
Monitoring - Our Engineering team has confirmed that the action taken to mitigate the impact of this incident is successful and customers are no longer seeing issues accessing or performing actions on Managed Kubernetes Clusters. 
We'll monitor the situation for a short while and will post a final update once we confirm full resolution.
Jan  7, 23:32 UTC
Identified - Our Engineering team has taken action to mitigate the impact of this incident and internal tests on impacted clusters are now passing. At this time, users should start to see recovery and be able to access clusters normally. 
We are still investigating root cause and ensuring this mitigation will continue to be successful. We'll provide another update as soon as possible.
Jan  7, 22:02 UTC
Investigating - Our Engineering team is investigating customer reports of errors when trying to perform actions on Managed Kubernetes Clusters using kubectl. At this time, customers may see errors indicating that the API token is unauthorized or missing credentials. We will provide an update as soon as we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2024-001]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2024-001</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2024-001"/>
        <updated>2024-01-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: On Windows before Tailscale version 1.52 and on Linux before
Tailscale 1.54, the tailscale serve and tailscale funnel features allowed
users to serve the contents of directories that their user account could not
access, but which the tailscaled service process could.
What happened?
A user could escalate their own file read access by running, for example,
tailscale.exe serve http / C:\, and then browsing to the local HTTP endpoint.
The issue can also occur on Linux if the local administrator enabled an operator
user ID with tailscale up --operator=$USER, as the $USER account could
serve itself files that it could not normally read.
Who is affected?
Owners of Windows deployments for which the users of Tailscale nodes do not also
have OS-level administrative access, and owners of Linux deployments where the
administrator enabled non-root --operator access.
This issue can only be triggered by a local user and cannot be triggered
remotely.
What is the impact?
This issue enables local privilege escalation (file read access). Access to
certain system files (such as /etc/shadow on Linux) can then be used to obtain
full administrative control over the host.
What do I need to do?
On Windows 10 and later, upgrade to Tailscale 1.52 (released 30 October
2023) or later, which resolves the issue.
On Windows 7 and 8, upgrade to Tailscale 1.44.3 (released 8 Jan
2024), which resolves the issue.
On Linux, upgrade to 1.54 (released 15 November 2023) or later,
which resolves the issue.
The best practice is to run the latest stable version, which as of this writing
is 1.56.1. Consider turning on automatic updates.
Use Tailscale ACLs to control the availability of Funnel.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues Affecting the Partner Portal]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/g0r1ty50zxsz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/g0r1ty50zxsz"/>
        <updated>2024-01-06T03:33:46.000Z</updated>
        <summary type="html"><![CDATA[Jan  5, 19:33 PST
Resolved - This incident has been resolved. Please contact us if you still can't see API-connected listings on the Partner Portal.
Jan  4, 22:32 PST
Update - We are still working on this issue.
You may still view listing-related events by filtering by Listing ID on the Partner Portal Events Dashboard.
You may also utilise GET requests to view the current snapshots of a listing if required.
We apologise for the inconvenience caused.
Jan  4, 08:53 PST
Update - We are still working on this issue, apologies for the inconvenience.
Jan  3, 16:00 PST
Identified - We are investigating an issue affecting our Diagnostic tools in the Partner Portal that started today Jan 3, 2024 around 12:00PM PST. Currently, you might not see any API-connected listings on the Listings dashboard, and in a listing's overview page, you might see the following message "The listing is not API connected". Please note that no listings were disconnected due to this incident; this issue affects the Partner Portal view only.
We are actively working to resolve this issue, and will post more updates as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Performance Issues]]></title>
        <id>https://status.notion.so/incidents/4kbzwnzxc4df</id>
        <link href="https://status.notion.so/incidents/4kbzwnzxc4df"/>
        <updated>2024-01-05T02:06:40.000Z</updated>
        <summary type="html"><![CDATA[Jan  4, 18:06 PST
Resolved - This incident has been resolved. 
Time of incident was approximately from 5pm PDT - 6pm PDT.
Jan  4, 17:32 PST
Investigating - We are experiencing latency issues with our collections service, so users may experience issues loading databases. 
We're actively looking into this, and we'll provide updates when we get more information.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Erroneous emails being sent to IT admins about user account privileges on Windows devices]]></title>
        <id>https://status.rippling.com/incidents/5qwcw06j2w1b</id>
        <link href="https://status.rippling.com/incidents/5qwcw06j2w1b"/>
        <updated>2024-01-04T22:53:03.000Z</updated>
        <summary type="html"><![CDATA[Jan  4, 22:53 UTC
Resolved - This incident has been resolved.
Jan  4, 22:45 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Jan  4, 19:22 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Settings in Cloud Control Panel Unavailable]]></title>
        <id>https://status.digitalocean.com/incidents/1pbm98zxkh8w</id>
        <link href="https://status.digitalocean.com/incidents/1pbm98zxkh8w"/>
        <updated>2024-01-04T15:59:14.000Z</updated>
        <summary type="html"><![CDATA[Jan  4, 15:59 UTC
Resolved - From approximately 15:00 - 15:35 UTC, users saw a 404 error when navigating to the Settings page in our Cloud Control Panel. 
Our Engineering team has confirmed full resolution of the issue. 
If you continue to experience issues, please open a Support ticket from within your account.
Jan  4, 15:39 UTC
Monitoring - Our Engineering team has identified the root cause of the issue with the Settings portion of our Cloud Control Panel throwing a 404 and has implemented a fix. At this time, users should be able to navigate to the Settings tab or directly to https://cloud.digitalocean.com/account/team normally. 
We are monitoring the fix and will post a final update once we confirm full resolution.
Jan  4, 15:24 UTC
Investigating - Our Engineering team is investigating an issue with the Settings portion of our Cloud Control Panel. At this time, users navigating to the Settings tab or directly to https://cloud.digitalocean.com/account/team will see a 404 error. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/zzy7pdshgqk0</id>
        <link href="https://www.githubstatus.com/incidents/zzy7pdshgqk0"/>
        <updated>2024-01-03T17:05:05.000Z</updated>
        <summary type="html"><![CDATA[Jan  3, 17:05 UTC
Resolved - From 10:30 to 16:15 UTC on January 3rd, our `/settings/emails` page experienced downtime due to an outage in our contact permissions system. During this time customers were unable to update their email subscription preferences.
This system plays a crucial role in managing email subscriptions and the contactability of GitHub customers by our marketing and sales teams. Unfortunately, the outage prevented the generation of links for managing email subscriptions, resulting in the `/settings/emails` page timing out. Consequently, customers were unable to update their email subscription preferences during this period.
During the incident we implemented graceful degradation on the `/settings/email` page to enable certain functionality, albeit with slower response time…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Availability in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/p6j21rldnxtl</id>
        <link href="https://status.digitalocean.com/incidents/p6j21rldnxtl"/>
        <updated>2023-12-30T12:23:06.000Z</updated>
        <summary type="html"><![CDATA[Dec 30, 12:23 UTC
Resolved - Our Engineering team has resolved the issue impacting Spaces Object Storage in our SGP1 region. From approximately 09:20 UTC - 11:31 UTC, users may have experienced an increased error rate and low performance while using the Spaces system. Spaces should now be operating normally. 
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Dec 30, 11:53 UTC
Monitoring - From 09:20 UTC, our Engineering team observed an issue with Spaces Object Storage availability in the SGP1 region. 
During this time you may have experienced an increased error rate and low performance while using the Spaces system. 
The impact has now subsided and users should no longer be experiencing issues with accessing the Spaces Object Storage.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/1ksntt4bk3bq</id>
        <link href="https://www.githubstatus.com/incidents/1ksntt4bk3bq"/>
        <updated>2023-12-29T21:21:38.000Z</updated>
        <summary type="html"><![CDATA[Dec 29, 21:21 UTC
Resolved - This incident has been resolved.
Dec 29, 21:09 UTC
Update - We are in the process of reverting a change that introduced these failures.
Dec 29, 20:05 UTC
Update - We’re investigating reports of increased failure rates for migrations with GitHub Enterprise Importer and exports using the Organization Migrations REST API.
Dec 29, 20:05 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests]]></title>
        <id>https://www.githubstatus.com/incidents/464b7g25n77j</id>
        <link href="https://www.githubstatus.com/incidents/464b7g25n77j"/>
        <updated>2023-12-29T18:33:52.000Z</updated>
        <summary type="html"><![CDATA[Dec 29, 18:33 UTC
Resolved - This incident has been resolved.
Dec 29, 18:31 UTC
Update - With a mitigation deploying, we see recovery in most API requests and are continuing to monitor full rollout and mitigation.
Dec 29, 18:21 UTC
Update - Secret Scanning and potentially other APIs are returning 500 error responses.  We're working on a mitigation.
Dec 29, 18:17 UTC
Investigating - We are investigating reports of degraded performance for API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/1h37j1mb9xnx</id>
        <link href="https://www.githubstatus.com/incidents/1h37j1mb9xnx"/>
        <updated>2023-12-29T02:04:44.000Z</updated>
        <summary type="html"><![CDATA[Dec 29, 02:04 UTC
Resolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/
Dec 29, 01:41 UTC
Update - Users without an existing valid session are unable to login and will see an error page.  We are working on a mitigation.
Dec 29, 01:41 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/lt4kyp4tgx45</id>
        <link href="https://www.githubstatus.com/incidents/lt4kyp4tgx45"/>
        <updated>2023-12-28T06:57:42.000Z</updated>
        <summary type="html"><![CDATA[Dec 28, 06:57 UTC
Resolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/
Dec 28, 06:49 UTC
Update - We have deployed a fix and email service should be restored shortly.
Dec 28, 06:48 UTC
Update - We are experiencing issues sending some pull request, actions and other notification emails. Some emails may not be received as the result of activity on GitHub. Web and mobile push notifications are not affected.
Dec 28, 06:43 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pages]]></title>
        <id>https://www.githubstatus.com/incidents/hzd6d6xhjpl6</id>
        <link href="https://www.githubstatus.com/incidents/hzd6d6xhjpl6"/>
        <updated>2023-12-27T19:25:23.000Z</updated>
        <summary type="html"><![CDATA[Dec 27, 19:25 UTC
Resolved - This incident has been resolved.
Dec 27, 19:25 UTC
Update - A recent update to an Action that GitHub Pages deployer service relies on impacted that service, and was corrected and redeployed.
Dec 27, 19:12 UTC
Update - We've identified the cause of some Pages errors and are deploying a mitigating fix now.
Dec 27, 19:01 UTC
Update - We continue to investigate issues with Pages, and will continue to keep users updated on progress towards mitigation.
Dec 27, 18:29 UTC
Update - Pages workflow builds which use the actions actions/upload-pages-artifact@v3 and actions/deploy-pages@v4 are currently failing.
Dec 27, 18:29 UTC
Investigating - We are investigating reports of degraded performance for Pages]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degenerated iOS mobile App Performance]]></title>
        <id>https://status.notion.so/incidents/mdxnttqpv9z2</id>
        <link href="https://status.notion.so/incidents/mdxnttqpv9z2"/>
        <updated>2023-12-27T16:41:14.000Z</updated>
        <summary type="html"><![CDATA[Dec 27, 08:41 PST
Resolved - The issue was resolved.
Dec 27, 07:20 PST
Identified - We have identified the issue & are implementing a fix.
Dec 27, 05:30 PST
Investigating - Some users are receiving an error when trying to re-enter the Notion App on an iOS mobile device after having opened a different App in the meantime. We are actively looking into this.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/rp7669f45fjw</id>
        <link href="https://www.githubstatus.com/incidents/rp7669f45fjw"/>
        <updated>2023-12-27T04:00:57.000Z</updated>
        <summary type="html"><![CDATA[Dec 27, 04:00 UTC
Resolved - This incident has been resolved.
Dec 27, 03:57 UTC
Update - We have applied a fix and most customers should now be seeing recovery.
Dec 27, 03:14 UTC
Update - Customers using Codespaces will be unable to connect to existing codespaces, create new ones, or export. We have identified the issue and are working on a remediation.
Dec 27, 03:09 UTC
Update - Codespaces is experiencing degraded availability. We are continuing to investigate.
Dec 27, 02:53 UTC
Update - Codespace services are experiencing connection issues
Dec 27, 02:53 UTC
Investigating - We are investigating reports of degraded availability for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces and API Requests]]></title>
        <id>https://www.githubstatus.com/incidents/xlm9bd5xq5bk</id>
        <link href="https://www.githubstatus.com/incidents/xlm9bd5xq5bk"/>
        <updated>2023-12-27T03:06:56.000Z</updated>
        <summary type="html"><![CDATA[Dec 27, 03:06 UTC
Resolved - This incident has been resolved.
Dec 27, 03:06 UTC
Update - Codespaces is operating normally.
Dec 27, 02:51 UTC
Investigating - We are investigating reports of degraded performance for Codespaces and API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>