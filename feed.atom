<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-11-20T00:22:28.402Z</id>
    <title>osmos::feed</title>
    <updated>2023-11-20T00:22:28.402Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Japan SMS Carrier Maintenance - NTT Docomo]]></title>
        <id>https://status.twilio.com/incidents/lr7phncxm2ts</id>
        <link href="https://status.twilio.com/incidents/lr7phncxm2ts"/>
        <updated>2023-11-20T10:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Nov 20, 02:00 - 04:00 PST
Nov  6, 07:53 PST
Scheduled - The NTT Docomo network in Japan is conducting a planned maintenance from 20 November 2022 at 02:00 PST until 20 November 2022 at 04:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from NTT Docomo handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/cw4f5xl5g245</id>
        <link href="https://status.twilio.com/incidents/cw4f5xl5g245"/>
        <updated>2023-11-20T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Nov 19, 22:00 - 23:00 PST
Nov 16, 23:54 PST
Scheduled - Our SMS carrier partner in the US is conducting an emergency maintenance from 19 November 2023 at 22:00 PST until 19 November 2023 at 23:00 PST. During the maintenance window, there could be intermittent delivery report delay from AT&T US to Twilio Toll-free numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Zealand SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/0cdtcgbnpt3b</id>
        <link href="https://status.twilio.com/incidents/0cdtcgbnpt3b"/>
        <updated>2023-11-20T00:00:19.000Z</updated>
        <summary type="html"><![CDATA[Nov 19, 16:00 PST
Completed - The scheduled maintenance has been completed.
Nov 19, 15:01 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Nov 16, 03:12 PST
Scheduled - Our SMS carrier partner in New Zealand is conducting a planned maintenance from 18 November 2023 at 14:00 PST until 18 November 2023 at 15:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from a subset of New Zealand handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Orange Network in Cameroon]]></title>
        <id>https://status.twilio.com/incidents/zjlmmv9kqpg1</id>
        <link href="https://status.twilio.com/incidents/zjlmmv9kqpg1"/>
        <updated>2023-11-19T22:58:45.000Z</updated>
        <summary type="html"><![CDATA[Nov 19, 14:48 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to to Orange Network in Cameroon. We will continue monitoring the service to ensure a full recovery. We will provide another update in 1 hour or as soon as more information becomes available.
Nov 19, 14:01 PST
Update - We are experiencing SMS delivery delays when sending messages to Orange Network in Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Nov 19, 13:57 PST
Investigating - Our monitoring systems have detected a potential issue with SMS Delivery Delays to Orange Network in Cameroon. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Cubacel in Cuba]]></title>
        <id>https://status.twilio.com/incidents/1wxh70bp86gd</id>
        <link href="https://status.twilio.com/incidents/1wxh70bp86gd"/>
        <updated>2023-11-19T08:34:27.000Z</updated>
        <summary type="html"><![CDATA[Nov 19, 00:34 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Cubacel in Cuba. This incident has been resolved.
Nov 18, 22:57 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Cubacel in Cuba. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Nov 18, 04:42 PST
Update - We continue to experience SMS delivery delays when sending messages to Cubacel in Cuba. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 24 hours or as soon as more information becomes available.
Nov 17, 05:12 PST
Update - We continue to experience SMS delivery delays when sendin…]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue affecting template duplication]]></title>
        <id>https://status.notion.so/incidents/tk0hmlbd3lg9</id>
        <link href="https://status.notion.so/incidents/tk0hmlbd3lg9"/>
        <updated>2023-11-18T00:04:17.000Z</updated>
        <summary type="html"><![CDATA[Nov 17, 16:04 PST
Resolved - Our team has now resolved the issue preventing template duplication, and this is working as normal again. We appreciate your patience while we worked through this issue.
Nov 17, 14:05 PST
Identified - We are experiencing an issue with duplicating published templates and our team is actively working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues with huddles]]></title>
        <id>https://status.slack.com//2023-11/2b97ec921a81988a</id>
        <link href="https://status.slack.com//2023-11/2b97ec921a81988a"/>
        <updated>2023-11-17T21:42:44.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

On November 8, 2023 from 7:55 AM PT until 8:27 PM PT, a small number of users experienced errors when attempting to join or start huddles.


We traced this back to an unexpected spike in a database load for which we quickly saw recovery.


We monitored the situation closely and in an effort to prevent further unexpected error spikes, we implemented a strategic configuration adjustment to optimize the load on our database.


Affected users should no longer experience issues with huddles.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/vvx0wz4pgd9k</id>
        <link href="https://status.notion.so/incidents/vvx0wz4pgd9k"/>
        <updated>2023-11-17T03:11:59.000Z</updated>
        <summary type="html"><![CDATA[Nov 16, 19:11 PST
Resolved - The incident has been resolved. Time of resolution Nov 16 2023 6:46PM PST
Nov 16, 19:09 PST
Identified - Notion AI is down. We are working with them on a fix. Time - Nov 16 2023 6:11PM PST]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent scenario execution errors in eu1, eu2 and us1 zones]]></title>
        <id>https://status.make.com/incidents/d82gd0vnvgz1</id>
        <link href="https://status.make.com/incidents/d82gd0vnvgz1"/>
        <updated>2023-11-16T08:45:00.000Z</updated>
        <summary type="html"><![CDATA[Nov 16, 09:45 CET
Resolved - Due to a configuration error, some scenarios in the eu1.make.com, eu2.make.com and us1.make.com zones may have intermittently failed to execute and in case of multiple consecutive errors get disabled by the system. According to our telemetry, the number of impacted scenarios was very small. Customers may have experienced this behavior between 9:45am and 8:00pm CET. We have addressed the configuration problem and all executions are now stable. We will continue monitoring the situation.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/mnv0g944fncw</id>
        <link href="https://www.githubstatus.com/incidents/mnv0g944fncw"/>
        <updated>2023-11-15T11:34:14.000Z</updated>
        <summary type="html"><![CDATA[Nov 15, 11:34 UTC
Resolved - On 2023-11-15, from 09:44 to 10:42 UTC, some GitHub customers experienced increased latency or errors accessing repo data.
High concurrent access to a specific git object exposed a bug that forced a backend service to perform excessive calculations, overloading the service. Access to this repo was paused while load was re-rerouted, mitigating the problem.
The conditions that triggered the expensive operations have been identified and refactored.
Nov 15, 11:33 UTC
Update - Error rates and performance have returned to normal.
Nov 15, 11:21 UTC
Update - We have identified the source of the issue and have removed the additional load from the service. Sporadic delays in pull request experiences and intermittent 500s are still occurring and impacting a very small percentage of traffic. Next update is expected within 30 minutes.
Nov 15, 11:04 UTC
Update - We are seeing connectivity issues between some of our systems and git backend services. This is causing intermittent error responses and delays in pull request experiences for a very small percentage of traffic. We are investigating mitigations and expect to provide another update within 30 minutes.
Nov 15, 09:50 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack not loading for some users]]></title>
        <id>https://status.slack.com//2023-11/0e7bc0e7a7e7cd87</id>
        <link href="https://status.slack.com//2023-11/0e7bc0e7a7e7cd87"/>
        <updated>2023-11-15T03:01:39.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 3:15 PM PST on November 14, 2023 to around 4:13 PM PST, a small number of customers using the Slack desktop app were unable to connect to Slack. This may have manifested as a "Something's gone awry" error page.


A recent code change inadvertently introduced a logic error that prevented the desktop app from connecting as expected. We reverted this code change as an immediate mitigation step, then rolled out a fix to correct the logic error. 


The fix resolved the issue for all affected customers, restoring full access to Slack.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOKS in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/wpz7gdly2p32</id>
        <link href="https://status.digitalocean.com/incidents/wpz7gdly2p32"/>
        <updated>2023-11-15T01:59:25.000Z</updated>
        <summary type="html"><![CDATA[Nov 15, 01:59 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting DOKS across all regions. 
After the rollout, we are no longer reliant on that affected upstream provider for our DOKS product.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Nov 15, 00:42 UTC
Monitoring - Our Engineering team has completed the rollout to pivot away from the affected upstream provider and we are no longer reliant on that provider for our DOKS product. 
At this time, Users should no longer see any issues with nodes going into not ready states, creating new clusters, or scaling up additional nodes. 
We're monitoring the fix and will post another update once we confirm this issue is fully resolved.
Nov 14, 23:24 UTC
Update - Our Engineering team is currently working to pivot away from the affected upstream provider to mitigate impact from this incident. That fix is rolling out across our fleet and users should start to see conditions on affected clusters improve. 
As soon as the fix is rolled out completely, we'll post another update.
Nov 14, 22:20 UTC
Update - Our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS across all regions which was initially reported for a few regions. 
During this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state across all regions.
We will share an update as soon as we have any information from our upstream provider.
Nov 14, 21:59 UTC
Identified - Beginning around 20:00 UTC, our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS in our multiple regions. 
During this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state. 
We will share an update as soon as we have any information from our upstream provider.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EU2 hooks delayed processing]]></title>
        <id>https://status.make.com/incidents/dzjb57y129qn</id>
        <link href="https://status.make.com/incidents/dzjb57y129qn"/>
        <updated>2023-11-14T17:04:00.000Z</updated>
        <summary type="html"><![CDATA[Nov 14, 18:04 CET
Resolved - This incident has been resolved.
Nov 14, 13:53 CET
Update - The current situation is stable, and after performing a series of checks, no issues were observed. We will maintain ongoing monitoring for the next 4 hours, and if everything remains stable during this period, we will proceed to resolve the incident.
Nov 13, 21:16 CET
Monitoring - The issue is currently stable. We will continue careful monitoring of the situation and provide updates regularly.
Nov 13, 16:58 CET
Update - Our team is continuing to investigate the technical difficulties affecting webhooks and mailhooks on eu2.make.com. At this time, we have not yet identified the root cause, and users may still experience sporadic delays in the processing of these services.
Nov 13, 14:54 CET
Investigating - We are currently experiencing technical difficulties with webhooks and mailhooks on eu2.make.com. Users may encounter delays in the processing of webhooks and mailhooks.
We will provide another update on this Statuspage within the next 2 hours or as soon as more information becomes available.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/knl5pxnx0byt</id>
        <link href="https://www.githubstatus.com/incidents/knl5pxnx0byt"/>
        <updated>2023-11-13T21:38:40.000Z</updated>
        <summary type="html"><![CDATA[Nov 13, 21:38 UTC
Resolved - Between 20:35 and 21:38 we experienced up to a 20 minute delay delivering around 30,000 notifications due to side effects of some planned maintenance on supporting systems. We have noted the unexpected user impact of this type of maintenance and will address it in future maintenance planning.
Nov 13, 21:38 UTC
Update - An issue related to notifications has been resolved. Users should again be seeing their notifications.
Nov 13, 21:15 UTC
Update - We're seeing issues related to notifications.
Nov 13, 21:13 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: User presence is unexpectedly changing]]></title>
        <id>https://status.slack.com//2023-11/16fed44d7948cf49</id>
        <link href="https://status.slack.com//2023-11/16fed44d7948cf49"/>
        <updated>2023-11-13T15:57:36.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 4:00 PM PDT on October 31, 2023 to 2:00 PM PDT on November 9, 2023, user presence would unexpectedly change to away or inactive.


We determined that an error during a routine system optimization on connectivity state caused this issue. 


We reverted the change which fixed the issue for all affected customers.


Thank you for your patience while we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pages, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/wyk0ns67krlz</id>
        <link href="https://www.githubstatus.com/incidents/wyk0ns67krlz"/>
        <updated>2023-11-11T02:14:08.000Z</updated>
        <summary type="html"><![CDATA[Nov 11, 02:14 UTC
Resolved - On November 11, 2023, at 1:00 UTC, GitHub background jobs encountered delays lasting up to 50 minutes. This delay affected various services utilizing background jobs, including Actions, Webhooks, Pull Requests, and Pages. The impact persisted for approximately one hour until 2:10 UTC.
During the incident, some customers experienced delays in starting Github Actions workflow runs and Pages builds. We estimate that about 10% of Actions workflow runs were delayed during the impact window and 99% of Pages builds failed from 1:00 UTC to 1:20 UTC. Users may have experienced a delay in seeing recent pushes reflected in pull request views. This delay averaged between 5 and 10 minutes and affected up to 30% of pull request page views during the incident. 1% of pull requ…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/54s1f3rs3n56</id>
        <link href="https://status.rippling.com/incidents/54s1f3rs3n56"/>
        <updated>2023-11-10T23:59:07.000Z</updated>
        <summary type="html"><![CDATA[Nov 10, 23:59 UTC
Resolved - This incident has been resolved.
Nov  9, 19:56 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov  9, 18:41 UTC
Update - We are continuing to investigate this issue.
Nov  9, 18:39 UTC
Update - We are continuing to investigate this issue.
Nov  9, 18:34 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent 500 errors on the GET Resolutions API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt</id>
        <link href="https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt"/>
        <updated>2023-11-09T22:49:38.000Z</updated>
        <summary type="html"><![CDATA[Nov  9, 14:49 PST
Resolved - This incident has been resolved.
Nov  8, 10:38 PST
Identified - We have observed a higher number of intermittent 500 errors on the GET Resolutions API. We are actively working on resolving this issue and plan to deploy a fix tomorrow (Nov 9, 2023). Thank you for your understanding and patience as we work to rectify this situation.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with user status, read state, and file previews]]></title>
        <id>https://status.slack.com//2023-11/ea3a2a3e32e79902</id>
        <link href="https://status.slack.com//2023-11/ea3a2a3e32e79902"/>
        <updated>2023-11-09T04:29:31.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 10:00 PM PST on November 7, 2023 to 1:15 PM PST on November 8, 2023, some users experienced issues with their user status not updating, removing previews, and being unable to mark channels as read. Some keyboard shortcuts were also affected and were unable to be used.


We determined that a recent code change was the root cause for the unexpected behaviour with these features.


To restore functionality, we reverted the related code. We then did some additional testing and monitoring to confirm all issues were fully resolved and Slack was operating as expected.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking Connectivity Between NYC and SFO]]></title>
        <id>https://status.digitalocean.com/incidents/hxcw6mrw1ywv</id>
        <link href="https://status.digitalocean.com/incidents/hxcw6mrw1ywv"/>
        <updated>2023-11-08T18:07:20.000Z</updated>
        <summary type="html"><![CDATA[Nov  8, 18:07 UTC
Resolved - Our Engineering team has seen no recurrences and performance has remained stable since 16:40 UTC. This incident is fully resolved.
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience!
Nov  8, 17:23 UTC
Update - Our Engineering team identified a configuration that was responsible for the recurrence we saw. From 16:20 - 16:40 UTC, connectivity between SFO2 and the rest of DigitalOcean's network was impacted.
As of 16:40 UTC, all impact has subsided and users should no longer face any issues.
We are monitoring the situation closely and will share an update once the issue is completely resolved.
Nov  8, 16:32 UTC
Update - Our Engineering team is seeing a recurrence of network alerts that indicate we're exp…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/n1sp5244k09v</id>
        <link href="https://status.notion.so/incidents/n1sp5244k09v"/>
        <updated>2023-11-08T15:28:28.000Z</updated>
        <summary type="html"><![CDATA[Nov  8, 07:28 PST
Resolved - The incident has been resolved. Time of resolution Nov 8 2023 7:28AM PST
Nov  8, 07:28 PST
Monitoring - Our AI provider has implemented a fix and we are seeing Notion AI recover gradually since 7:28AM PST. We are currently monitoring the situation.
Nov  8, 06:57 PST
Update - The issue has been identified now and a fix is being worked on for this issue.
Nov  8, 06:29 PST
Identified - One of our AI providers is down. We are working with them on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Users unable to connect to Slack or send messages]]></title>
        <id>https://status.slack.com//2023-11/ef3e4b0ebcf16d8d</id>
        <link href="https://status.slack.com//2023-11/ef3e4b0ebcf16d8d"/>
        <updated>2023-11-08T01:59:22.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On November 4, 2023, between 5:09 PM PDT and 5:20 PM PDT, many customers were unable send messages or to connect to Slack.


A routine code change introduced a database error that prevented cached data from being cleared correctly, resulting in severe performance issues.


We rolled back the code change and refreshed all affected servers, resolving the issue for all users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests and Issues]]></title>
        <id>https://www.githubstatus.com/incidents/m61vxgn4kvh2</id>
        <link href="https://www.githubstatus.com/incidents/m61vxgn4kvh2"/>
        <updated>2023-11-07T14:25:40.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 14:25 UTC
Resolved - Our internal search infrastructure experienced increased latency and timeouts between 13:15 and 14:05 UTC leading to some end user timeouts and slow responses for any requests that made use of that subsystem. This included but was not limited to: user search, repository search, releases, and audit logs.
We mitigated the issue by migrating traffic to an older version of our search clusters and are investigating what caused the performance issues in our new clusters.
Nov  7, 14:25 UTC
Update - API Requests is operating normally.
Nov  7, 14:15 UTC
Update - Response times stabilized back to normal at 13:58 UTC.  We are continuing to monitor the slow dependency to ensure it's stable before resolving this incident.
Nov  7, 14:03 UTC
Update - We're seeing intermittent spikes in latency of API requests and page loads.  We are investigating but do not have an ETA at this time.
Nov  7, 13:50 UTC
Update - We are investigating reports of issues with service(s): Issues, API Requests. We will continue to keep users updated on progress towards mitigation.
Nov  7, 13:46 UTC
Update - Issues is experiencing degraded performance. We are continuing to investigate.
Nov  7, 13:44 UTC
Investigating - We are investigating reports of degraded performance for API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Failing to execute scenarios]]></title>
        <id>https://status.make.com/incidents/g6gklsv4shx6</id>
        <link href="https://status.make.com/incidents/g6gklsv4shx6"/>
        <updated>2023-11-07T11:43:11.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 12:43 CET
Resolved - This incident has been resolved.
Nov  6, 16:30 CET
Monitoring - We have noticed a degradation of performance on eu1.make.celonis.com. A fix has been applied and the system is fully operational again.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/w8r2ldkc0rcj</id>
        <link href="https://status.rippling.com/incidents/w8r2ldkc0rcj"/>
        <updated>2023-11-07T07:51:01.000Z</updated>
        <summary type="html"><![CDATA[Nov  7, 07:51 UTC
Resolved - This incident has been fully resolved.
Nov  6, 19:08 UTC
Monitoring - The Rippling app has mostly recovered but there are still a few lags in performance that we're further monitoring and investigating.
Nov  6, 18:33 UTC
Investigating - We are continuing to see degraded performance in the Rippling app, so we are continuing to investigate.
Nov  6, 17:28 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov  6, 17:25 UTC
Investigating - There are issues loading Rippling. We are working on a fix.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/bw3r3j9b5ph5</id>
        <link href="https://status.digitalocean.com/incidents/bw3r3j9b5ph5"/>
        <updated>2023-11-04T08:18:05.000Z</updated>
        <summary type="html"><![CDATA[Nov  4, 08:18 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting App Platform Deployments. 
From 11:54 on Nov 2nd to 06:42 on Nov 4th UTC, App Platform users may have experienced delays when deploying new apps or when deploying updates to existing Apps. Our Upstream provider and the Engineering team closely worked together to resolve the issue. 
The impact has been completely subsided and users should no longer see any issues with the impacted services.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel. Thank you for your patience and we apologize for any inconvenience.
Nov  4, 06:53 UTC
Update - As per the recent update from our Upstream provider, they fully recovered the services used…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in SFO Regions]]></title>
        <id>https://status.digitalocean.com/incidents/nz33vs0sjhqy</id>
        <link href="https://status.digitalocean.com/incidents/nz33vs0sjhqy"/>
        <updated>2023-11-03T23:14:12.000Z</updated>
        <summary type="html"><![CDATA[Nov  3, 23:14 UTC
Resolved - Our Engineering team has confirmed the full resolution of issue impacting network connectivity in our SFO regions. 
Users should no longer experience any latency or timeout issue with any of the Droplet based services. 
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Nov  3, 20:58 UTC
Monitoring - As of 19:55 UTC, our Engineering team has confirmed that a fix has been implemented by our upstream carrier to mitigate the cause of the issue impacting network connectivity in our SFO region. 
We are closely monitoring the situation and will update as soon as we have more information from the provider.
Nov  3, 19:43 UTC
Identified - Our Engineering team has identified the cause of the issue impacting network connectivity in our SFO region. Upstream congestion with a network provider between Los Angeles and Dallas is impacting traffic traversing out of our SFO datacentres.
A case has been opened by our team with the provider. Our team has attempted to shift traffic to improve the situation, but unfortunately, we continue to see approximately 10% of customer traffic impacted by this issue.
Our team is working on an option to shift to an alternate provider if this issue is not able to be resolved by the provider in a timely manner. We will share another update once we have further information from the provider or we have an update from our Engineering team.
Nov  3, 19:27 UTC
Investigating - As of 17:40 UTC, our Engineering team is investigating an issue impacting networking in the SFO regions. During this time, a subset of users may experience packet loss/latency and timeouts with Droplet based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations, Issues, Pull Requests, Actions, API Requests, Codespaces, Packages, Pages and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/xb30mby9fs5x</id>
        <link href="https://www.githubstatus.com/incidents/xb30mby9fs5x"/>
        <updated>2023-11-03T19:21:48.000Z</updated>
        <summary type="html"><![CDATA[Nov  3, 19:21 UTC
Resolved - A performance and resilience optimization to the authorization microservice contained a memory leak that was exposed under high traffic. This resulted in a number of pages returning 404’s that should not have. Testing the build in our canary ring did not expose the service to sufficient traffic to discover the leak, allowing it to graduate to production at 6:37 PM UTC.  The memory leak under high load caused pods to crash repeatedly starting at 6:42 PM UTC, failing authorization checks. These failures triggered alerts at 6:44 PM UTC. Rolling back the authorization service change was delayed as parts of the deployment infrastructure relied on the authorization service and required manual intervention to complete. Rollback completed at 7:08 PM UTC and all impacte…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple services down and API availability]]></title>
        <id>https://status.digitalocean.com/incidents/y3q3gh220jyj</id>
        <link href="https://status.digitalocean.com/incidents/y3q3gh220jyj"/>
        <updated>2023-11-02T11:50:30.000Z</updated>
        <summary type="html"><![CDATA[Nov  2, 11:50 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting the Cloud Control Panel, API, and multiple services.
From 05:05 - 08:40 UTC, users may have encountered errors with the Cloud Control Panel and public API while attempting to create new user registrations, or while making payments. Users also may have experienced issues with processing Droplet and Managed Kubernetes cluster creations along with Droplet-based events and experienced latencies while accessing our Cloud Control Panel along with the DigitalOcean Container Registry. Our Upstream provider and the Engineering team closely worked together to resolve the issues. 
The impact has been completely subsided and users should no longer see any issues with the impacted services.
If you…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Issues for customers enrolled in the French data region]]></title>
        <id>https://status.slack.com//2023-10/8c886b57284762b6</id>
        <link href="https://status.slack.com//2023-10/8c886b57284762b6"/>
        <updated>2023-11-02T03:45:50.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:


From 5:01 PM PDT on October 31, 2023 to around 5:54 PM PDT, customers with international data residency in the Paris, France region, experienced issues connecting to Slack and sending messages.


A routine credential rotation caused database sync issues for the Paris, France data residency region. We reverted this code change and restarted the affected databases, resolving the issue for all impacted customers. 


Once we had mitigated the immediate impact and restored connectivity for customers in the Paris, France data residency region, we reviewed the credential rotation for all other data residency regions to ensure the same issue would not occur anywhere else.


Please note that the start and end time of the incident have been edited for accuracy.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-008]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-008/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-008/"/>
        <updated>2023-11-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: Privilege escalation bugs in the Tailscale
Kubernetes operator’s API proxy allowed authenticated tailnet clients
to send Kubernetes API requests as the operator’s service account.
Tailscale Kubernetes operator version v1.53.37 fixes the issue and
users of the operator who enable the API proxy functionality should
update as described below.
What happened?
The Tailscale Kubernetes operator can optionally act as an API server
proxy
for the cluster’s Kubernetes API. This proxy allows authenticated
tailnet users to use their tailnet identity in Kubernetes
authentication and RBAC rules. The API server proxy uses
impersonation
headers
to translate tailnet identities to Kubernetes identities.
The operator prior to v1.53.37 has two bugs in the forwarding logic,
which affects different …]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue Affecting Webhooks]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/875yz70rnb1p</id>
        <link href="https://airbnbapi.statuspage.io/incidents/875yz70rnb1p"/>
        <updated>2023-10-31T17:00:19.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 10:00 PDT
Resolved - This incident has been resolved.
Oct 30, 18:14 PDT
Monitoring - Today (Oct 30, 2023) between 1:00 PM and 3:00 PM PDT we encountered an incident that impacted our webhooks functionality. As a result, some reservations may have been missing webhooks during this time period. The issue has been resolved, and our team is actively monitoring the results to ensure the stability of our systems. If you have noticed any reservations missing webhooks, we recommend using the GET Reservations API to retrieve the details of those reservations. Alternatively, you can contact us with a list of affected reservations, and we will assist you in backfilling the missing information.
We apologize for any inconvenience this may have caused and appreciate your understanding.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Admin account not visible in the account dropdown for Super Admins]]></title>
        <id>https://status.rippling.com/incidents/fjw9lbztvc7f</id>
        <link href="https://status.rippling.com/incidents/fjw9lbztvc7f"/>
        <updated>2023-10-27T15:51:44.000Z</updated>
        <summary type="html"><![CDATA[Oct 27, 15:51 UTC
Resolved - The issue has now been resolved. All admins are able to access their Admin Account view as expected.
Oct 27, 14:50 UTC
Identified - We are aware of an issue where 'Admin Account' is not visible in the account dropdown for Super Admins. After they navigate to their 'Employee Account' view they can't navigate back to their admin view. 
The root cause has been identified and the issue should be resolved in the next hour.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Storage - SGP1 and SFO3]]></title>
        <id>https://status.digitalocean.com/incidents/nwm1sn0gjfc3</id>
        <link href="https://status.digitalocean.com/incidents/nwm1sn0gjfc3"/>
        <updated>2023-10-27T14:47:58.000Z</updated>
        <summary type="html"><![CDATA[Oct 27, 14:47 UTC
Resolved - As of 14:30 UTC, our Engineering team has resolved the issue impacting Spaces in our SGP1 and SFO3 regions. Users should no longer experience slowness or timeouts when trying to create or access their Spaces resources in the SGP1 and SFO3 regions. 
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Oct 27, 14:03 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting Spaces in our SGP1 and SFO3 regions and is monitoring the situation closely. We will post an update as soon as the issue is fully resolved.
Oct 27, 12:04 UTC
Update - Our Engineering team is continuing to investigate an issue impacting Object Storage in our SGP1 region. Additionally, we have become aware that this issue has also impacted Object Storage in the SFO3 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 27, 11:25 UTC
Investigating - As of 10:22 UTC, our Engineering team is investigating an issue with Object Storage in our SGP1 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in LON1]]></title>
        <id>https://status.digitalocean.com/incidents/1zckjwhpq1xy</id>
        <link href="https://status.digitalocean.com/incidents/1zckjwhpq1xy"/>
        <updated>2023-10-26T15:52:41.000Z</updated>
        <summary type="html"><![CDATA[Oct 26, 15:52 UTC
Resolved - As of 15:45 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. 
Thank you for your patience and we apologize for any inconvenience.
Oct 26, 15:18 UTC
Monitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. 
We will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.
Oct 26, 14:25 UTC
Identified - Our Engineering team has identified the cause of the issue impacting the networking in the LON1 region and is actively working on a fix. During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. 
We will post an update as soon as additional information is available.
Oct 26, 12:50 UTC
Investigating - As of 11:40 UTC, our Engineering team is investigating an issue impacting the networking in the LON1 region. During this time, a subset of users may experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. 
We will share an update once we have further information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-007]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-007/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-007/"/>
        <updated>2023-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: Microsoft Defender is flagging Tailscale 1.46.1 as malware.
These classifications are false positives, and we are working with Microsoft to
resolve the situation.
As of 2023-10-27 1:05 AM UTC, we have confirmed that Microsoft have addressed
the false positive, meaning Defender no longer flags Tailscale 1.46.1 as
malware. A rescan of tailscaled.exe 1.46.1 on VirusTotal confirms this.
What happened?
Microsoft Defender was flagging Tailscale 1.46.1 as malware. This caused
Defender to quarantine the binaries, meaning they could not run.
We submitted Tailscale 1.46.1 to Microsoft to investigate the false positive,
who then updated Defender to avoid flagging this release as malware at
2023-10-27 1:05 AM UTC.
Who is affected?
People using Defender and Tailscale 1.46.1.
What is the impact?
Tailscale will not run on affected machines.
What do I need to do?
To resolve this issue on your own tailnet, you can take either or both of 2
approaches:
Update to a newer version of Tailscale. Newer versions are not affected by this problem.
Create an exception in Microsoft Defender. Microsoft has published instructions explaining how to do this.
Update Microsoft Defender.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/sbjdwn6mvht7</id>
        <link href="https://www.githubstatus.com/incidents/sbjdwn6mvht7"/>
        <updated>2023-10-25T22:15:54.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 22:15 UTC
Resolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.
Beginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.
In order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.
Oct 25, 21:37 UTC
Update - The observed Copilot API error rate is around 5% of the requests. As a result, some of the Copilot code suggestions are skipped or not delivered on time.
Oct 25, 21:19 UTC
Update - We are seeing an impact in the US region as well. We continue the investigation.
Oct 25, 20:53 UTC
Update - Copilot is experiencing intermittent issues in our Japan region. Engineers are currently investigating.
Oct 25, 20:50 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/vnm5lc6f0b91</id>
        <link href="https://status.rippling.com/incidents/vnm5lc6f0b91"/>
        <updated>2023-10-25T21:10:01.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 21:10 UTC
Resolved - This incident has been resolved.
Oct 25, 21:02 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 25, 20:52 UTC
Identified - An issue has been identified and we are working on a fix.
Oct 25, 20:46 UTC
Update - We are continuing to monitor for any further issues.
Oct 25, 20:22 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 25, 20:17 UTC
Identified - The issue has been identified and a fix is being implemented.
Oct 25, 20:14 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/p351mbywbp0t</id>
        <link href="https://www.githubstatus.com/incidents/p351mbywbp0t"/>
        <updated>2023-10-25T13:02:51.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 13:02 UTC
Resolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.
Beginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.
In order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.
Oct 25, 12:56 UTC
Update - We have applied a fix to help with Copilot performance. Initial signals show good recovery. We will continue to monitor for the time being and resolve when confident the issue has been resolved.
Oct 25, 12:29 UTC
Update - We are investigating degraded performance in Europe for Copilot. We will continue to keep users updated on progress towards mitigation.
Oct 25, 12:10 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/y5rwfhsl0gqw</id>
        <link href="https://status.digitalocean.com/incidents/y5rwfhsl0gqw"/>
        <updated>2023-10-24T23:00:07.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 23:00 UTC
Completed - The scheduled maintenance has been completed.
Oct 24, 21:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 20, 19:40 UTC
Update - After a thorough review by the team performing this maintenance, we have determined that our initial messaging does not convey the complete scope and potential impact of this event. Existing infrastructure will continue running without issue during this maintenance window. However, users may experience increased latency with some platform operations, including: 
Cloud Control Panel and API operations
Event processing
Droplet creates, resizes, rebuilds, and power events
Managed Kubernetes reconciliation and scaling
Load Balancer operations
Container Registry operations
App …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with single sign-on from Rippling to third-party applications]]></title>
        <id>https://status.rippling.com/incidents/vlnmlxcw85r8</id>
        <link href="https://status.rippling.com/incidents/vlnmlxcw85r8"/>
        <updated>2023-10-24T22:51:30.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 22:51 UTC
Resolved - This incident has been resolved.
Oct 24, 22:08 UTC
Update - We are continuing to monitor for any further issues.
Oct 24, 22:07 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 24, 21:58 UTC
Identified - The issue has been identified and a fix is being implemented.
Oct 24, 21:57 UTC
Investigating - Customers are experiencing intermittent issues using single sign-on from Rippling (IdP-initiated SAML) to third-party applications. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: A small number of users are having problems loading Slack.]]></title>
        <id>https://status.slack.com//2023-10/2ef86432e31615ea</id>
        <link href="https://status.slack.com//2023-10/2ef86432e31615ea"/>
        <updated>2023-10-24T21:08:02.000Z</updated>
        <summary type="html"><![CDATA[Customers should no longer be experiencing any connection issues with Slack. Apologies for the trouble today and thank you for your patience.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in multiple regions.]]></title>
        <id>https://status.digitalocean.com/incidents/gp9bzm1hnlk4</id>
        <link href="https://status.digitalocean.com/incidents/gp9bzm1hnlk4"/>
        <updated>2023-10-24T09:15:10.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 09:15 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Oct 24, 09:03 UTC
Monitoring - Our Engineering team has received communication from the upstream provider that a fix to resolve the networking issue has been implemented. We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.
Oct 24, 07:41 UTC
Identified - Our Engineering team has identified the cause of issues impacting networking in multiple regions. The issues are a direct result of traffic congestion from our upstream providers, which is in the process of being repaired.
At this time, a subset of users will continue to experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.
We apologize for the inconvenience and will share an update once we have more information.
Oct 24, 06:11 UTC
Investigating - As of 05:30 UTC, our Engineering team is investigating an issue impacting the networking in multiple regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.
At the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/lw4dvwltm025</id>
        <link href="https://www.githubstatus.com/incidents/lw4dvwltm025"/>
        <updated>2023-10-22T16:07:58.000Z</updated>
        <summary type="html"><![CDATA[Oct 22, 16:07 UTC
Resolved - This incident has been resolved.
From 11:21 to 16:07 UTC some GitHub customers experienced errors cloning via workflows or via the command line.
A third-party configuration change resulted in an unexpected behavior to our systems that resulted in Git clone failures. Once we detected the change we were able to disable it, and our systems started operating normally.
With the incident mitigated, we are working with our third-party provider to improve subsequent configuration change rollouts.
Oct 22, 15:58 UTC
Update - We have mitigated the cause of the issue and are awaiting positive confirmation from impacted customers that the issue is fully resolved.
Oct 22, 15:34 UTC
Update - We are currently investigating reports from some customers encountering errors when cloning repositories via workflows or via the command line. We do not currently have an ETA for resolution. Next update in 30 minutes.
Oct 22, 15:16 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>