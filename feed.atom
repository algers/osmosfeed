<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-08-30T00:19:52.474Z</id>
    <title>osmos::feed</title>
    <updated>2023-08-30T00:19:52.474Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/5g389cjvwfbp</id>
        <link href="https://status.twilio.com/incidents/5g389cjvwfbp"/>
        <updated>2023-08-30T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Aug 29, 21:00 PDT  -  Aug 30, 01:00 PDT
Aug 28, 16:39 PDT
Update - We will be undergoing scheduled maintenance during this time.
Aug 28, 15:06 PDT
Scheduled - A subset of small US networks in the US are conducting a planned maintenance from 29 August 2023 at 21:00 PDT until 30 August 2023 at 01:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from small US Carriers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canada SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/v2fbzb3bb46y</id>
        <link href="https://status.twilio.com/incidents/v2fbzb3bb46y"/>
        <updated>2023-08-30T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Aug 29, 21:00 PDT  -  Aug 30, 01:00 PDT
Aug 25, 08:16 PDT
Scheduled - Our SMS carrier partner in Canada is conducting a planned maintenance from 29 August 2023 at 21:00 PDT until 30 August 2023 at 01:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from Canada handsets via subset of short codes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Failures to T-Mobile Network in the US for a Subset of Short Codes]]></title>
        <id>https://status.twilio.com/incidents/c7dlyr9frw4f</id>
        <link href="https://status.twilio.com/incidents/c7dlyr9frw4f"/>
        <updated>2023-08-29T23:58:34.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 16:58 PDT
Resolved - We are no longer experiencing SMS delivery failures when sending messages T-Mobile Network in the US for a Subset of Short Codes. This incident has been resolved.
Aug 29, 16:54 PDT
Update - We are observing successful SMS delivery when sending messages T-Mobile Network in the US for a Subset of Short Codes.  We will continue to monitor to ensure full service recovery. We expect to provide another update in 2 hours or as soon as more information becomes available
Aug 29, 14:54 PDT
Monitoring - We are observing successful SMS delivery when sending messages T-Mobile Network in the US for a Subset of Short Codes.  We will continue to monitor to ensure full service recovery. We expect to provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 13:18 PDT
Update - We continue to experience SMS delivery failures when sending messages  T-Mobile Network in the US for a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Aug 29, 12:18 PDT
Update - We continue to experience SMS delivery failures when sending messages  T-Mobile Network in the US for a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 11:18 PDT
Update - We are experiencing SMS delivery failures when sending messages  T-Mobile Network in the US for a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Aug 29, 11:11 PDT
Investigating - Our monitoring systems have detected a potential issue with delivery delays to T-Mobile Network in the US for a Subset of Short Codes. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays To US Cellular Corp Network In US Over A Subset Of Shortcodes]]></title>
        <id>https://status.twilio.com/incidents/wjqpqr7553vb</id>
        <link href="https://status.twilio.com/incidents/wjqpqr7553vb"/>
        <updated>2023-08-29T23:50:23.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 16:50 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to US Cellular Corp network in the US over a subset of shortcodes. This incident has been resolved.
Aug 29, 14:50 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to US Cellular Corp network in the US over a subset of shortcodes. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 13:13 PDT
Update - We continue to experience SMS delivery delays when sending messages to US Cellular Corp network in the US over a subset of shortcodes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 12:13 PDT
Investigating - We are experiencing MMS/SMS delivery delays when sending messages to US Cellular Corp network in the US over a subset of shortcodes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays To Orange Network In Cameroon]]></title>
        <id>https://status.twilio.com/incidents/yxkj9yrpn4vw</id>
        <link href="https://status.twilio.com/incidents/yxkj9yrpn4vw"/>
        <updated>2023-08-29T23:36:37.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 16:36 PDT
Update - We are still experiencing SMS delivery delays when sending messages to Orange Network In Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 16 hours or as soon as more information becomes available.
Aug 29, 08:36 PDT
Update - We are still experiencing SMS delivery delays when sending messages to Orange Network In Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 8 hours or as soon as more information becomes available.
Aug 29, 04:57 PDT
Update - We are experiencing SMS delivery delays when sending messages to Orange Network In Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Aug 29, 03:02 PDT
Update - We are experiencing SMS delivery delays when sending messages to Orange Network In Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 02:08 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to Orange Network In Cameroon. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/wqv4yvlgtq62</id>
        <link href="https://www.githubstatus.com/incidents/wqv4yvlgtq62"/>
        <updated>2023-08-29T21:58:10.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 21:58 UTC
Resolved - This incident has been resolved.
Aug 29, 21:52 UTC
Update - GitHub webhooks are fully functional but can take up to 20 minutes to deliver. We are working on identifying the root cause of the increased latency.
Aug 29, 20:55 UTC
Update - The Webhooks backlog continues to process, and we anticipate continued latency while it clears.
Aug 29, 19:38 UTC
Update - The Webhooks backlog continues to process, and we anticipate continued latency while it clears.
Aug 29, 18:24 UTC
Update - We’ve mitigated the issue causing Webhooks latency and are monitoring processing of the backlog of queued requests.
Aug 29, 17:31 UTC
Update - We are investigating increased delivery time issues with Webhooks. We will continue to keep users updated on progress towards mitigation.
Aug 29, 17:29 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Voice Recordings Throwing 500 Errors]]></title>
        <id>https://status.twilio.com/incidents/3674mqxc6cj5</id>
        <link href="https://status.twilio.com/incidents/3674mqxc6cj5"/>
        <updated>2023-08-29T21:00:00.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 14:00 PDT
Resolved - From 2:22 PM PST to 2:32 PM PST a subset of Twilio customers would have received 500s for starting, stopping, pausing, or resuming voice recordings. This incident has been resolved.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[France Voice and SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/ytwtgvxchz6x</id>
        <link href="https://status.twilio.com/incidents/ytwtgvxchz6x"/>
        <updated>2023-08-29T20:00:01.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 13:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 17, 16:31 PDT
Scheduled - Our Voice and SMS carrier partner in France is conducting a planned maintenance from 29 August 2023 at 13:00 PDT until 29 August 2023 at 18:00 PDT. During the maintenance window, there could be intermittent call failures to and from a subset of Twilio's France phone numbers and intermittent SMS delays delivering to and from France handsets via a subset of France longcodes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WhatsApp Messages Delays]]></title>
        <id>https://status.twilio.com/incidents/s525g6zvslt0</id>
        <link href="https://status.twilio.com/incidents/s525g6zvslt0"/>
        <updated>2023-08-29T18:33:00.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 11:33 PDT
Resolved - CloudAPI was degraded for 23 minutes between 08:15 and 08:38 Pacific Time on 08/29/2023. During this period of time, customers may have experienced delays when sending WhatsApp messages. The issue has now been resolved.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hosted Numbers Moving to Action Required]]></title>
        <id>https://status.twilio.com/incidents/rd42kfkms09m</id>
        <link href="https://status.twilio.com/incidents/rd42kfkms09m"/>
        <updated>2023-08-29T18:02:02.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 11:02 PDT
Resolved - The issue affecting intermittent the hosted numbers onboarding process where the numbers are moving to Action Required has been resolved, and the service is functioning normally at this time.
Aug 29, 09:21 PDT
Update - We still are experiencing an issue with the hosted numbers onboarding process where the numbers are moving to Action Required. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available
Aug 29, 08:21 PDT
Update - We are experiencing an issue with the hosted numbers onboarding process where the numbers are moving to Action Required. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.
Aug 29, 07:51 PDT
Update - We are experiencing an issue with the hosted numbers onboarding process where the numbers are moving to Action Required. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 30 minutes or as soon as more information becomes available.
Aug 29, 07:39 PDT
Investigating - We've become aware of a potential issue with numbers moving to action required errors. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Less than 1% of users may be experiencing slowness when loading Slack]]></title>
        <id>https://status.slack.com//2023-08/094f489e39b57d9f</id>
        <link href="https://status.slack.com//2023-08/094f489e39b57d9f"/>
        <updated>2023-08-29T16:54:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 28, 2023 from 4:30 AM PDT to 10:30 AM PDT, less than 1% of Slack users encountered slowness when loading channels, DMs, and threads. Some impacted users were also unable to load the sidebar.


We found that a host that caches workspace member, channel, and integration data was impacted by timeout errors. This prevented users on some workspaces from routing to the host and, in turn, reaching their cached data. 


We restarted the affected host, cleared the associated cache for impacted workspaces, and asked affected users to reload their Slack apps. This combination of steps resolved the issue for all users. 


We're continuing to investigate what caused the timeout errors to prevent future reoccurrence.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/99rhjnxq99tp</id>
        <link href="https://www.githubstatus.com/incidents/99rhjnxq99tp"/>
        <updated>2023-08-29T16:16:46.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 16:16 UTC
Resolved - This incident has been resolved.
Aug 29, 16:05 UTC
Update - Intermittent failures with our resource provider prevented around 200 Codespaces from launching or resuming in Europe West and Southeast Asia. We have mitigated impact and continue to monitor.
Aug 29, 16:05 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Failures to MTN Network in Nigeria]]></title>
        <id>https://status.twilio.com/incidents/dk18cd2yx5s7</id>
        <link href="https://status.twilio.com/incidents/dk18cd2yx5s7"/>
        <updated>2023-08-29T16:03:01.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 09:03 PDT
Resolved - We are no longer experiencing SMS delivery failures to MTN Network in Nigeria. This incident has been resolved.
Aug 29, 07:24 PDT
Update - We keep observing successful SMS delivery failures to MTN Network in Nigeria.  We will continue to monitor to ensure full service recovery. We expect to provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 07:01 PDT
Monitoring - We are observing successful SMS delivery failures to MTN Network in Nigeria.  We will continue to monitor to ensure full service recovery. We expect to provide another update in 30 minutes or as soon as more information becomes available.
Aug 29, 05:47 PDT
Update - We are experiencing SMS delivery failures to MTN Network in Nigeria. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 05:07 PDT
Investigating - We are experiencing SMS delivery failures to MTN Network in Nigeria. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/yyr2ghh113dj</id>
        <link href="https://www.githubstatus.com/incidents/yyr2ghh113dj"/>
        <updated>2023-08-29T15:35:49.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 15:35 UTC
Resolved - This incident has been resolved.
Aug 29, 15:25 UTC
Update - We are investigating issues with Codespaces in the Europe West and Southeast Asia geographic areas. Some users may not be able to connect to their Codespaces at this time. We will update on progress.
Aug 29, 15:13 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Australia Voice Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/xlv6j0ym1w8b</id>
        <link href="https://status.twilio.com/incidents/xlv6j0ym1w8b"/>
        <updated>2023-08-29T13:31:15.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 06:31 PDT
Completed - The scheduled maintenance has been completed.
Aug 29, 06:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 28, 23:42 PDT
Scheduled - Our Voice carrier partner in Australia is conducting an emergency maintenance from 29 August at 06:00 PDT until 29 August at 06:30 PDT. During the maintenance window, there could be intermittent call disconnects and call failures for calls from and to Twilio Australia phone numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Multiple Networks in Mexico via Registered Sender Ids and Long codes]]></title>
        <id>https://status.twilio.com/incidents/lxjs48mgxg53</id>
        <link href="https://status.twilio.com/incidents/lxjs48mgxg53"/>
        <updated>2023-08-29T11:57:23.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 04:57 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Multiple Networks in Mexico via Registered Sender IDs and Long codes. This incident has been resolved.
Aug 29, 03:12 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Multiple Networks in Mexico via Registered Sender IDs and Long codes. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 01:52 PDT
Update - We are experiencing SMS delivery delays when sending messages to Multiple Networks in Mexico via Registered Sender IDs and Long codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Aug 29, 00:58 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to Multiple Networks in Mexico via Registered Sender IDs and Long codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Maintenance - U.S. Cellular]]></title>
        <id>https://status.twilio.com/incidents/1j0f2w34j269</id>
        <link href="https://status.twilio.com/incidents/1j0f2w34j269"/>
        <updated>2023-08-29T10:01:39.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 03:01 PDT
Completed - The scheduled maintenance has been completed.
Aug 28, 22:02 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 28, 14:29 PDT
Update - We will be undergoing scheduled maintenance during this time.
Aug 28, 14:29 PDT
Scheduled - The U.S. Cellular network in the US is conducting a planned maintenance from 28 August 2023 at 22:00 PDT until 29 August 2023 at 03:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from U.S. Cellular US handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United States Account Security Carrier Partner Maintenance - Verizon Wireless]]></title>
        <id>https://status.twilio.com/incidents/5wd983ymwpz2</id>
        <link href="https://status.twilio.com/incidents/5wd983ymwpz2"/>
        <updated>2023-08-29T07:00:24.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 00:00 PDT
Completed - The scheduled maintenance has been completed.
Aug 28, 20:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 28, 00:00 PDT
Scheduled - Our carrier partner Verizon Wireless US is conducting an emergency maintenance from 28 Aug 2023 at 20:00 PDT until 29 Aug 2023 at 00:00 PDT. During the maintenance window, there could be intermittent API request failures for Verizon Wireless US customers.

Impacted Products: 
Verify Silent Network Auth 
Lookup SIM Swap 
Legacy Identity MatchAndAttributes]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, Pull Requests and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/spz29wshw9mh</id>
        <link href="https://www.githubstatus.com/incidents/spz29wshw9mh"/>
        <updated>2023-08-29T02:36:47.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 02:36 UTC
Resolved - This incident has been resolved.
Aug 29, 02:34 UTC
Update - Webhooks is operating normally.
Aug 29, 02:34 UTC
Update - Pull Requests is operating normally.
Aug 29, 02:34 UTC
Update - Actions is operating normally.
Aug 29, 02:15 UTC
Update - GitHub webhooks are delayed, and required pull-request checks and reviews are not triggered.
Aug 29, 02:14 UTC
Update - Webhooks is experiencing degraded performance. We are continuing to investigate.
Aug 29, 02:07 UTC
Update - Newly triggered Actions workflows are stalled. We identified the problem, and we are working on a mitigation.
Aug 29, 02:02 UTC
Update - Pull Requests is experiencing degraded performance. We are continuing to investigate.
Aug 29, 01:59 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App platform deployment and rollbacks in NYC region]]></title>
        <id>https://status.digitalocean.com/incidents/5zn0vljq3hnd</id>
        <link href="https://status.digitalocean.com/incidents/5zn0vljq3hnd"/>
        <updated>2023-08-27T12:22:38.000Z</updated>
        <summary type="html"><![CDATA[Aug 27, 12:22 UTC
Resolved - As of 12:22 UTC, our Engineering team has resolved the issue impacting the builds and rollbacks in our NYC region. Everything should be functioning normally and users should no longer experience issues with the builds and rollbacks being stuck.
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Aug 27, 11:29 UTC
Monitoring - As of 11:28 UTC, our Engineering team has implemented a fix to resolve the issue with the App platform impacting the builds and rollbacks in our NYC region and monitoring the situation. At this time, users should no longer experience issues with active deployment and rollbacks being stuck. 
We will share another update once the matter is fully resolved.
Aug 27, 09:32 UTC
Update - As of 09:32 UTC, our Engineering team has identified the cause of the issue with the App platform impacting the builds and rollbacks in our NYC region and has implemented a fix that has partially mitigated the issue. Currently, our Engineering team is working on investigating the issue further and implementing a permanent fix.
We will post an update as soon as additional information is available.
Aug 27, 06:32 UTC
Identified - As of 06:21 UTC, our Engineering team has identified the cause of the issue with the App platform impacting the builds and rollbacks in our NYC region and is actively working on a fix. We will post an update as soon as additional information is available.
Aug 27, 05:35 UTC
Investigating - Our Engineering team is investigating an issue with App Platform builds and rollbacks in our NYC region. At this time, users may experience issues with active deployment and rollbacks being stuck.
We will post an update as soon as additional information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We're investigating reports of an issue with ChromeOS. We will provide more information shortly. The affected users are unable to access ChromeOS.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/96Dcae9UcSDexAmQHhwW</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/96Dcae9UcSDexAmQHhwW"/>
        <updated>2023-08-25T21:39:40.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-08-25 16:36</strong> and ended at <strong>2023-08-25 21:21</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><span>  The problem with ChromeOS has been resolved. We apologize for the inconvenience and thank you for your patience and continued support.  </span></div><hr><p>Affected products: ChromeOS</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be missing sidebar sections in Slack]]></title>
        <id>https://status.slack.com//2023-08/dd4ce480d5ff7e49</id>
        <link href="https://status.slack.com//2023-08/dd4ce480d5ff7e49"/>
        <updated>2023-08-25T21:31:10.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On August 24, 2023 between 10:41 AM PDT to 4:10 PM PDT, channel sections were missing from some users' sidebars.


We determined that a recent update in code failed to process an older format of data objects coming from our cache.


While we reverted this change and saw the majority of the issue resolved, this action caused an issue localized to the new data format and some users were still affected. 


Once we pushed a fix to normalize and handle all objects in the cache, channel sections were restored for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues uploading files.]]></title>
        <id>https://status.slack.com//2023-08/8fd09ebe40c64df0</id>
        <link href="https://status.slack.com//2023-08/8fd09ebe40c64df0"/>
        <updated>2023-08-25T20:32:40.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 4, 2023 from 8:04 AM PDT to 11:14 AM PDT, some users were unable to upload files in Slack.


We determined that a recent backend code change was erroneously detecting certain users as deactivated, which in turn was causing these users requests to upload files to be rejected. We reverted this change, which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users are unable to post to the #general channel]]></title>
        <id>https://status.slack.com//2023-08/414e590c93cb5636</id>
        <link href="https://status.slack.com//2023-08/414e590c93cb5636"/>
        <updated>2023-08-25T20:20:37.000Z</updated>
        <summary type="html"><![CDATA[We’re still actively investigating this issue, but we don’t have any new information to share at this time. Thanks for sticking with us as we continue to work towards a fix.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DigitalOcean Docs (https://docs.digitalocean.com) Unavailable]]></title>
        <id>https://status.digitalocean.com/incidents/2z768rr0lytf</id>
        <link href="https://status.digitalocean.com/incidents/2z768rr0lytf"/>
        <updated>2023-08-24T17:14:15.000Z</updated>
        <summary type="html"><![CDATA[Aug 24, 17:14 UTC
Resolved - Our Engineering team identified an issue with the DigitalOcean Docs site (https://docs.digitalocean.com), resulting in all users attempting to view the main site or any docs pages seeing 500 errors. 
Beginning around 16:55 UTC, the Docs site became unavailable following a complication with a deploy. At 17:16, our Engineering team triggered a new deploy and the site came back online. 
We apologize for the disruption. If you have any questions or concerns, please reach out through a Support ticket from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[delayed webhook response time on eu1.make.com]]></title>
        <id>https://status.make.com/incidents/l2625994sxpw</id>
        <link href="https://status.make.com/incidents/l2625994sxpw"/>
        <updated>2023-08-24T10:12:34.000Z</updated>
        <summary type="html"><![CDATA[Aug 24, 12:12 CEST
Resolved - The incident has been resolved.
Aug 24, 11:39 CEST
Monitoring - A fix was applied and webhook response time is back at expected level. We are closely monitoring the situation.
Aug 24, 11:16 CEST
Investigating - Webhook response time might be delayed on eu1.make.com. Our team is currently investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/w86r7g2x32j1</id>
        <link href="https://www.githubstatus.com/incidents/w86r7g2x32j1"/>
        <updated>2023-08-23T16:23:11.000Z</updated>
        <summary type="html"><![CDATA[Aug 23, 16:23 UTC
Resolved - This incident has been resolved.
Aug 23, 15:56 UTC
Update - We are investigating and working towards mitigating slight delays in Webhook deliveries.
Aug 23, 15:38 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HEIC Images Cannot be Rendered]]></title>
        <id>https://status.notion.so/incidents/bn3xy3vc4n90</id>
        <link href="https://status.notion.so/incidents/bn3xy3vc4n90"/>
        <updated>2023-08-23T15:30:00.000Z</updated>
        <summary type="html"><![CDATA[Aug 23, 08:30 PDT
Resolved - Between August 22nd at 11:25 AM PT and August 23rd at 8:39 AM PT, HEIC images were unable to render in Notion.
This incident has now been resolved. Users should now be able to render HEIC images.
Affected users will need to hard refresh their local cache to successfully load their images.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unveiling the newest upgrades to Smily's Automated Discount Codes 🚀]]></title>
        <id>272959</id>
        <link href="https://changelog.bookingsync.com/unveiling-the-newest-upgrades-to-smily's-automated-discount-codes-272959"/>
        <updated>2023-08-23T13:33:56.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
👉 After releasing the first version of the discount code feature, your invaluable feedback has spoken volumes, We want to express our gratitude for your involvement and your contribution 
Understanding the significance of the discount codes for you, we hosted dynamic workshops and engaged in insightful discussions with users like yourself. Your input has been important to add the following features designed to cater to your needs.
📈 Tracking discount codes:
Seamlessly track discount code usage and view the discounted booking list, using our discount code filtering feature!

📅 Eligible date:
Unlock the power of time-limited offers: Set eligible dates for booking discount codes to drive sales for specific seasons, create urgency, and maximize guest Engagement!

We invite you to discover all these new features, and we hope it will help you manage better your bookings, save time, and book more!
If you have any questions or need assistance, please don't hesitate to contact us through this form.
Wishing you all a pleasant day!]]></summary>
        <author>
            <name>Zahra, Product manager, PMS team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Login issues for Android customers using Intune and Edge]]></title>
        <id>https://status.slack.com//2023-08/cecb6f52bb0ace68</id>
        <link href="https://status.slack.com//2023-08/cecb6f52bb0ace68"/>
        <updated>2023-08-22T21:00:44.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Beginning August 16th 2023 at 11:00 AM PDT, some Android users signing into the Slack via Microsoft Intune and Microsoft Edge, saw the error "For reasons unknown, we were unable to sign you into your workspace" or "Drat, your login didn't work".


We found the error is related to a web redirect issue with Slack and Microsoft Edge. 


We'll work to fix this internally through a bug ticket that has been filed as a result of this incident. Though we haven't resolved this issue yet, we will provide further updates through customer support tickets with the impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/smms25d68d09</id>
        <link href="https://www.githubstatus.com/incidents/smms25d68d09"/>
        <updated>2023-08-22T17:01:49.000Z</updated>
        <summary type="html"><![CDATA[Aug 22, 17:01 UTC
Resolved - This incident has been resolved.
Aug 22, 16:14 UTC
Update - Actions Larger Runners are experiencing delayed queue times. A mitigation is being worked on and will be applied shortly.
Aug 22, 15:59 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Workspace name appearing next to channel names]]></title>
        <id>https://status.slack.com//2023-08/91459fdf86fd71f7</id>
        <link href="https://status.slack.com//2023-08/91459fdf86fd71f7"/>
        <updated>2023-08-22T11:57:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 21, 2023 from 11:30 AM PDT to August 22, 2023 around 2:30 AM PDT, customers may have noticed the workspace name appearing beside the channel names.


The issue was caused by a product change that impacted our channel name display logic, causing workspace names to appear next to the channel names in the sidebar. 


We identified and introduced a fix, resolving the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some iOS users are experiencing errors when sending messages]]></title>
        <id>https://status.slack.com//2023-08/0e77cc5cab99dd94</id>
        <link href="https://status.slack.com//2023-08/0e77cc5cab99dd94"/>
        <updated>2023-08-22T04:22:28.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On August 21, 2023 from 3:33 PM PDT to around 5:41 PM PDT, customers using the iOS app may have experienced issues sending messages. 


A code change inadvertently introduced an error in our message validation logic, preventing the iOS app from sending messages on a first attempt. Some customers may have been able to resend messages successfully.


We identified and reverted the culprit code change, resolving the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-006]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-006/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-006/"/>
        <updated>2023-08-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: An issue in the Tailscale client, combined with a behavior
of the UPnP implementations in some routers, could expose all UDP ports of a
node to external networks (usually the internet).
As of 2023-08-22 2:30 AM UTC, we have changed the Tailscale coordination server
to advise nodes to stop using UPnP for port mapping. In some cases this can
degrade NAT traversal and may cause some connections to route through DERP.
This may increase node-to-node latency and decrease throughput. Version 1.48.1
resolves the issue and re-enables port mapping via UPnP.
What happened?
Tailscale nodes use UPnP as one of the mechanisms to open UDP port forwarding
in routers to help with NAT traversal. Tailscale picks a node port and an
external router port and requests forwarding between them. On firs…]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be having trouble loading Slack.]]></title>
        <id>https://status.slack.com//2023-08/d67213498dd5cc16</id>
        <link href="https://status.slack.com//2023-08/d67213498dd5cc16"/>
        <updated>2023-08-21T23:07:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 21, 2023 from 8:00 AM PDT to 8:51 AM PDT, less than 1% of users experienced trouble loading Slack.


We determined that a recent code change on our back end was resulting in a higher than expected memory footprint, and that this combined with high traffic loads was causing these issues.


We immediately rolled back this change, which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/q8swpy90g6pp</id>
        <link href="https://www.githubstatus.com/incidents/q8swpy90g6pp"/>
        <updated>2023-08-21T18:41:06.000Z</updated>
        <summary type="html"><![CDATA[Aug 21, 18:41 UTC
Resolved - This incident has been resolved.
Aug 21, 18:23 UTC
Update - Mitigation is in place for delays in Actions workflows and should be fully recovered in the next 30 minutes.
Aug 21, 17:41 UTC
Update - We’re currently investigating reports of delays in Actions workflows.
Aug 21, 17:27 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Trouble loading Slack]]></title>
        <id>https://status.slack.com//2023-08/56ebd3ce13963c6a</id>
        <link href="https://status.slack.com//2023-08/56ebd3ce13963c6a"/>
        <updated>2023-08-18T01:00:12.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 17, 2023 from 9:20 AM PDT to 9:37 AM PDT, some users encountered issues with messages not sending and channels and threads failing to load.


We determined that a recent indexing change caused a higher than expected level of requests to our database. The resulting high load errors were responsible for the impacted users' inability to load messages and channels.


We immediately reverted this change, which resolved the issue for affected users. We then deployed a fix to prevent this from reoccurring in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/nj39bg7wbgdq</id>
        <link href="https://www.githubstatus.com/incidents/nj39bg7wbgdq"/>
        <updated>2023-08-17T19:28:50.000Z</updated>
        <summary type="html"><![CDATA[Aug 17, 19:28 UTC
Resolved - This incident has been resolved.
Aug 17, 19:12 UTC
Update - Intermittent large runners and self hosted runners issue has been mitigated
Aug 17, 19:12 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/jg4xwzwflb3r</id>
        <link href="https://www.githubstatus.com/incidents/jg4xwzwflb3r"/>
        <updated>2023-08-17T19:09:11.000Z</updated>
        <summary type="html"><![CDATA[Aug 17, 19:09 UTC
Resolved - This incident has been resolved.
Aug 17, 18:44 UTC
Update - Customers using large runners and self hosted runners may see delays over 10 minutes for actions runs. We are working on mitigating. This effects approximately 1% of customers.
Aug 17, 18:36 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VPC Connectivity Disruption in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/fwqxkfl5glc1</id>
        <link href="https://status.digitalocean.com/incidents/fwqxkfl5glc1"/>
        <updated>2023-08-17T01:23:39.000Z</updated>
        <summary type="html"><![CDATA[Aug 17, 01:23 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue impacting VPC network connectivity in NYC3. As of 23:58 UTC, users should now be able to place resources correctly into VPC's in NYC3 via API create requests, and VPC connectivity between NYC3 resources should now be restored.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Aug 17, 00:44 UTC
Monitoring - The mitigation performed by our Engineering team has been confirmed as resolving customer impact for the issue with VPC's in NYC3. 
We'll continue to monitor for a short period and will post an update once we confirm this issue is fully resolved.
Aug 17, 00:22 UTC
Identified - Our Engineering team identified an issue with VPC Connectivity for resources in our NYC3 region. 
Users creating new resources were not able to place them correctly into VPC's in NYC3 via API create requests. Users also saw network connectivity errors between members of NYC3 VPC's. This issue impacted Droplet-based products like Load Balances and Managed Kubernetes clusters as well, manifesting in various VPC-related errors.
Our team was able to mitigate this issue and we're currently monitoring the situation. As of 00:00 UTC, users should see normal VPC behavior in NYC3. 
We apologize for the disruption and will post another update shortly.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/cst5jm8gwzsz</id>
        <link href="https://status.digitalocean.com/incidents/cst5jm8gwzsz"/>
        <updated>2023-08-16T06:05:59.000Z</updated>
        <summary type="html"><![CDATA[Aug 16, 06:05 UTC
Resolved - As of 05:55, our Engineering team has resolved the issue impacting Python builds for the App platform. The users will not be experiencing the error "Requested runtime 'python-3.11.3' is not available for this stack (heroku-18)" during the build deployment. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our Support team for further review.
Aug 16, 05:28 UTC
Monitoring - As of 05:18 UTC, our Engineering team has deployed a fix to resolve the issue impacting Python builds for App platform. We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. Thank you for your patience and we apologize for the inconvenience.
Aug 16, 01:30 UTC
Update - Our Engineering team continues work on the rollout with the fix for Python builds failing in App Platform. 
We've identified that a very small number of Apps are impacted by this issue - less than 1% of users have encountered this issue. 
We will post an update once the rollout completes.
Aug 15, 20:27 UTC
Identified - Our Engineering team has investigated and identified an issue impacting Python builds for App Platform. Beginning approximately 17:30 UTC, users will see the error "Requested runtime 'python-3.11.3' is not available for this stack (heroku-18)". 
The team is in the process of rolling out a fix. 
In the meantime as a workaround, users may update their Apps to use the Ubuntu 22.04-based stack. This stack is in beta, but provides dependency upgrades, and removes support for PHP version 8.0 and earlier. To update, users will need to specify the new stack in the app spec by adding: 
"features:
    - buildpack-stack=ubuntu-22"
We will post another update once the fix has completed rolling out.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Users authenticating to networks utilizing RADIUS are experiencing timeouts]]></title>
        <id>https://status.rippling.com/incidents/233zk80cc63v</id>
        <link href="https://status.rippling.com/incidents/233zk80cc63v"/>
        <updated>2023-08-15T23:10:30.000Z</updated>
        <summary type="html"><![CDATA[Aug 15, 23:10 UTC
Resolved - This incident has been resolved.
Aug 15, 23:06 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Aug 15, 23:04 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/ljhhhw2tplqp</id>
        <link href="https://www.githubstatus.com/incidents/ljhhhw2tplqp"/>
        <updated>2023-08-15T21:46:57.000Z</updated>
        <summary type="html"><![CDATA[Aug 15, 21:46 UTC
Resolved - The backlog of webhook deliveries has been processed and delivery times have returned to normal.
Aug 15, 20:33 UTC
Update - We are continuing to process our backlog of webhook deliveries and our estimated time to clear the backlog is 2 hours.
Aug 15, 19:24 UTC
Update - We are processing our backlog of webhook deliveries and estimate that it will take 3 hours to clear. Webhook deliveries will be delayed during this time.
Aug 15, 18:20 UTC
Update - We are continuing to investigate the degraded performance of Webhooks and will provide further updates as we continue to investigate.  The current impact is delayed delivery of webhooks.
Aug 15, 18:05 UTC
Update - We are continuing to investigate the degraded performance of Webhooks. The current impact is delayed delivery of hooks.
Aug 15, 17:24 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRA1 Network Maintenance Phase 2]]></title>
        <id>https://status.digitalocean.com/incidents/y7tbd8m83pzj</id>
        <link href="https://status.digitalocean.com/incidents/y7tbd8m83pzj"/>
        <updated>2023-08-15T20:59:32.000Z</updated>
        <summary type="html"><![CDATA[Aug 15, 20:59 UTC
Completed - The scheduled maintenance has been completed.
Aug 15, 18:02 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 15, 17:06 UTC
Scheduled - Start: 2023-08-15 18:00 UTC
End: 2023-08-15 22:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the FRA1 region. This will be the second of the two maintenance activities performed by our team in the region on consecutive days.
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/fjj29dxw0m7c</id>
        <link href="https://www.githubstatus.com/incidents/fjj29dxw0m7c"/>
        <updated>2023-08-15T17:41:34.000Z</updated>
        <summary type="html"><![CDATA[Aug 15, 17:41 UTC
Resolved - This incident has been resolved.
Aug 15, 17:24 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRA1 Network Maintenance Phase 1]]></title>
        <id>https://status.digitalocean.com/incidents/jqpf8zhmx2j3</id>
        <link href="https://status.digitalocean.com/incidents/jqpf8zhmx2j3"/>
        <updated>2023-08-14T22:23:29.000Z</updated>
        <summary type="html"><![CDATA[Aug 14, 22:23 UTC
Completed - The scheduled maintenance has been completed.
Aug 14, 20:12 UTC
Update - Our Engineering team has completed the rollback and VPC connectivity for all Droplets and Droplet-based services is restored. 
We'll now continue this maintenance as scheduled. 
We apologize for the brief interruption.
Aug 14, 20:03 UTC
Verifying - Our Engineering team has identified an issue stemming from this maintenance, which impacts approximately 1-3% of Droplets and Droplet-based services in our FRA1 region. 
At this time, a small percentage of customers will experience VPC network connectivity issues. An emergency rollback is already underway to mitigate this issue. 
We will update this post with further information as soon as possible.
Aug 14, 18:42 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 14, 18:41 UTC
Scheduled - Start: 2023-08-14 18:00 UTC
End: 2023-08-14 22:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the FRA1 region. This maintenance will occur in two parts on consecutive days and we will send another maintenance notice for the second phase. 
Expected Impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions or concerns regarding this maintenance, please reach out to us by opening up a ticket on your account via https://cloudsupport.digitalocean.com/s/createticket.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/45pv9lj0zj1v</id>
        <link href="https://status.digitalocean.com/incidents/45pv9lj0zj1v"/>
        <updated>2023-08-12T04:27:09.000Z</updated>
        <summary type="html"><![CDATA[Aug 12, 04:27 UTC
Resolved - From 23:16 - 04:00, updates to DigitalOcean Container Registry images were not displayed correctly in the UI (Cloud Control Panel), due to a lapsed internal authorization token. Our Engineering team was able to issue a new token and rolled out the change regionally in order to not disrupt multiple data planes at once. 
This issue is fully resolved. If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Aug 12, 03:54 UTC
Monitoring - Our Engineering team has confirmed the root cause of the issue with updates to the UI for DigitalOcean Container Registry and a fix is now rolling out regionally. 
As this rolls out, we expect users to once again be able to see updates to Container Registry images in the UI (Cloud Control Panel). 
We will post another update once the fix finishes rolling out in all regions.
Aug 12, 02:09 UTC
Identified - From 23:16 UTC, our Engineering team has identified an issue with DigitalOcean Container Registry and is actively working on a fix. During this time, customers will not be able to see changes on UI for new DOCR Images in multiple regions. We will post an update as soon as additional information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/sz8pjjhzcy3t</id>
        <link href="https://www.githubstatus.com/incidents/sz8pjjhzcy3t"/>
        <updated>2023-08-11T21:41:56.000Z</updated>
        <summary type="html"><![CDATA[Aug 11, 21:41 UTC
Resolved - This issue has been resolved.
Aug 11, 20:44 UTC
Update - From 16:45-20:01 UTC, OAuth App and GitHub App user auth tokens created with OAuth flow were not SAML authorized as expected. App authorizations created during this time may need to be reauthorized.
Aug 11, 20:44 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGP1 Network Maintenance - Phase 2]]></title>
        <id>https://status.digitalocean.com/incidents/xh9qhhhvcl77</id>
        <link href="https://status.digitalocean.com/incidents/xh9qhhhvcl77"/>
        <updated>2023-08-10T23:24:53.000Z</updated>
        <summary type="html"><![CDATA[Aug 10, 23:24 UTC
Completed - The scheduled maintenance has been completed.
Aug 10, 20:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug  6, 14:39 UTC
Scheduled - Start: 2023-08-10 20:00 UTC
End:  2023-08-11 00:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the SGP1 region. 
Expected impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support tickets submission issue]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/6c9mb4kd3fy7</id>
        <link href="https://airbnbapi.statuspage.io/incidents/6c9mb4kd3fy7"/>
        <updated>2023-08-10T22:27:41.000Z</updated>
        <summary type="html"><![CDATA[Aug 10, 15:27 PDT
Resolved - This incident has been resolved.
Aug 10, 05:08 PDT
Investigating - We are currently investigating an issue that prevents partners from creating Support Tickets. For critical issues that require escalation, kindly contact your Airbnb Connectivity Partner Manager. 
Please see ticket priority guidelines - https://developer.airbnb.com/docs/create-and-manage-support-cases#ticket-priority
Our engineering teams are working to get everything up and running again, and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble signing into the desktop app for some Enterprise Grid customers]]></title>
        <id>https://status.slack.com//2023-08/c68673e38899ac38</id>
        <link href="https://status.slack.com//2023-08/c68673e38899ac38"/>
        <updated>2023-08-10T21:28:27.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

On August 9, 2023 from 9:00 AM PDT to August 10th 2023, 8:00 AM PDT, some Enterprise Grid users experienced a failure to redirect from browser to app during login - focus was moved to the app, but the sign in process did not complete.


We made a change to some logging functions and introduced a fix that mitigated the issue. This fix is permanent and will keep this issue from happening again.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/rk1918knvr0w</id>
        <link href="https://www.githubstatus.com/incidents/rk1918knvr0w"/>
        <updated>2023-08-10T21:20:23.000Z</updated>
        <summary type="html"><![CDATA[Aug 10, 21:20 UTC
Resolved - This incident has been resolved.
Aug 10, 21:07 UTC
Update - Webhooks delivery latency has recovered, and we will continue to monitor.
Aug 10, 20:56 UTC
Update - Actions is operating normally.
Aug 10, 20:21 UTC
Update - We have applied mitigations and are seeing improvements in webhooks delivery latency. We continue to monitor for full recovery. We are also working to mitigate unrelated issues with Actions.
Aug 10, 19:47 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Aug 10, 19:33 UTC
Update - We are still working to resolve high webhooks delivery latency.
Aug 10, 18:54 UTC
Update - We are still seeing high latency in webhook delivery and are actively working to resolve the issue.
Aug 10, 18:12 UTC
Update - Webhooks deliveries are experiencing high latency. We are actively investigating and working to mitigate.
Aug 10, 17:35 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGP1 Network Maintenance - Phase 1]]></title>
        <id>https://status.digitalocean.com/incidents/k65k44vxwr9j</id>
        <link href="https://status.digitalocean.com/incidents/k65k44vxwr9j"/>
        <updated>2023-08-09T22:46:01.000Z</updated>
        <summary type="html"><![CDATA[Aug  9, 22:46 UTC
Completed - The scheduled maintenance has been completed.
Aug  9, 20:01 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug  6, 14:36 UTC
Scheduled - Start: 2023-08-09 20:00 UTC
End:  2023-08-10 00:00 UTC
During the above window, our Networking team will be making changes to our core networking infrastructure to improve performance and scalability in the SGP1 region. This maintenance will occur in two parts on consecutive days and we will send another maintenance notice for the second phase.
Expected impact:
These upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience a temporary loss of private connectivity between VPCs. We will endeavor to keep any such impact to a minimum.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/nwb8m3ppfxw6</id>
        <link href="https://www.githubstatus.com/incidents/nwb8m3ppfxw6"/>
        <updated>2023-08-09T20:14:05.000Z</updated>
        <summary type="html"><![CDATA[Aug  9, 20:14 UTC
Resolved - This incident has been resolved.
Aug  9, 20:02 UTC
Update - Git Operations is operating normally.
Aug  9, 19:54 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Aug  9, 19:42 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in AMS3, LON1, and SFO2]]></title>
        <id>https://status.digitalocean.com/incidents/q4b09b022nmh</id>
        <link href="https://status.digitalocean.com/incidents/q4b09b022nmh"/>
        <updated>2023-08-09T16:01:48.000Z</updated>
        <summary type="html"><![CDATA[Aug  9, 16:01 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services. Users should now be able to process events normally for Droplets and Droplet-based services like Load Balancers, Kubernetes or Database clusters, etc.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Aug  9, 14:58 UTC
Monitoring - The rollout of the fix to redundant networking equipment is fully completed, meaning all networking devices in AMS3, LON1, and SFO2 have been patched. Our Engineering team s…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues uploading images and creating canvases.]]></title>
        <id>https://status.slack.com//2023-08/5f97959abbaeeaa1</id>
        <link href="https://status.slack.com//2023-08/5f97959abbaeeaa1"/>
        <updated>2023-08-07T17:39:42.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On August 7, 2023, customers may have experienced issues with loading images, creating canvases, and workflow modals being delayed. A small number of customers may have been impacted as early as 5:50 AM PDT, with larger impact seen between 7:03AM PDT to 7:14AM PDT.


We traced the issue to a change that caused our server infrastructure to be overloaded. We rolled back this change, resolving the issue for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increased site latency]]></title>
        <id>https://status.rippling.com/incidents/s4g3w61q9lsr</id>
        <link href="https://status.rippling.com/incidents/s4g3w61q9lsr"/>
        <updated>2023-08-07T17:35:47.000Z</updated>
        <summary type="html"><![CDATA[Aug  7, 17:35 UTC
Resolved - This incident has been resolved. Site performance should be back to normal.
Aug  7, 16:26 UTC
Monitoring - We've stopped a data migration job that is believed to be contributing, and we're now seeing site latency return back to baseline numbers. The site latency should now be back to normal, but we are continuing to monitor.
Aug  7, 16:01 UTC
Investigating - There is degraded performance across the Rippling application, with certain pages loading slowly. We are investigating the issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues connecting to Slack]]></title>
        <id>https://status.slack.com//2023-08/8ec13e4962a9bf43</id>
        <link href="https://status.slack.com//2023-08/8ec13e4962a9bf43"/>
        <updated>2023-08-03T02:32:16.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 2, 2023 from 9:01 AM PDT to 11:00 AM PDT, customers may have experienced issues with blurry images in Slack, as well as trouble uploading files. A small percentage of customers may have also experienced errors or slowness when loading channels, sending messages, accessing admin pages, and connecting to Slack.


An issue with a routine database cluster migration inadvertently caused a decrease in capacity. Unfortunately, this coincided with a scheduled job to process a large number of custom status requests, an activity that commonly occurs at the top of each hour. The number of incoming requests exceeded the unintentionally reduced capacity of the database cluster, leading to errors and sluggishness for some actions in Slack. As these requests piled up, other Slack services began to experience issues as well, including a small number of users being unable to connect.


We paused the scheduled custom status job to immediately mitigate the load on the affected database cluster. This resolved some of the errors affecting Slack services. Once we established we had sufficient capacity to handle the scheduled job, we resumed processing the custom status requests, fully restoring normal service for all affected users.


Once the load was balanced and custom status processing had returned to normal, we deployed a fix to correct the issue with the database cluster migration, bringing capacity back to the expected level.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/s9scz41r9wkx</id>
        <link href="https://www.githubstatus.com/incidents/s9scz41r9wkx"/>
        <updated>2023-08-01T20:28:01.000Z</updated>
        <summary type="html"><![CDATA[Aug  1, 20:28 UTC
Resolved - This incident has been resolved.
Aug  1, 20:12 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Push Notifications]]></title>
        <id>https://status.notion.so/incidents/56mkzzfczbws</id>
        <link href="https://status.notion.so/incidents/56mkzzfczbws"/>
        <updated>2023-08-01T17:10:44.000Z</updated>
        <summary type="html"><![CDATA[Aug  1, 10:10 PDT
Resolved - This incident has been resolved.
Aug  1, 10:10 PDT
Investigating - Between 7/26 and 8/1, Notion experienced an issue which prevented users from receiving push notifications on mobile devices. The issue was resolved today, at approximately 9:45 am PDT.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
</feed>