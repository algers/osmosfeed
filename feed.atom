<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-12-18T00:23:05.880Z</id>
    <title>osmos::feed</title>
    <updated>2023-12-18T00:23:05.880Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Germany Account Security Carrier Partner Maintenance - Deutsche Telekom]]></title>
        <id>https://status.twilio.com/incidents/k12nf4bvjppf</id>
        <link href="https://status.twilio.com/incidents/k12nf4bvjppf"/>
        <updated>2023-12-18T08:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Dec 18, 00:00 - 08:00 PST
Dec  4, 15:52 PST
Scheduled - Our carrier partner Deutsche Telekom is conducting a planned maintenance from 18 December 2023 at 00:00 PST until 18 December 2023 at 08:00 PST. During the maintenance window, there could be intermittent API request failures for Deutsche Telekom customers.

Impacted Products: Verify Silent Network Auth, Lookup SIM Swap, Lookup Identity Match and Legacy Identity MatchAndAttributes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United States SMS Carrier Partner Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/m9xz21bn5psd</id>
        <link href="https://status.twilio.com/incidents/m9xz21bn5psd"/>
        <updated>2023-12-18T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Dec 17, 22:00 PST Â -Â  Dec 18, 01:00 PST
Dec 15, 08:34 PST
Scheduled - Our SMS carrier partner in the United States is conducting a planned maintenance from 17 December 2023 at 22:00 PST until 18 December 2023 at 01:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T Network in the United States handsets via United States short codes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays To CTBC/Algar Telecom Network In Brazil]]></title>
        <id>https://status.twilio.com/incidents/t2xvtky35sy5</id>
        <link href="https://status.twilio.com/incidents/t2xvtky35sy5"/>
        <updated>2023-12-17T19:36:29.000Z</updated>
        <summary type="html"><![CDATA[Dec 17, 11:36 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to CTBC/Algar Telecom Network In Brazil. This incident has been resolved.
Dec 17, 09:44 PST
Monitoring - We are observing recovery in delivery report delays when sending messages to CTBC/Algar Telecom Network In Brazil. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 17, 07:50 PST
Update - We continue to experience SMS delivery delays when sending messages to CTBC/Algar Telecom Network In Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 17, 06:57 PST
Investigating - We are experiencing SMS delivery delays when sending messages to CTBC/Algar Telecom Network In Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Report Delays to Africell Network in Sierra Leone]]></title>
        <id>https://status.twilio.com/incidents/ttg3x35y1t19</id>
        <link href="https://status.twilio.com/incidents/ttg3x35y1t19"/>
        <updated>2023-12-17T13:40:44.000Z</updated>
        <summary type="html"><![CDATA[Dec 17, 05:40 PST
Resolved - We are no longer experiencing SMS delivery report delays when sending messages to Africell Network in Sierra Leone. This incident has been resolved.
Dec 16, 06:38 PST
Monitoring - We are observing recovery in SMS delivery report delays when sending messages to Africell Network in Sierra Leone. We will continue monitoring the service to ensure a full recovery. We will provide another update in 24 hours or as soon as more information becomes available.
Dec 15, 21:00 PST
Update - We are still experiencing SMS delivery report delays when sending messages to the Africell network in Sierra Leone. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 24 hours or as soon as more information becomes available.
Dec 14,â€¦]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Call Issues in Programmable Voice Services]]></title>
        <id>https://status.twilio.com/incidents/nwb8py7syrp1</id>
        <link href="https://status.twilio.com/incidents/nwb8py7syrp1"/>
        <updated>2023-12-16T16:39:33.000Z</updated>
        <summary type="html"><![CDATA[Dec 16, 08:39 PST
Resolved - Programmable voice services were degraded for 75 minutes between 06:00 AM and 07:15 AM Pacific Time on 12/15/2023. During this period of time, Programmable Voice and Flex customers may have experienced an increase in latency for call creation and call transfers.  Customers would also have seen an increase in call failures and a spike of errors. The issue has now been resolved.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Globe Network in Philippines]]></title>
        <id>https://status.twilio.com/incidents/ypcnss619qsm</id>
        <link href="https://status.twilio.com/incidents/ypcnss619qsm"/>
        <updated>2023-12-16T09:58:35.000Z</updated>
        <summary type="html"><![CDATA[Dec 16, 01:58 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Globe Network in the Philippines. This incident has been resolved.
Dec 16, 00:13 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Globe Network in the Philippines. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 21:28 PST
Update - We are experiencing SMS Delivery Delay To Globe Network In Philippines - 515-02. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Dec 15, 19:25 PST
Update - We are experiencing SMS Delivery Delay To Globe Network In Philippines - 515-02. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 18:36 PST
Investigating - We are experiencing SMS Delivery Delay To Globe Network In Philippines - 515-02. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Carrier Partner Maintenance - Mexico]]></title>
        <id>https://status.twilio.com/incidents/8gqy9728skz8</id>
        <link href="https://status.twilio.com/incidents/8gqy9728skz8"/>
        <updated>2023-12-16T07:30:08.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 23:30 PST
Completed - The scheduled maintenance has been completed.
Dec 15, 22:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Dec 14, 22:09 PST
Scheduled - Our SMS carrier partner in Mexico is conducting a planned maintenance from 15 December 2023 at 22:00 PST until 15 December 2023 at 23:30 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from Mexico handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevated 500s from account manager service]]></title>
        <id>https://status.twilio.com/incidents/4n5xxl199g13</id>
        <link href="https://status.twilio.com/incidents/4n5xxl199g13"/>
        <updated>2023-12-16T06:23:47.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 22:23 PST
Resolved - We have fully investigated the issue related to an elevated rate of HTTP 500 series responses to account manager service, and it was determined that there is no noticeable customer impact. All systems are operational.
Dec 15, 20:55 PST
Monitoring - The issue related to elevated rate of HTTP 500 series responses to account manager service is now operating normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.
Dec 15, 20:39 PST
Investigating - We are currently investigating an elevated rate of HTTP 500 series responses to account manager service. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Verify Products Affected by Canada API Errors]]></title>
        <id>https://status.twilio.com/incidents/hpwnz7lzmvxb</id>
        <link href="https://status.twilio.com/incidents/hpwnz7lzmvxb"/>
        <updated>2023-12-16T05:17:30.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 21:17 PST
Resolved - The issue with Verify products, impacting Verify Silent Network Auth Lookup Identity Match - Lookup Line Type Intelligence [Twilio API] - Lookup SIM Swap - Legacy Identity MatchAndAttributes over Canada is now operating normally is operating normally at this time.
Dec 15, 20:50 PST
Monitoring - The issue with Verify products, impacting Verify Silent Network Auth Lookup Identity Match - Lookup Line Type Intelligence [Twilio API] - Lookup SIM Swap - Legacy Identity MatchAndAttributes over Canada is now operating normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.
Dec 15, 18:30 PST
Update - We continue to experience an issue with Verify products, impacting Verify Sâ€¦]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Cellularone NE Arizona Network in the US]]></title>
        <id>https://status.twilio.com/incidents/6nff6yd9whk1</id>
        <link href="https://status.twilio.com/incidents/6nff6yd9whk1"/>
        <updated>2023-12-16T02:15:55.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 18:15 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Cellularone NE Arizona Network in the US. This incident has been resolved.
Dec 15, 16:16 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Cellularone NE Arizona Network in the US. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 12:18 PST
Update - We continue to experience SMS delivery delays when sending messages to Cellularone NE Arizona Network in the US. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Dec 15, 10:18 PST
Update - We continue to experience SMS delivery delays when sending messages to Cellularone NE Arizona Network in the US. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 09:18 PST
Investigating - We are experiencing SMS delivery delays when sending messages to Cellularone NE Arizona Network in the US. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent errors while accessing Rippling]]></title>
        <id>https://status.rippling.com/incidents/mppy0hf49wk0</id>
        <link href="https://status.rippling.com/incidents/mppy0hf49wk0"/>
        <updated>2023-12-16T00:30:06.000Z</updated>
        <summary type="html"><![CDATA[Dec 16, 00:30 UTC
Resolved - This incident has been resolved.
Dec 15, 22:45 UTC
Monitoring - A fix from the third party has been implemented and weâ€™re monitoring the results
Dec 15, 22:41 UTC
Investigating - Our third-party network provider has been encountering intermittent issues since December 15th, 02:01 PM PST, leading to errors on the rippling.com domain. We are investigating this issue with the third-party provider.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to All Networks in Colombia Over a Subset Of Short Codes]]></title>
        <id>https://status.twilio.com/incidents/h7b0wkcwfsy0</id>
        <link href="https://status.twilio.com/incidents/h7b0wkcwfsy0"/>
        <updated>2023-12-15T23:50:36.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 15:50 PST
Resolved - The incident causing SMS delays to all networks in Colombia over a subset of short codes was determined to affect only a very small number of Twilio customers. Affected customers have been contacted directly, and this incident will be closed.
Dec 15, 15:48 PST
Investigating - We are experiencing SMS delivery delays when sending to all networks in Colombia from a subset of short codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to CTBC/Algar Telecom in Brazil]]></title>
        <id>https://status.twilio.com/incidents/nzjrkvh19bcp</id>
        <link href="https://status.twilio.com/incidents/nzjrkvh19bcp"/>
        <updated>2023-12-15T20:26:31.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 12:26 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to CTBC/Algar Telecom in Brazil. This incident has been resolved.
Dec 15, 10:23 PST
Update - We are still observing recovery in SMS delivery delays when sending messages to CTBC/Algar Telecom in Brazil. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 08:23 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to CTBC/Algar Telecom in Brazil. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 07:35 PST
Investigating - We are experiencing SMS delivery delays when sending messages to CTBC/Algar Telecom in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A2P Onboarding Registration Wizard Timing out/Looping in Twilio Console for 10DLC Sole Proprietor Registrations]]></title>
        <id>https://status.twilio.com/incidents/j9g70vsx6xrt</id>
        <link href="https://status.twilio.com/incidents/j9g70vsx6xrt"/>
        <updated>2023-12-15T19:25:18.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 11:25 PST
Resolved - The issue with A2P Onboarding Registration wizard in Twilio console has been resolved and it is operating normally at this time.
Dec 15, 09:35 PST
Monitoring - A2P Onboarding Registration wizard in Twilio console is now operating normally. We will continue to monitor for system stability. We'll provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 08:35 PST
Update - We are still investigating a service interruption in Twilio console where customers are getting stuck in a redirect loop in the Sole proprietor A2P Registration wizard. We expect to provide another update in 1 hour or as soon as more information becomes available.
Dec 15, 08:05 PST
Investigating - We are investigating a service interruption where customers are getting stuck in a redirect loop in the A2P Workflow in the Twilio Console. We expect to provide another update in 30 minutes or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singapore Voice Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/pcmn60p553lb</id>
        <link href="https://status.twilio.com/incidents/pcmn60p553lb"/>
        <updated>2023-12-15T18:00:05.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 10:00 PST
Completed - The scheduled maintenance has been completed.
Dec 15, 08:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Dec 12, 17:05 PST
Scheduled - Our Voice carrier partner in Singapore is conducting a planned maintenance from 15 December 2023 at 08:00 PST until 15 December 2023 at 10:00 PST. During the maintenance window, there could be intermittent call disconnects and call failures for calls to and from a subset of Twilio Singapore phone numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to US Cellular in the United States from a Subset of Short Codes]]></title>
        <id>https://status.twilio.com/incidents/2581g4b0z33d</id>
        <link href="https://status.twilio.com/incidents/2581g4b0z33d"/>
        <updated>2023-12-15T14:58:17.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 06:58 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to the US Cellular network in the United States from a Subset of Short Codes. This incident has been resolved.
Dec 15, 05:08 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to the US Cellular network in the United States from a Subset of Short Codes. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 15, 00:04 PST
Update - We continue to experience SMS delivery delays to the US Cellular network in the United States from a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 8 hours or as soon as more information becomes available.
Dec 14, 20:10 PST
Update - We continue to experience SMS delivery delays to the US Cellular network in the United States from a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Dec 14, 18:10 PST
Update - We continue to experience SMS delivery delays to the US Cellular network in the United States from a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 17:10 PST
Investigating - We are experiencing SMS delivery delays to the US Cellular network in the United States from a Subset of Short Codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Google Voice Network in the US for a Subset of Short Codes]]></title>
        <id>https://status.twilio.com/incidents/0r71x8gk9439</id>
        <link href="https://status.twilio.com/incidents/0r71x8gk9439"/>
        <updated>2023-12-15T14:53:51.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 06:53 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Google Voice Network in the US for a Subset of Short Codes. This incident has been resolved.
Dec 15, 05:06 PST
Monitoring - Our Engineers are monitoring the recovery of SMS delivery delays when sending messages to the Google Voice network in the US for a subset of short codes. Our engineers continue to work with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 23:01 PST
Update - We continue to experience SMS delivery delays when sending messages to the Google Voice network in the US for a subset of short codes. Our engineers continue to work with our carrier partner to resolve the issue. We will provide another update in 8 hours or as soon as more information becomes available.
Dec 14, 19:01 PST
Update - We continue to experience SMS delivery delays when sending messages to the Google Voice network in the US for a subset of short codes. Our engineers continue to work with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Dec 14, 17:01 PST
Update - We continue to experience SMS delivery delays when sending messages to the Google Voice network in the US for a subset of short codes. Our engineers continue to work with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 16:00 PST
Investigating - We are experiencing SMS delivery delays when sending messages to the Google Voice network in the US for a subset of short codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Twilio's Messaging Services Feature is Undergoing a Routine Maintenance]]></title>
        <id>https://status.twilio.com/incidents/8v7p6ft6vshh</id>
        <link href="https://status.twilio.com/incidents/8v7p6ft6vshh"/>
        <updated>2023-12-15T08:00:32.000Z</updated>
        <summary type="html"><![CDATA[Dec 15, 00:00 PST
Completed - The scheduled maintenance has been completed.
Dec 14, 23:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Nov 17, 09:25 PST
Scheduled - Weâ€™re conducting a planned maintenance in all countries between 11:00 PM and 12:00 AM Pacific Time on 12/14/2023. During the maintenance window, customers will see high latency and possible 5xx errors while trying to add/remove senders from a Messaging Service.
There will be no impact on any of the messages being sent via a messaging service.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Zamtel Network in Zambia]]></title>
        <id>https://status.twilio.com/incidents/p0wkq6xh6dly</id>
        <link href="https://status.twilio.com/incidents/p0wkq6xh6dly"/>
        <updated>2023-12-15T01:27:33.000Z</updated>
        <summary type="html"><![CDATA[Dec 14, 17:27 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Zamtel network in Zambia. This incident has been resolved.
Dec 14, 11:12 PST
Update - We continue to monitor the SMS service on Zamtel network in Zambia to ensure a full recovery, and we will provide another update in 24 hours or as soon as more information becomes available.
Dec 14, 09:12 PST
Update - We continue to monitor the SMS service on Zamtel network in Zambia to ensure a full recovery, and we will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 07:12 PST
Update - We continue to monitor the SMS service on Zamtel network in Zambia to ensure a full recovery, and we will provide another update in 2 hours or as soon as more information becoâ€¦]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMS Delivery Failures to AT&T Network in the US from a Subset of Toll Free Numbers]]></title>
        <id>https://status.twilio.com/incidents/43gvbfgzz4j8</id>
        <link href="https://status.twilio.com/incidents/43gvbfgzz4j8"/>
        <updated>2023-12-15T01:05:18.000Z</updated>
        <summary type="html"><![CDATA[Dec 14, 17:05 PST
Resolved - We are no longer experiencing MMS delivery failures when sending to the AT&T network in the US from a subset of toll free numbers. This incident has been resolved.
Dec 14, 15:04 PST
Monitoring - We are observing successful MMS delivery when sending to the AT&T network in the US from a subset of toll free numbers. We expect to provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 14:52 PST
Update - We continue to experience MMS delivery failures when sending to the AT&T network in the US from a subset of toll free numbers. Our engineers continue to work with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 14:01 PST
Investigating - We are experiencing MMS delivery failures when sending to the AT&T network in the US from a subset of toll free numbers. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to US Cellular in the United States From a Subset of Shortcodes]]></title>
        <id>https://status.twilio.com/incidents/0yw215clrdgt</id>
        <link href="https://status.twilio.com/incidents/0yw215clrdgt"/>
        <updated>2023-12-14T23:58:02.000Z</updated>
        <summary type="html"><![CDATA[Dec 14, 15:58 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to US Cellular in the United States from a subset of shortcodes. This incident has been resolved.
Dec 14, 13:57 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to US Cellular in the United States from a subset of shortcodes. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 12:50 PST
Update - We are still experiencing SMS delivery delays when sending messages to US Cellular in the United States from a subset of shortcodes. Our engineers are continuing working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Dec 14, 10:50 PST
Update - We are still experiencing SMS delivery delays when sending messages to US Cellular in the United States from a subset of shortcodes. Our engineers are continuing working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Dec 14, 09:50 PST
Investigating - We are experiencing SMS delivery delays when sending messages to US Cellular in the United States from a subset of shortcodes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We're investigating reports of an issue with Classroom. We will provide more information shortly.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/jPWZ4p7bTSq65P1b9Gfn</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/jPWZ4p7bTSq65P1b9Gfn"/>
        <updated>2023-12-14T00:45:03.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-12-07 18:00</strong> and ended at <strong>2023-12-07 20:32</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Incident Report</h1>
<h2>Summary</h2>
<p>On 07 December 2023, various Google Workspace and Google Cloud customers experienced intermittent authentication issues when routed to one of our data centers in the US East region. As a result, a limited number of users could not sign-up, login, or re-authenticate for a duration of 1 hour and 32 minutes.</p>
<p>To our Google Workspace and Google Cloud customers who were impacted during this issue, we sincerely apologize â€“ this is not the level of quality and reliability we strive to offer you.</p>
<h2>Root Cause</h2>
<p>Googleâ€™s authentication stack is broad and deep, running across every region and zone to provide functionality globally across all Google products. This particular issue was scoped to a single microservice&#39;s serving jobs within a single data center; this microservice handles serving some of the user-facing pages involved in sign-up, login, and re-authentication flows.</p>
<p>To improve resource usage efficiency, engineers had been implementing more advanced autoscaling algorithms across these serving jobs globally. These changes were rolled out gradually over a period of several weeks without any issues.</p>
<p>On 7 December, the rollout kicked off rolling restarts across the individual tasks within the serving jobs in a single data center in the US East region. Due to internal network routing at the time, this data center was serving traffic from the US East area as well as South America. These factors placed additional demand on each task&#39;s thread pool, both overloading some tasks and affecting those tasks&#39; ability to report metrics back to load-balancers. This caused requests to be intermittently dropped between the load-balancer and the individual serving tasks; these dropped requests led to user-facing 500 error pages.</p>
<h2>Remediation and Prevention</h2>
<p>Google engineers were alerted by our internal monitoring system on 07 December at 11:16 US/Pacific and immediately started investigating the issue. Once the nature and scope of the issue became clear, Google engineers worked to confirm that traffic could be safely re-routed away from the impacted serving jobs as a mitigation strategy. Traffic was rerouted to other data centers, and the customer impact was mitigated by 12:32 US/Pacific. Our engineers then reviewed and tuned the stability and performance of both this and related serving jobs globally before declaring the incident fully resolved.</p>
<p>Google is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business. We are committed to preventing a repeat of this issue in the future and are completing the following actions:</p>
<ul>
<li>Enhance metrics and dashboards to make these complex load-balancing dynamics more legible to engineers, reducing the time to mitigate similar future issues.</li>
<li>Improve performance-testing of resource-allocation profiles for each individual serving job</li>
</ul>
<h2>Detailed Description of Impact</h2>
<p><strong>Google Workspace:</strong></p>
<ul>
<li>During the issue, a limited number Google Workspace users served from this data center had issues with log-in and sign-up for part of unauthenticated traffic, including some users going through &quot;Sign in with Google &#39;&#39; interactive flows using their Google Workspace account</li>
<li>Multiple Google Workspace product log-in attempts, reauthentication attempts, and some new account creation requests from users routed to this data center failed with error 502s.</li>
<li>Users that were already logged in were not affected by the issue.</li>
</ul>
<p><strong>Google OAuth:</strong></p>
<ul>
<li>Some users served from this data center intermittently experienced error 502s when attempting to &quot;Sign in with Google&quot; and encountering interactive sign-in or re-authentication flows.</li>
</ul>
<p><strong>Google Cloud Console:</strong></p>
<ul>
<li>A limited number of  users served from this data center experienced error 502s intermittently when attempting to log-in to the Cloud Console.</li>
</ul>
<hr>
</div><hr><p>Affected products: Classroom</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delay in scenario executions on eu1.make.celonis.com]]></title>
        <id>https://status.make.com/incidents/671c34fz9z5z</id>
        <link href="https://status.make.com/incidents/671c34fz9z5z"/>
        <updated>2023-12-13T13:18:17.000Z</updated>
        <summary type="html"><![CDATA[Dec 13, 14:18 CET
Resolved - The platform remains stable. The root cause has been identified, and we will actively work on the matter to prevent future reoccurrences.
Dec 12, 15:12 CET
Monitoring - We have identified delays in scenario executions due to intermittent network issues affecting communication between our platform and one of our third-party services. We are actively working with the provider to investigate this and will share more information as soon as it becomes available.
Dec 12, 14:15 CET
Update - The zone is currently stable. Scenario execution duration times are back to normal. We are still investigating the culprit.
Dec 12, 13:45 CET
Investigating - We are currently experiencing issues with increased scenario execution and webhook response times. We are actively investigating the issue and we will provide an update within the next 15 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planned maintenance on status.make.com]]></title>
        <id>https://status.make.com/incidents/hfzsmxh14lmh</id>
        <link href="https://status.make.com/incidents/hfzsmxh14lmh"/>
        <updated>2023-12-12T12:02:50.000Z</updated>
        <summary type="html"><![CDATA[Dec 12, 13:02 CET
Completed - The scheduled maintenance has been completed.
Dec 12, 10:22 CET
Verifying - Verification is currently underway for the maintenance items.
Dec  5, 09:57 CET
Scheduled - Hello,
Please note that there is a scheduled Make Statuspage maintenance on 12.12.2023, 9-10 AM CET, during which the Make Statuspage will not be available.
Thank you very much for your understanding.
Kind regards,
Make team]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automate your host reviews on Airbnb! â­ï¸]]></title>
        <id>281231</id>
        <link href="https://changelog.bookingsync.com/automate-your-host-reviews-on-airbnb!-281231"/>
        <updated>2023-12-11T16:53:50.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
Exciting news! 
Introducing Host Review Automation on Smily â€“ a game-changer for hosts like you! ðŸš€
Now, with just a few clicks, you can automate the process of posting reviews about your guests on Airbnb.

ðŸ’™ Why you'll love it:
Save Time: Say goodbye to the hassle of manually posting reviews. Our automation feature streamlines the process, allowing you to focus on what matters most â€“ creating unforgettable guest experiences.
â€‹
Consistency: Donâ€™t miss any opportunity to provide a 5-star review. It encourages positive behavior from your guests, enhances your guest experience, and fosters a sense of appreciation and satisfaction.
â€‹
Enhanced Communication: Strengthen your host profile, posting a review encourages your guest to review their experience with you. Timely feedback benefits future guests and contributes to building a trustworthy host reputation.
â€‹
Boost your visibility: Help your property shine bright in search results! Airbnb loves hosts with high review rates â€“ the more, the merrier. Your listing gets pushed to the top, ensuring more eyes on your amazing space.



ðŸŒŸ Activate the Host Review Automation feature today through your Smily account by navigating to Reviews â© Host Reviews â© Automation



ðŸ’¡ For more detailed information, simply click here and follow the easy setup instructions.
We can't wait to hear your thoughts on this fantastic new feature!]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Latency Errors]]></title>
        <id>https://status.notion.so/incidents/r1ppwpszz0bv</id>
        <link href="https://status.notion.so/incidents/r1ppwpszz0bv"/>
        <updated>2023-12-10T03:56:56.000Z</updated>
        <summary type="html"><![CDATA[Dec  9, 19:56 PST
Resolved - Between 6:45 PST - 7:30 PST, users may have encountered errors while saving their changes. This issue has now been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/1hv33vh6sf2l</id>
        <link href="https://status.digitalocean.com/incidents/1hv33vh6sf2l"/>
        <updated>2023-12-08T18:00:00.000Z</updated>
        <summary type="html"><![CDATA[Dec  8, 18:00 UTC
Resolved - From 17:53 - 18:22 UTC, we experienced an issue impacting networking in our BLR1 region, with an impact to internal DigitalOcean services.
During that time, public networking for customer services like Droplets and Droplet-based products was not impacted. However, users experienced issues with CRUD (create, read, update, delete) operations for services in BLR1, such as creating Droplets, managing Spaces Buckets, etc. Additionally, Cloud Firewalls were unable to be applied to services in BLR1 during that timeframe.
Our Engineering team has resolved the issue and all services should be functioning normally.
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/y557k86bkdts</id>
        <link href="https://status.digitalocean.com/incidents/y557k86bkdts"/>
        <updated>2023-12-08T14:33:52.000Z</updated>
        <summary type="html"><![CDATA[Dec  8, 14:33 UTC
Resolved - As of 09:18 UTC, our Engineering team has confirmed the resolution of the issue impacting Spaces CDN in our SGP1 region. Users should no longer experience issues when accessing their Spaces resources over the CDN endpoint in the SGP1 region. 
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Dec  8, 09:47 UTC
Identified - As of 07:45 UTC, our Engineering team has been made aware of an issue with the Spaces CDN in the SGP1 region. During this time, users may experience errors for objects served over the CDN. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSO Log-In Issues for Windows]]></title>
        <id>https://status.notion.so/incidents/9jymy8sknh0f</id>
        <link href="https://status.notion.so/incidents/9jymy8sknh0f"/>
        <updated>2023-12-05T21:22:51.000Z</updated>
        <summary type="html"><![CDATA[Dec  5, 13:22 PST
Resolved - The team will be investigating this incident as bug.
Dec  5, 12:16 PST
Monitoring - The incident has been identified as a bug and the team is actively monitoring reports and the issue
Dec  5, 11:08 PST
Investigating - We are currently investigating an issue where Notion users are unable to log-in via SSO on the Windows Desktop App]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble loading some parts of Slack]]></title>
        <id>https://status.slack.com//2023-12/b0072b040087e20b</id>
        <link href="https://status.slack.com//2023-12/b0072b040087e20b"/>
        <updated>2023-12-05T04:20:17.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On December 3, 2023 from 4:35 PM PST to around 5:29 PM PST, a small number of customers experienced trouble loading parts of Slack such as threads, channels, and direct messages. Affected customers may have also had trouble sending messages and running some workflows.


One of our databases experienced a minor hardware issue. This triggered an automatic intervention from our redundancy systems, and a new database was brought into service. However, the new database entered a state where it could not successfully respond to incoming queries.


As an immediate mitigation step, we performed a manual replacement and redirected the incoming queries to a third database. This resolved the issue for all impacted customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support Contact Form]]></title>
        <id>https://status.digitalocean.com/incidents/8gycz6w7qj30</id>
        <link href="https://status.digitalocean.com/incidents/8gycz6w7qj30"/>
        <updated>2023-12-04T20:32:18.000Z</updated>
        <summary type="html"><![CDATA[Dec  4, 20:32 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with contact form submissions. From 14:59 - 19:27 UTC, submissions from https://www.digitalocean.com/company/contact were not being routed to our Support teams. All submissions have been routed to our teams and are being addressed.
If you continue to experience problems, please reach out to our support team. Thank you for your patience throughout this incident!
Dec  4, 20:21 UTC
Monitoring - Our Engineering team has completed the fix to resolve the issue with submissions from our Support Contact form and at this time, services should be functioning as expected. 
Our Support team is addressing all submissions as quickly as possible. We're monitoring the fix and will post an update as soon as we confâ€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with accessing Help Center]]></title>
        <id>https://status.rippling.com/incidents/53rn1yy678tz</id>
        <link href="https://status.rippling.com/incidents/53rn1yy678tz"/>
        <updated>2023-12-02T04:59:20.000Z</updated>
        <summary type="html"><![CDATA[Dec  2, 04:59 UTC
Resolved - This incident has been resolved.
Dec  2, 01:41 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Dec  2, 00:12 UTC
Identified - The issue has been identified and a fix is being implemented.
Dec  2, 00:12 UTC
Investigating - Customers are experiencing issues with accessing the Rippling Help Center. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/fj7v3z6fy7ym</id>
        <link href="https://www.githubstatus.com/incidents/fj7v3z6fy7ym"/>
        <updated>2023-12-01T18:16:24.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 18:16 UTC
Resolved - On December 1st, GitHubâ€™s internal metrics highlighted increased latency for Copilot Chat requests. We declared an incident while investigating and discovered that the issue was only affecting GitHub employees. It was caused by a performance issue in a recently enabled feature that was only available internally.
We resolved the issue by disabling the feature.
The team has invested in improved telemetry, monitoring, and playbooks for Copilot Chat to ensure we can accurately assess impact and respond appropriately to alerting.
Dec  1, 17:36 UTC
Update - A small percentage of Copilot Chat users are still experiencing long request times and errors. We are still investigating to determine the root cause.
Dec  1, 16:41 UTC
Update - Some customers are experiencing higher latency for Copilot Chat. We are continuing our investigation.
Dec  1, 15:55 UTC
Update - Copilot is experiencing degraded performance. We are continuing to investigate.
Dec  1, 15:53 UTC
Update - We are investigating reports that that some customers are experiencing increased latency and failed requests for Copilot Chat.
Dec  1, 15:49 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Login code delay]]></title>
        <id>https://status.notion.so/incidents/4htm8cyzfh5w</id>
        <link href="https://status.notion.so/incidents/4htm8cyzfh5w"/>
        <updated>2023-12-01T11:30:54.000Z</updated>
        <summary type="html"><![CDATA[Dec  1, 03:30 PST
Resolved - Between 3:30 AM - 5:50 AM PST, some users experienced delays with their sign up and login code emails sending.
This issue was resolved at 5:50 AM PST. All sign up and login code emails should be delivered as expected.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in BLR1 for a Subset of Droplets]]></title>
        <id>https://status.digitalocean.com/incidents/1n8mgx97nfb0</id>
        <link href="https://status.digitalocean.com/incidents/1n8mgx97nfb0"/>
        <updated>2023-11-30T18:12:21.000Z</updated>
        <summary type="html"><![CDATA[Nov 30, 18:12 UTC
Resolved - Beginning around 16:48 UTC, our Engineering team identified an issue impacting a subset of Droplets in our BLR1 region. From 16:48 UTC to 17:45 UTC, impacted Droplets did not have correctly functioning networking capabilities. Additionally, users were unable to perform any events (such as power off, recycle, resize, etc) and any changes to the Cloud Firewalls for impacted Droplets were not being applied.
Our Engineering team has resolved the issue and networking and the Cloud Firewalls on all Droplets should be functioning normally.  
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with some users who are attempting to enroll their Windows computers with Rippling's MDM server]]></title>
        <id>https://status.rippling.com/incidents/3vm3nr1kcc68</id>
        <link href="https://status.rippling.com/incidents/3vm3nr1kcc68"/>
        <updated>2023-11-29T22:16:11.000Z</updated>
        <summary type="html"><![CDATA[Nov 29, 22:16 UTC
Resolved - This incident has been resolved.
Nov 29, 19:38 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 29, 17:22 UTC
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Slack not loading for some users]]></title>
        <id>https://status.slack.com//2023-11/edc9f768095e939b</id>
        <link href="https://status.slack.com//2023-11/edc9f768095e939b"/>
        <updated>2023-11-29T14:41:20.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 9:30 AM PST on November 28, 2023 to 12:30 AM PST on November 29, 2023, some users in Asia were experiencing issues with Slack loading. Most of the impacted users were in Japan and India.


We traced this back to an unexpected spike in a database load for which we quickly saw recovery.


We monitored the situation closely and as the load reduced without intervention, we did not take any mitigating actions.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/dq9m4n5tfyr6</id>
        <link href="https://www.githubstatus.com/incidents/dq9m4n5tfyr6"/>
        <updated>2023-11-28T19:40:12.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 19:40 UTC
Resolved - This incident has been resolved.
Nov 28, 19:36 UTC
Update - We were not able to publish webhooks in response to push events triggered between 16:23 and 17:12 UTC. To avoid further disruption to customer workflows, weâ€™ve decided not to continue our attempts to re-process those events. 
Workflows that trigger on pull_request or git push events may not have been run during this time period. 
You can run impacted workflows manually or by pushing a new commit to the same branch.
Nov 28, 18:09 UTC
Update - Customers saw push event deliveries for Actions and Webhooks fail between 16:23 and 17:12 (UTC). We fixed the issue, and we are working to re-process push events for the affected time period.
Nov 28, 18:09 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/dsb3kn1zfdl7</id>
        <link href="https://www.githubstatus.com/incidents/dsb3kn1zfdl7"/>
        <updated>2023-11-28T17:59:15.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 17:59 UTC
Resolved - This incident has been resolved. An interaction between two feature flag rollouts caused us to suppress delivery of push webhooks between 16:23 and 17:11 UTC in a manner that evaded our existing observability. This affected 71,000 repositories whose users will have noted missing webhook deliveries and/or experienced Actions jobs with push triggers failing to start. During this period, around 25% of new Actions jobs were impacted. The issues were resolved by disabling one of the feature flags in question.
After weighing various options for retroactively dispatching old push webhooks, we concluded that the risk of delivering stale data to customers with these redeliveries outweighed the possible benefit.
As follow up to this incident, we are working to improve monitoring of webhook throughput and to document a policy around webhook redelivery timelines.
Nov 28, 17:59 UTC
Update - Actions is operating normally.
Nov 28, 17:36 UTC
Update - Customers saw pull requests push event deliveries for Actions and Webhooks fail between 16:23 and 17:12 (UTC). We fixed the issue, and we are working to re-process push events for the affected time period.
Nov 28, 17:24 UTC
Investigating - We are investigating reports of degraded performance for Actions and Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Datastore App timing out]]></title>
        <id>https://status.make.com/incidents/3yz9s3tkz2xh</id>
        <link href="https://status.make.com/incidents/3yz9s3tkz2xh"/>
        <updated>2023-11-28T08:55:30.000Z</updated>
        <summary type="html"><![CDATA[Nov 28, 09:55 CET
Resolved - The implemented workaround has been effective and eu1.make.celonis.com remains stable. The incident has been resolved.
Nov 27, 17:18 CET
Update - The situation stays stable. We are still investigating the root cause of the problem. We will maintain ongoing monitoring and notify once we fully resolve the incident.
Nov 27, 16:31 CET
Monitoring - The tuning of our Data Store infrastructure did relieve the situation and Scenarios using the Data Store App are stable again. We are continuously monitoring the state of our platform. We will come up with an update in the next 30 minutes. Thank you for your understanding and patience.
Nov 27, 15:59 CET
Update - We are still investigating the root cause of this incident. Meanwhile, we are tuning our Data Store infrastructure to relieve the situation and put the platform in a more stable state. We will come up with an update in the next 30 minutes. Thank you for your understanding and patience.
Nov 27, 15:21 CET
Investigating - We have detected multiple instances of the Datastore App timing out and failing on eu1.make.celonis.com. We are currently investigating the root cause and will come back to you once we find more information. Thank you for your understanding and patience.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests, Issues, Webhooks and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/66vhjmd266r9</id>
        <link href="https://www.githubstatus.com/incidents/66vhjmd266r9"/>
        <updated>2023-11-27T21:11:04.000Z</updated>
        <summary type="html"><![CDATA[Nov 27, 21:11 UTC
Resolved - This incident has been resolved.
On November 27, 2023 at 18:46 UTC, we attempted to rotate our OpenID Connect (OIDC) authentication flow certificates. Due to an error in the certificate formatting, we uploaded an invalid certificate configuration that was not observed in our pre-production testing. Our background job servers were unable to start because a valid configuration is required at worker start up. As a result, users experienced delays in Pull Requests, Webhooks, Issues, Actions and Projects. Rollback of the change was slowed by the invalid certificate as our deployment system relied on the same certificate. Rollback was completed at 20:35 UTC. Most services recovered by 20:44 UTC. 

Delayed updates to Issues and Pull Requests were applied normally onceâ€¦]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing our enhanced Stripe integration ðŸš€]]></title>
        <id>279856</id>
        <link href="https://changelog.bookingsync.com/introducing-our-enhanced-stripe-integration-279856"/>
        <updated>2023-11-24T13:25:46.000Z</updated>
        <summary type="html"><![CDATA[New!
Â 
Improvement
Â 
Action required
Â Â 
We're thrilled to share some fantastic updates on our Stripe integration, aiming to improve your Smily experience. ðŸŽ‰
What's New?
Effortless Stripe Account Addition: managing multiple payment gateways just got easier! We've streamlined the process of adding new Stripe accounts for flexibility and convenience. 


Improved Customer Support Access: your satisfaction is our top priority. Now, our customer support is better equipped to assist you with any questions or issues related to your Stripe integration.


Why Does This Matter?
This upgrade is designed with your convenience in mind, offering the following benefits:
Time Saving: adding new Stripe accounts is now a few clicks away, eliminating lengthy back-and-forth communications.


Reduced Frustration: improved support ensures prompt resolution of queries or issues.


Maintained Control: you retain complete control over your Stripe accounts, payouts, and settings, experiencing no change in usage or payment reception.


What's Next?
Ready to unlock the enhanced Stripe Integration? Go to Settings > Payments in your Smily account, click connect on any unconnected Stripe payment gateways, and enjoy seamless transactions. 
Rest assured, these improvements maintain the highest security standards for your financial transactions.
For detailed information on connecting your Stripe account, check our FAQ.
We're here to support you every step of the way. If you have questions or need assistance, reach out to us. 
Let's embark on this journey together to make your vacation rental dreams a reality ðŸš€]]></summary>
        <author>
            <name>Megan, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with the Adobe integration]]></title>
        <id>https://status.rippling.com/incidents/sj7thsw4jlzl</id>
        <link href="https://status.rippling.com/incidents/sj7thsw4jlzl"/>
        <updated>2023-11-23T22:41:38.000Z</updated>
        <summary type="html"><![CDATA[Nov 23, 22:41 UTC
Resolved - This incident has been resolved.
Nov 23, 16:29 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 22, 18:03 UTC
Identified - The issue has been identified and a fix is being implemented.
Nov 22, 17:37 UTC
Investigating - Customers are experiencing issues with the assignment of Adobe licenses to users. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion AI is down]]></title>
        <id>https://status.notion.so/incidents/69628qhdf41j</id>
        <link href="https://status.notion.so/incidents/69628qhdf41j"/>
        <updated>2023-11-22T02:00:20.000Z</updated>
        <summary type="html"><![CDATA[Nov 21, 18:00 PST
Resolved - This is resolved.
Nov 21, 16:50 PST
Update - Provider calls are more stable. We are monitoring our provider.
Nov 21, 16:43 PST
Monitoring - A fix has been implemented and we are monitoring the results.
Nov 21, 16:42 PST
Identified - Notion AI is partially not working as one of our service providers is down]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/x39xrr5m11b3</id>
        <link href="https://www.githubstatus.com/incidents/x39xrr5m11b3"/>
        <updated>2023-11-21T11:27:43.000Z</updated>
        <summary type="html"><![CDATA[Nov 21, 11:27 UTC
Resolved - On November 21, 2023, at 09:50 UTC GitHub Actions jobs encountered delays due to an incident in our background job service caused by excessive rebalancing in a Kafka consumer group. After a quick mitigation, we began to see recovery on the job queues by 10:02 UTC. During this time window 100% of Actions jobs were delayed in starting for up to 11 minutes.
Unfortunately, the rapid queue recovery sent a thundering herd of jobs to Actions hosted runner pools, causing a database deadlock that resulted in some hosted runner pools having increased latency when accepting new jobs. This affected only a small percentage of overall jobs, around 2%. Configuration changes led to a resolution and the system was fully recovered by 11:27 UTC and all in progress jobs were processed.
The incident is now resolved.
Nov 21, 11:12 UTC
Update - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.
Nov 21, 10:24 UTC
Update - We have recovery for the underlying issue but are waiting for Actions queues to catch up. We expect this to be completed in less than 1 hour(s).
Nov 21, 10:11 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>