<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-02-26T00:22:07.393Z</id>
    <title>osmos::feed</title>
    <updated>2024-02-26T00:22:07.393Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[United States SMS Carrier Partner Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/w1bfpgb6kqfr</id>
        <link href="https://status.twilio.com/incidents/w1bfpgb6kqfr"/>
        <updated>2024-02-26T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 25, 22:00 PST  -  Feb 26, 01:00 PST
Feb 22, 18:43 PST
Scheduled - Our SMS carrier partner in the United States is conducting a planned maintenance from 25 February 2024 at 22:00 PST until 26 February 2024 at 01:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T Network in the United States handsets via United States short codes.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Canada SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/qs3jwyyvhsf6</id>
        <link href="https://status.twilio.com/incidents/qs3jwyyvhsf6"/>
        <updated>2024-02-26T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 25, 21:00 PST  -  Feb 26, 00:00 PST
Feb 23, 16:11 PST
Scheduled - Our SMS carrier partner in Canada is conducting a planned maintenance from 25 February 2024 at 21:00 PST until 26 February 2024 at 00:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from Canada handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/6hnyx0v9zjbh</id>
        <link href="https://status.make.com/incidents/6hnyx0v9zjbh"/>
        <updated>2024-02-26T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 26, 06:00 - 08:00 CET
Feb 15, 16:40 CET
Scheduled - Hello,
Please note there is scheduled infrastructure maintenance on 26.02.2024, between 6:00-8:00 AM CET, when you can expect a slower scenario processing. Specifically, scenarios using a datastore module might see reduced performance which can cause its longer execution.
We will inform you once the maintenance is completed.
Thank you very much for understanding.
Kind regards,
Make team]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/pf9dhczrcxq1</id>
        <link href="https://status.twilio.com/incidents/pf9dhczrcxq1"/>
        <updated>2024-02-26T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Feb 25, 19:00 PST  -  Mar 1, 04:00 PST
Feb 24, 01:32 PST
Scheduled - The AT&T network in the US is conducting a series of planned maintenance from 25 February 2024 at 19:00 PST until 01 March 2024 at 04:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T US handsets.

Note, the maintenance will be carried out on each of the following dates and times:

25 February 2024 at 19:00 PST until 26 February 2024 at 04:00 PST
26 February 2024 at 19:00 PST until 27 February 2024 at 04:00 PST
27 February 2024 at 19:00 PST until 28 February 2024 at 04:00 PST
28 February 2024 at 19:00 PST until 29 February 2024 at 04:00 PST
29 February 2024 at 19:00 PST until 01 March 2024 at 04:00 PST]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Receipt and Delivery Delays to AT&T Network in the US]]></title>
        <id>https://status.twilio.com/incidents/6byhqwnyljf5</id>
        <link href="https://status.twilio.com/incidents/6byhqwnyljf5"/>
        <updated>2024-02-26T00:04:23.000Z</updated>
        <summary type="html"><![CDATA[Feb 25, 16:04 PST
Update - We are experiencing SMS Delivery Receipt and Delivery Delays to AT&T Network in the US. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 25, 15:32 PST
Update - We are experiencing SMS Delivery Receipt and Delivery Delays to AT&T Network in the US. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Feb 25, 15:11 PST
Investigating - We are experiencing SMS Delivery Receipt Delays to AT&T Network in the US from a subset of Shortcodes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Failures to Evatis Network in Djibouti]]></title>
        <id>https://status.twilio.com/incidents/5f4wngk78hbl</id>
        <link href="https://status.twilio.com/incidents/5f4wngk78hbl"/>
        <updated>2024-02-25T20:11:30.000Z</updated>
        <summary type="html"><![CDATA[Feb 25, 12:11 PST
Update - We are still experiencing SMS delivery failures to Evatis network in Djibouti. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 8 hours or as soon as more information becomes available.
Feb 25, 08:29 PST
Update - We are still experiencing SMS delivery failures to Evatis network in Djibouti. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 4 hours or as soon as more information becomes available.
Feb 25, 06:31 PST
Update - We are experiencing SMS delivery failures to Evatis Network in Djibouti. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available.
Feb 25, 05:34 PST
Update - We are continuing to investigate this issue.
Feb 25, 05:31 PST
Investigating - We are experiencing SMS delivery failures to Evatis Network in Djibouti. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Safaricom Network in Kenya]]></title>
        <id>https://status.twilio.com/incidents/p6zbnl40pytt</id>
        <link href="https://status.twilio.com/incidents/p6zbnl40pytt"/>
        <updated>2024-02-25T18:19:04.000Z</updated>
        <summary type="html"><![CDATA[Feb 25, 10:19 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Safaricom network in Kenya. This incident has been resolved.
Feb 25, 08:25 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Safaricom network in Kenya. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 25, 06:45 PST
Update - We are experiencing SMS delivery delays when sending messages to Safaricom network in Kenya. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 25, 05:48 PST
Update - We are continuing to investigate this issue.
Feb 25, 05:45 PST
Investigating - We are experiencing SMS delivery delays when sending messages to Safaricom Network in Kenya. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to MTN Irancell Network in Iran]]></title>
        <id>https://status.twilio.com/incidents/n8tww4fdzc2c</id>
        <link href="https://status.twilio.com/incidents/n8tww4fdzc2c"/>
        <updated>2024-02-25T16:22:09.000Z</updated>
        <summary type="html"><![CDATA[Feb 25, 08:22 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to MTN Irancell network in Iran. This incident has been resolved.
Feb 25, 06:26 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to MTN Irancell network in Iran. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 25, 05:24 PST
Investigating - We are experiencing SMS delivery delays when sending messages to MTN Irancell Network in Iran. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Zealand SMS Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/kb8h2y30ls8b</id>
        <link href="https://status.twilio.com/incidents/kb8h2y30ls8b"/>
        <updated>2024-02-24T21:30:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 24, 13:30 PST
Completed - The scheduled maintenance has been completed.
Feb 24, 13:00 PST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  8, 06:46 PST
Scheduled - Our SMS carrier partner in New Zealand is conducting a planned maintenance from 24 February 2024 at 13:00 PST until 24 February 2024 at 13:30 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from a subset of New Zealand handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Mitto Network in Afghanistan]]></title>
        <id>https://status.twilio.com/incidents/c35y7gr00654</id>
        <link href="https://status.twilio.com/incidents/c35y7gr00654"/>
        <updated>2024-02-24T18:26:41.000Z</updated>
        <summary type="html"><![CDATA[Feb 24, 10:26 PST
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Mitto Network in Afghanistan. This incident has been resolved.
Feb 24, 08:35 PST
Update - We keep observing recovery in SMS delivery delays when sending messages to Mitto Network in Afghanistan. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 24, 08:02 PST
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Mitto Network in Afghanistan. We will continue monitoring the service to ensure a full recovery. We will provide another update in 30 min or as soon as more information becomes available.
Feb 24, 06:56 PST
Update - We keep experiencing SMS delivery delays when sending messages to Mitto Network in Afghanistan. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Feb 24, 06:01 PST
Investigating - We are experiencing SMS delivery delays when sending messages to Mitto Network in Afghanistan. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some template duplication may fail]]></title>
        <id>https://status.notion.so/incidents/gnzy4hx09dq6</id>
        <link href="https://status.notion.so/incidents/gnzy4hx09dq6"/>
        <updated>2024-02-24T00:52:27.000Z</updated>
        <summary type="html"><![CDATA[Feb 23, 16:52 PST
Resolved - Our engineering team identified the issue and released a fix to resolve it.
Feb 23, 16:42 PST
Investigating - Customers may experience an error when duplicating some templates.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Threads aren't loading for some users]]></title>
        <id>https://status.slack.com//2024-02/548b72776056c03b</id>
        <link href="https://status.slack.com//2024-02/548b72776056c03b"/>
        <updated>2024-02-23T03:05:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 2:54 PM to 2:59 PM PDT on February 22, 2024, some users were having issues loading Threads.


We carried out a thorough investigation and observed trends, but found no evidence of an issue on our side. By around 2:59 PM PDT, error rates had already subsided and we received confirmation from several users that they could load Threads again by reloading Slack.


Whilst no remedial action was taken on our end, we're analyzing data from this event to understand ways to mitigate similar issues that may occur in the future.


We're confident that there will be no further impact to users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Screen readers are not announcing unread messages when using the Ctrl + K command on Windows.]]></title>
        <id>https://status.slack.com//2024-02/83f4c18fc4d11c93</id>
        <link href="https://status.slack.com//2024-02/83f4c18fc4d11c93"/>
        <updated>2024-02-23T01:46:26.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On February 21, 2024 from 6:30 PM PST to 8:00 PM PST, customers using the JAWS or NVDA screen readers on Windows may have experienced issues with announcements for unread messages in the Quick Switcher (Ctrl + K). 


A recent code change inadvertently introduced a conflict with an HTML label for an interactive element.


We reverted the code change, fixing the issue and restoring normal announcement behaviour.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with password resets]]></title>
        <id>https://status.slack.com//2024-02/e04276624660caf9</id>
        <link href="https://status.slack.com//2024-02/e04276624660caf9"/>
        <updated>2024-02-22T14:32:08.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 


We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 


Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are unable to upload, download, and view files in Slack.]]></title>
        <id>https://status.slack.com//2024-01/f39851209d6c471a</id>
        <link href="https://status.slack.com//2024-01/f39851209d6c471a"/>
        <updated>2024-02-22T14:31:13.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


Between 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. 


We traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. 


Thank you for your patience while we resolved this and we apologize for any disruption to your work day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry Latency in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/n9211k9rmml7</id>
        <link href="https://status.digitalocean.com/incidents/n9211k9rmml7"/>
        <updated>2024-02-21T20:43:48.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 20:43 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting the Container Registry in multiple regions. 
Everything involving the Container Registry should now be functioning normally. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Feb 21, 18:03 UTC
Monitoring - Our Engineering team has identified an internal operation within the Container Registry service which was placing load on the service, leading to latency and errors. The team has paused that operation in order to resolve the issue impacting the Container Registry in multiple regions. Users should not be facing any latency issues while interacting with their Container registries and also while building their Apps. 
We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. 
Thank you for your patience and we apologize for the inconvenience.
Feb 21, 15:41 UTC
Investigating - Our Engineering team is investigating an issue with the DigitalOcean Container Registry service. Beginning around 20:00 UTC on February 20, there has been an uptick in 401 errors for image pulls from the Container Registry service.
During this time, a subset of customers may experience latency or see 401 errors while interacting with Container Registries. This issue also impacts App Platform builds and users may encounter delays while building their Apps or experience timeout errors in builds as a result. Users utilizing Container Registry for images for deployment to Managed Kubernetes clusters may also see latency or failures to deploy.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/wn6s1w8vkk1y</id>
        <link href="https://www.githubstatus.com/incidents/wn6s1w8vkk1y"/>
        <updated>2024-02-21T17:30:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 21, 17:30 UTC
Resolved - On Wednesday February 21, 2024, 17:07 UTC, we deployed a configuration change to one of our services inside of Actions. At 17:14 UTC we noticed an increase in exceptions that impacted approximately 85% of runs at that time. 
At 17:18 UTC, we reverted the deployment and our service immediately recovered. During this timeframe, customers may have noticed their workflows failed to trigger or workflows were queued but did not progress.
To prevent this issue in the future we are improving our deployment observability tooling to detect errors earlier in the deployment pipeline.
Feb 21, 17:20 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We are investigating issues with Google Voice starting at 07:23 PT on 13-February-2024. There is no workaround at this time.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/mvR44baNg5v3UMTjsYr4</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/mvR44baNg5v3UMTjsYr4"/>
        <updated>2024-02-20T21:12:53.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2024-02-13 16:00</strong> and ended at <strong>2024-02-16 22:37</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><p>A mini incident report has been posted at <a href="https://www.google.com/appsstatus/dashboard/incidents/By2PE4BYiuCuJ1wZQ4XV">https://www.google.com/appsstatus/dashboard/incidents/By2PE4BYiuCuJ1wZQ4XV</a></p>
</div><hr><p>Affected products: Google Voice</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Availability in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/rccs80k3p2pb</id>
        <link href="https://status.digitalocean.com/incidents/rccs80k3p2pb"/>
        <updated>2024-02-20T07:07:10.000Z</updated>
        <summary type="html"><![CDATA[Feb 20, 07:07 UTC
Resolved - As of 06:25 am UTC, our Engineering team has confirmed the resolution of the issue impacting Spaces availability in the BLR1 region.
Users should no longer experience issues with their Spaces resources in the BLR1 region.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Feb 20, 06:39 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the Spaces availability issues in the BLR1 region and is monitoring the situation. 
Users should no longer encounter errors when accessing Spaces in the BLR1 region and should be able to create new Spaces buckets from the cloud control panel. 
We will post an update as soon as the issue is fully resolved.
Feb 20, 05:46 UTC
Investigating - Our Engineering team is investigating an issue with Spaces availability in the BLR1 region. During this time users may encounter errors when accessing Spaces objects and creating new buckets in the BLR1 region. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BLR1 Network Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/5z0npmmmnc1h</id>
        <link href="https://status.digitalocean.com/incidents/5z0npmmmnc1h"/>
        <updated>2024-02-19T17:00:56.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 17:00 UTC
Completed - The scheduled maintenance has been completed.
Feb 19, 14:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb 16, 14:52 UTC
Scheduled - Start: 2024-02-19 14:00 UTC
End:  2024-02-19 17:00 UTC
Hello,
During the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration.
Expected impact:
As part of this maintenance, event processing in BLR1 will be disabled for a period of up to 15 minutes during the three-hour window. During this period, users won't be able to create, destroy, or modify new or existing DO services in BLR1 (such as Droplets, DBaaS/DOKS clusters, etc.).
If you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket

Thank you,
Team DigitalOcean]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Products Impacted in BLR1]]></title>
        <id>https://status.digitalocean.com/incidents/q2sl05rc7mgp</id>
        <link href="https://status.digitalocean.com/incidents/q2sl05rc7mgp"/>
        <updated>2024-02-19T15:50:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 19, 15:50 UTC
Resolved - From 15:50 - 16:46, our team received customer reports of issues impacting multiple products in our BLR1 region, including the accessibility of Managed Databases and Managed Kubernetes clusters and general network connectivity disruption. These issues may be related to a scheduled maintenance event in the region, per our status post linked below:
https://status.digitalocean.com/incidents/5z0npmmmnc1h
Our team continues to review customer reports and diagnose the impact related to this maintenance. In the meantime, we have rolled back the maintenance process and all services should now be responding normally. If you experience any further issues, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/yp30nqsv310k</id>
        <link href="https://status.digitalocean.com/incidents/yp30nqsv310k"/>
        <updated>2024-02-18T19:08:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 18, 19:08 UTC
Resolved - As of 17:47 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Managed Kubernetes service in our NYC3 region. The Cilium pods inside the clusters should be functioning normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb 18, 18:09 UTC
Monitoring - Our Engineering team has deployed the fix for the issue with Managed Kubernetes service where users were experiencing network connectivity issues with Cilium pods being restarted inside the clusters. Cilium pods should now be functioning normally. 
We are monitoring the situation and will post another update once we confirm the fix resolves this incident.
Feb 18, 16:57 UTC
Investigating - Our Engineering team is investigating an issue with our Managed Kubernetes service in NYC3 region. 
During this time users may experience network connectivity issues specifically with the Cilium pods inside their clusters.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Reports]]></title>
        <id>https://status.rippling.com/incidents/p5wbyw5s471p</id>
        <link href="https://status.rippling.com/incidents/p5wbyw5s471p"/>
        <updated>2024-02-15T22:38:53.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 22:38 UTC
Resolved - This issue has been resolved. Customers should no longer be impacted by this issue.
Feb 15, 19:50 UTC
Identified - We are aware of issues with Reports loading. We have identified a fix and are working to roll it out.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Errors updating Notion subscription]]></title>
        <id>https://status.notion.so/incidents/qpyywl1mc14q</id>
        <link href="https://status.notion.so/incidents/qpyywl1mc14q"/>
        <updated>2024-02-15T17:30:36.000Z</updated>
        <summary type="html"><![CDATA[Feb 15, 09:30 PST
Resolved - On February 15th 2023 between 01:39 PST - 08:43 PST, Notion was experiencing an issue with subscription updates, which may have resulted in errors when changing billing plans.
Our engineering team identified the issue and increased capacity to this service to prevent errors, and this is now working as normal. We appreciate your patience and will work to prevent similar errors in future.
Feb 15, 08:52 PST
Investigating - Customers may experience an error when attempting to change their Notion subscription.
Our team is investigating the root cause now and we will share updates as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Profile photos aren't displaying properly]]></title>
        <id>https://status.slack.com//2024-02/16aecfd53af7219b</id>
        <link href="https://status.slack.com//2024-02/16aecfd53af7219b"/>
        <updated>2024-02-15T14:40:11.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:


Starting around 12:46 PM PST on February 6, 2024, until around 4:06 PM PST on February 9, 2024, some users experienced issues with their profile pictures loading, resulting in the default avatar image displaying instead of their custom image.


We identified a bug that caused missing image data from a database cache. We fixed the bug and reset the backend cache to pull new data, which initially fixed the issue around 4:00 PM PST on February 6. However, some users began experiencing the problem again once their Slack client refreshed.


We continued investigating the issue, tracing it back to a piece of code that would push older image data to a database that housed profile information, and promptly reverted the incorrect code which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are now supporting and managing Booking.com' Sponsored Benefit program at Smily]]></title>
        <id>285929</id>
        <link href="https://changelog.bookingsync.com/we-are-now-supporting-and-managing-booking-com'-sponsored-benefit-program-at-smily-285929"/>
        <updated>2024-02-15T13:30:51.000Z</updated>
        <summary type="html"><![CDATA[New!
  
We are thrilled to share with you some great news :)
We are now supporting and managing Booking.com Sponsored Benefit program at Smily;
Benefits for you:
You were facing the obstacle where booking.com **was paying a small amount of the booking.com reservation due to a discount offered to the guest.
1/ it was not blocking the synchronisation of your listing but was creating an issue for the payment;
2/ you had to send manually a payment link to collect the remaining amount to the guests. This won’t be necessary anymore!


Now, as we do support the Sponsored benefit program, you will still see and receive this small amount in advance paid by Booking.com and we will process with success the rest of the payment!


You can now join this program if you were waiting for this functionality 🙂
How does this program benefit your business?
When applied, it offers the following advantages for your business:
Boosts bookings from price-conscious customers
Lowers cancellations with customers paying in advance
Increases your revenue by covering the reduced amount to drive more bookings
Take the next step:
Take a look at what this program is about and check out if you are eligible to enable it and enjoy it;
Check our new manual
Your business deserves the best, and with Smily’s latest feature, we’re here to ensure you get just that.]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Airbnb - enrich your listing thanks to the listing expectation types]]></title>
        <id>285927</id>
        <link href="https://changelog.bookingsync.com/airbnb-enrich-your-listing-thanks-to-the-listing-expectation-types-285927"/>
        <updated>2024-02-15T13:24:14.000Z</updated>
        <summary type="html"><![CDATA[New!
  
The Airbnb Listing expectation type feature is now available on Smily. 
This update lets you enrich your listings with additional information, directly from Smily's user-friendly interface.
Pain Points Addressed:
Gone are the days of toggling between platforms. Previously, you had to navigate Airbnb's website to update listing details, a time-consuming and inefficient process. Smily's new feature streamlines this, offering a seamless integration within the interface.
🚀 Benefits for you:
Easily accessible under the "amenities" section in the Smily interface, the new "Property Info" category empowers you to select and customize expectation types, with an optional description of up to 100 characters.
This information is then flawlessly synced with Airbnb for those using full synchronization, ensuring a cohesive and accurate listing.
It will also be synchronised on your website if you have one with Smily!
Such information is key and valuable for the guests to avoid any negative surprises on check-in day!
→ Here is where to find this new section on the Smily interface👇
-> Then click on the pen to edit/add an optional description;

ℹ️ Next steps and important information
For those using this via Airbnb, we've already fetched your existing listing expectations from Airbnb as of February 9th.


To ensure data alignment with this new feature, please update any changes made since then before our official release on February 21st via the Smily interface;
→ On February 21st, we will start the synchronisation of your data from Smily to the Airbnb application. Any values not reflected on the Smily interface will be removed from Airbnb.


Please note, that the Smily interface allows a description limit of 100 characters compared to Airbnb's 300. Should this affect your listings, we'll reach out individually.
→ if you cannot update your description before February 21st, let us know please;


Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble with canvas.]]></title>
        <id>https://status.slack.com//2024-02/804d3a52e234f2fa</id>
        <link href="https://status.slack.com//2024-02/804d3a52e234f2fa"/>
        <updated>2024-02-13T22:15:24.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On February 12, 2024 from 6:00 AM PST to 8:32 AM PST, some users experienced latency when loading canvases or errors related to editing and saving canvases.


Upon investigation we determined that this was caused by rate limits put in place by an upstream provider. We restarted the impacted endpoints to mitigate the issue immediately, which resolved the issue for all affected users. We're continuing to partner with our upstream provider to reduce the likelihood of this occurring in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion app experienced a brief outage due to a deployment that didn't function as expected]]></title>
        <id>https://status.notion.so/incidents/2b5f3nmpht76</id>
        <link href="https://status.notion.so/incidents/2b5f3nmpht76"/>
        <updated>2024-02-12T23:34:06.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 15:34 PST
Resolved - This incident has been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/k0qjh7s64fxk</id>
        <link href="https://www.githubstatus.com/incidents/k0qjh7s64fxk"/>
        <updated>2024-02-12T18:14:07.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 18:14 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to inci…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/vv6z7v5w80yr</id>
        <link href="https://www.githubstatus.com/incidents/vv6z7v5w80yr"/>
        <updated>2024-02-12T12:39:20.000Z</updated>
        <summary type="html"><![CDATA[Feb 12, 12:39 UTC
Resolved - On Monday February 12th, 2024, 03:00 UTC we deployed a code change to a component of Copilot. At 06:00 UTC we observed an increase in timeouts for code completions impacting 55% of Copilot users at peak across Asia and Europe.
At 12:00 UTC we restarted the nodes, and response durations returned to normal operation until 13:00 UTC when response durations degraded again. At 16:15 UTC we made a configuration change to send traffic to regions that were not exhibiting the errors, which resulted in code completions working fully although completing at a higher latency than normal for some users. At 18:00 UTC we reverted the deploy and response durations returned to normal. 
We have added better monitoring to components that failed to decrease resolution times to incidents like this in the future.
Feb 12, 12:29 UTC
Update - We are starting to see recovery based on the signals that the team have been monitoring, following mitigation steps being taken. When confident that recovery is complete, we will resolve this incident.
Feb 12, 12:00 UTC
Update - We are continuing to investigate increased failure rates for Copilot code completion for some users in Europe.
Feb 12, 11:38 UTC
Update - We are investigating reports that GitHub Copilot code completions are not working for some users in Europe.
Feb 12, 11:38 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You can now publish your listings on HomeToGo! 🎉]]></title>
        <id>285614</id>
        <link href="https://changelog.bookingsync.com/you-can-now-publish-your-listings-on-hometogo!-285614"/>
        <updated>2024-02-12T11:32:13.000Z</updated>
        <summary type="html"><![CDATA[New!
  


We are excited to announce our new partnership with a popular booking platform among travelers: HomeToGo!
  

💡What is HomeToGo?
HomeToGo is a German group founded in 2014 that has become a leader in the vacation rental industry. 
Thanks to its exponential growth, HomeToGo is now an essential booking platform for property managers worldwide. 🌍



💡 Why pushing your listings on HomeToGo?

Quality bookings: 7 days of average Length of Stay and a superior average transaction value.
Access to a broad audience: Particularly from the Germany-Switzerland-Austria region, along with other Northern European countries, guests with high purchasing power.
Flexibility: Hosts can apply their own cancellation policies and terms and conditions.
Direct communication channels with guests: Both during and post-stay.
Enhanced visibility: Through HomeToGo's extensive network of brands.
 
👉 Install the HomeToGo app here
 
💌 Learn more on how to connect your properties in our dedicated manual page.
  
If you have any questions, please don't hesitate to contact channel.manager@hometogo.com]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/my7r3l9lsqnk</id>
        <link href="https://www.githubstatus.com/incidents/my7r3l9lsqnk"/>
        <updated>2024-02-09T11:28:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  9, 11:28 UTC
Resolved - On February 9, 2024 between 10:34 UTC and 11:24 UTC, the Webhooks service was degraded and 63% of webhooks were delayed by up to 16 minutes with an average delay of 5 minutes. No webhook deliveries were lost. This was due to an issue with an overloaded backend data store that was unable to process network requests fast enough.
We mitigated the incident by manually failing over traffic to healthy hosts.
We are expanding the capacity of the backing store as well as making the Webhooks service more resilient to this kind of issue. 

Feb  9, 11:25 UTC
Update - Webhooks is operating normally.
Feb  9, 11:11 UTC
Update - We are investigating latency in processing webhooks. Customers may see a delay of around 5 minutes at this time. We will continue to keep users updated on progress towards mitigation.
Feb  9, 11:09 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage on eu2.make.com]]></title>
        <id>https://status.make.com/incidents/ldktvddm239d</id>
        <link href="https://status.make.com/incidents/ldktvddm239d"/>
        <updated>2024-02-08T16:03:02.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 17:03 CET
Resolved - This incident has been resolved.
Feb  8, 14:41 CET
Update - We are continuing to monitor for any further issues.
Feb  8, 14:39 CET
Monitoring - We have identified and resolved the issue associated with the recent code change. Stability has been restored to eu2.make.com since 14:20 CET. We will continue to monitor the situation for the next several hours.
Feb  8, 14:21 CET
Investigating - We are currently experiencing issues with the eu2.make.com zone. Login to the webpage is unreachable, and scenario executions are affected. We are actively investigating the issue and will provide an update within the next 60 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unresponsive shared hooks]]></title>
        <id>https://status.make.com/incidents/wtq70l2b2g74</id>
        <link href="https://status.make.com/incidents/wtq70l2b2g74"/>
        <updated>2024-02-08T12:52:07.000Z</updated>
        <summary type="html"><![CDATA[Feb  8, 13:52 CET
Resolved - This incident has been resolved.
Feb  8, 10:04 CET
Update - The fix has been successfully rolled out. Webhooks and mailhooks should now be fully functional for eu1.make.com. We will continue to monitor the situation for the next few hours.
Feb  8, 09:33 CET
Update - While monitoring the issue, we observed certain problems with mailhooks and webhooks. We have identified the issue and are working on rolling out the fix. The issue pertains only to eu1.make.com, the remaining zones are functional.
Feb  8, 01:19 CET
Monitoring - A fix has been implemented and we are monitoring the current behavior.
Feb  8, 00:45 CET
Investigating - We are currently experiencing issues with some of our services responsible for shared hooks on our clusters.
Shared hooks might currently be unresponsive. We are actively investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be having trouble connecting to Slack.]]></title>
        <id>https://status.slack.com//2024-02/558e3bb8ce654659</id>
        <link href="https://status.slack.com//2024-02/558e3bb8ce654659"/>
        <updated>2024-02-08T02:06:20.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On February 7, 2024 from around 9:23 AM PST to 9:51 AM PST, a small percentage of users may have experienced issues connecting to Slack or searching within Slack. 


Our automated alerting systems detected an unusual spike in traffic to one of our servers. We carried out a thorough investigation, but the load subsided naturally as we worked, resolving the issue for all affected users.


While we did not take any remedial action in this case, we continued our investigation to better understand the problem and potential mitigation strategies for similar issues in future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Resize Events]]></title>
        <id>https://status.digitalocean.com/incidents/w1sspyd728kg</id>
        <link href="https://status.digitalocean.com/incidents/w1sspyd728kg"/>
        <updated>2024-02-07T20:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 20:54 UTC
Resolved - As of 19:37 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Droplet resize events in all regions. All the Droplet resize events should now be succeeding normally. 
If you continue to experience problems, please open a ticket with our Support team. 
Thank you for your patience and we apologize for the inconvenience.
Feb  7, 19:41 UTC
Monitoring - Our Engineering team has fully deployed the fix for the issue with Droplet resizes and is now monitoring the situation. Users can now retry Droplet resizes and should see them succeed.
We'll post another update once we confirm the fix resolves this incident.
Feb  7, 15:49 UTC
Identified - Our Engineering team has identified the root cause of the issue with failed Droplet resizes and a fix is in the process of being deployed. 
Users attempting to resize Droplets where the image for the Droplet has been deleted or retired (e.g. a user created a Droplet from a Snapshot, but later deleted that Snapshot) will see failures. All other resizes are succeeding normally.
We'll post another update once the fix has completed deployment.
Feb  7, 15:32 UTC
Investigating - Our Engineering team is investigating an uptick in failed Droplet resizes, beginning Feb 6, 20:57 UTC. 
During this time, some users may experience failures when attempting to resize Droplets, in all regions. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Async Request Processing]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/bs3vrsstwjvz"/>
        <updated>2024-02-07T17:48:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 09:48 PST
Resolved - This incident has been resolved.
Feb  7, 08:55 PST
Monitoring - Our Async Request Processing system had an issue this morning, which resulted in increased delays between the time a request was enqueued and when it was processed, and may have resulted in an elevated processing failure rate. This began around 8:10 AM PST, and was resolved by 9:15 AM PST.
We are still actively monitoring, but are not expecting any ongoing issues at this time.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in NYC Regions]]></title>
        <id>https://status.digitalocean.com/incidents/y2cm9wm1bvv7</id>
        <link href="https://status.digitalocean.com/incidents/y2cm9wm1bvv7"/>
        <updated>2024-02-07T07:21:30.000Z</updated>
        <summary type="html"><![CDATA[Feb  7, 07:21 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting network latency in our NYC regions.
The issues were a direct result of traffic congestion from our upstream providers, which has been repaired. Users should no longer experience packet loss or increased latency while interacting with their resources in the NYC regions.
We sincerely apologize and thank you for your patience as we worked through this issue. In case of any questions or concerns, please open a ticket with our Support team.
Feb  7, 04:03 UTC
Investigating - Our Engineering team is investigating multiple reports of network latency when connecting to services in our NYC regions. During this time, users may experience intermittent packet loss or increased latency while interacting with their resources in the NYC regions.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/1jwy0vy3hjfb</id>
        <link href="https://status.digitalocean.com/incidents/1jwy0vy3hjfb"/>
        <updated>2024-02-06T22:21:17.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 22:21 UTC
Completed - The scheduled maintenance has been completed.
Feb  6, 17:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Feb  2, 20:23 UTC
Scheduled - Start Time: 17:00 UTC Feb 6, 2024
End Time: 00:00 UTC Feb 7, 2024
During the above time, our Engineering Team will be performing maintenance to failover some internal databases from one cluster to another.
Extensive testing has been conducted to ensure this maintenance will be successful and result in minimal impact to DigitalOcean users. The actual failover is estimated to take less than 3 seconds.
Existing infrastructure, including Droplets and Droplet-based services, should continue running without issue. There is no network disruption to existing services expected as part of this maintenance. However, there are dependencies on multiple services. During the failover, there may be customer impacts that should be brief and transitory. The following actions may experience increased latency or failure rates during the maintenance period:
- API calls to the DigitalOcean public API 
- Events for Droplets and Droplet-based services such as create, delete, power on/off, resize, etc 
- Control operations through the DigitalOcean Cloud Control Panel 
Multiple teams will be engaged to keep downtime to a minimum and mitigate any impact that does occur. We’ll post updates here for any unexpected changes to this scheduled maintenance, as well as progress updates during the maintenance itself.
If you have any questions or concerns, please reach out to the Support team from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automations failing]]></title>
        <id>https://status.notion.so/incidents/2wtslqc6bv54</id>
        <link href="https://status.notion.so/incidents/2wtslqc6bv54"/>
        <updated>2024-02-06T17:58:31.000Z</updated>
        <summary type="html"><![CDATA[Feb  6, 09:58 PST
Resolved - This incident has been resolved.
Feb  6, 09:16 PST
Update - We are working hard to resolve the issue for you. Thank you for your continuous patience.
Feb  6, 07:00 PST
Update - We are still working on fixing the issue and appreciate your patience.
Feb  6, 05:05 PST
Identified - We are experiencing an issue with the Notion's database automations service that cause automation actions to fail or experience delays. Our engineers have identified this & are working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A number of South Korean users inadvertently banned from using vital APIs in Notion, potentially affecting some functionalities within the app.]]></title>
        <id>https://status.notion.so/incidents/f88y9tn65kl1</id>
        <link href="https://status.notion.so/incidents/f88y9tn65kl1"/>
        <updated>2024-02-06T00:32:44.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:32 PST
Resolved - This incident has been resolved.
Feb  5, 16:03 PST
Update - We are continuing to investigate this issue.
Feb  5, 16:02 PST
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/dmybzf132ygz</id>
        <link href="https://airbnbapi.statuspage.io/incidents/dmybzf132ygz"/>
        <updated>2024-02-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:00 PST
Resolved - We've identified spikes in errors across multiple endpoints that have occurred over the past day. The two distinct spikes were:
- February 5th, 4:10 PM to 4:30 PM (PST)
- February 6th, 6:30 AM to 7:30 AM (PST)
The issue is now resolved, and we don't expect any ongoing impact.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automation Failures]]></title>
        <id>https://status.notion.so/incidents/w7ppqptykrdz</id>
        <link href="https://status.notion.so/incidents/w7ppqptykrdz"/>
        <updated>2024-02-05T14:56:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 06:56 PST
Resolved - Between 3:00 UTC - 14:22 UTC, some users may have experienced database automation failures or delays in actions being triggered.
This is now resolved, and the affected automations have completed. Database automation services are now running as normal. 
Thank you for your patience while we worked through this issue.
Feb  5, 05:30 PST
Monitoring - Notion's database automation service began experiencing problems at approximately 3 AM UTC today, which caused a number of automation actions to fail or experience delays. 
Our engineers have identified a fix, and are now retrying automations that failed to trigger during this period. 
We will continue to monitor the situation and share an update when this is fully resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/3k40h28fkb2r</id>
        <link href="https://www.githubstatus.com/incidents/3k40h28fkb2r"/>
        <updated>2024-02-05T09:53:13.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 09:53 UTC
Resolved - On 2024-02-05, from 09:26 to 13:20 UTC some GitHub customers experienced errors when trying to download raw files. An overloaded server exposed a bug, causing us to return HTTP 500 error codes.
The issue was mitigated by disabling the server and re-routing traffic. We are implementing improvements to our routing logic to more quickly avoid troublesome hosts in the future. 

Feb  5, 09:40 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/1q1jhbr85zvv</id>
        <link href="https://status.digitalocean.com/incidents/1q1jhbr85zvv"/>
        <updated>2024-02-05T05:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 05:54 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting Spaces CDN in our SGP1 region.
From 03:02 UTC - 05:15 UTC, users were experiencing errors for objects served over the CDN.
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.
Feb  5, 05:10 UTC
Monitoring - Our Engineering team has applied a fix to mitigate the issue related to the Spaces CDN in the SGP1 region. Users should no longer experience errors for objects served over the CDN. 
We apologize for the inconvenience and will post another update once we're confident that the issue is fully resolved.
Feb  5, 04:52 UTC
Identified - From 03:02 UTC, our Engineering team has identified an issue with the Spaces CDN in our SGP1 region and is actively working on a fix. During this time, users may experience errors for objects served over the CDN. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joining or creating a workspace from sidebar is broken]]></title>
        <id>https://status.notion.so/incidents/q9j3jvg634lx</id>
        <link href="https://status.notion.so/incidents/q9j3jvg634lx"/>
        <updated>2024-02-02T22:45:22.000Z</updated>
        <summary type="html"><![CDATA[Feb  2, 14:45 PST
Resolved - As of 2:44 PM PT, this incident has been resolved.
Feb  2, 11:52 PST
Monitoring - As of 11:44 AM PT, a fix has been implemented and we are monitoring the results.
Feb  2, 11:42 PST
Update - We are continuing to investigate this issue.
Feb  2, 11:42 PST
Update - Users cannot join and create a workspace from their sidebar. In addition, users may experience increased latency across search, viewing and editing content, and syncing content.
We are actively investigating these issues and will follow up here with an update.
Feb  2, 11:36 PST
Investigating - Currently, the "Join or create a workspace" button from the workspace sidebar is broken.
We are actively investigating the issue and will follow up here with an update.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users in Germany having trouble receiving 2FA SMS codes]]></title>
        <id>https://status.slack.com//2024-02/30dfe547672802a4</id>
        <link href="https://status.slack.com//2024-02/30dfe547672802a4"/>
        <updated>2024-02-01T14:43:18.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On February 1, 2024 between 12:57 AM PST and 3:52 AM PST, some users in Germany were having trouble receiving two-factor authentication codes via SMS.


This was caused by an issue with a service provider and has now been resolved.


Users in Germany should no longer be having trouble and may also choose to use an authentication app to receive their 2FA codes instead of SMS.


Thank you for being patient with us, we appreciate it.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/7k5wl5j4f1t4</id>
        <link href="https://www.githubstatus.com/incidents/7k5wl5j4f1t4"/>
        <updated>2024-02-01T04:41:36.000Z</updated>
        <summary type="html"><![CDATA[Feb  1, 04:41 UTC
Resolved - An update to our design system caused issues loading dynamic content in the global side navigation menu and in other page-specific sidebar navigation elements. Impacted users saw continuous loading spinners in place of dynamic menu content. User impact lasted from 0:55 UTC to 4:41 UTC on February 1st.
We are working on a number of improvements in response to this incident. We are adding request volume monitors to sidebar navigation endpoints and making changes to our front end escalation paths to improve our time to detect and time to recovery for incidents of this nature. We have also begun work to improve both automated and manual testing for these types of changes in order to prevent recurrence.
Feb  1, 04:41 UTC
Update - This issue has been resolved. A reload of your browser window/tab may be required if you continue to experience issues with the collapsable navigation sidebars not loading.
Feb  1, 04:21 UTC
Update - We are in the process of deploying a remediation, and expect to see restoration of impacted functionality within the next hour.
Feb  1, 03:55 UTC
Update - We have identified an issue that is preventing some navigation components from loading while browsing GitHub.com, and are testing a remediation prior to deployment.
Feb  1, 03:14 UTC
Update - We are currently investigating reports of some components of the GitHub.com website not loading for some users.
Feb  1, 03:13 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Search API endpoint is down]]></title>
        <id>https://status.notion.so/incidents/t93zz4ynglj2</id>
        <link href="https://status.notion.so/incidents/t93zz4ynglj2"/>
        <updated>2024-01-31T23:01:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:01 PST
Resolved - This incident has been resolved.
Jan 31, 15:00 PST
Update - We are continuing to work on a fix for this issue.
Jan 31, 13:41 PST
Identified - We identified the root cause and are preparing a hotfix.
Jan 31, 11:05 PST
Update - We are continuing to investigate this issue.
Jan 31, 11:05 PST
Investigating - As of 12:35 AM PT the /search endpoint (https://api.notion.com/v1/search) has been down. 
We are currently investigating the issue and will share an update once the issue has been identified.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues loading threads and sending mesages.]]></title>
        <id>https://status.slack.com//2024-01/8e9a7cb95549d34c</id>
        <link href="https://status.slack.com//2024-01/8e9a7cb95549d34c"/>
        <updated>2024-01-31T18:14:04.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On January 31, 2024 between 7:43 AM PST and 8:27 AM PST, some users were unable to load threads and send messages.


We traced the issue to a backend failure and immediately implemented a change which fixed the issue for all affected users.


We apologize for any disruption to your work day and appreciate your patience while we resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customer Support Ticket Portal]]></title>
        <id>https://status.digitalocean.com/incidents/swc8grzsb33n</id>
        <link href="https://status.digitalocean.com/incidents/swc8grzsb33n"/>
        <updated>2024-01-31T15:26:00.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:26 UTC
Resolved - Our team has confirmed the full resolution for the problem with our support portal at https://cloudsupport.digitalocean.com/s/ where customers were unable to create tickets with 'Billing' ticket type. 
We sincerely apologize and thank you for your patience as we worked through this issue. 
In case of any questions or concerns, please open a ticket with our Support team.
Jan 31, 15:12 UTC
Monitoring - Our Engineering team has identified the cause of the issue and implemented a fix to resolve the problem with the Support Portal. Users should now be able to create the tickets in the Support portal with Billing ticket type. 
We are monitoring the situation now and will post an update as soon as the issue is fully resolved.
Jan 31, 14:26 UTC
Investigating - Our Engineering team is investigating an issue with customers being unable to create the support tickets to our support portal for Ticket type "Billing" at https://cloudsupport.digitalocean.com. 
As a temporary workaround, users may still contact us via the form here: https://www.digitalocean.com/company/contact/support
We apologize for the inconvenience and will post an update as soon as further information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/5y8b8lsqbbyq</id>
        <link href="https://www.githubstatus.com/incidents/5y8b8lsqbbyq"/>
        <updated>2024-01-31T14:57:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 14:57 UTC
Resolved - This incident was the result of an infrastructure change that was made to our load balancers to prepare us for IPv6 enablement of GitHub.com. This change was deployed to a subset of our global edge sites.
The change had the unintended consequence of causing IPv4 addresses to start being passed as an IPv4-mapped IPv6-compatible address to our IP Allow List functionality.
For example 10.1.2.3 became ::ffff:10.1.2.3. While our IP Allow List functionality was developed with IPv6 in mind, it wasn't developed to handle these mapped addresses, and hence started blocking requests as it deemed these to be not in the defined list of allowed addresses. Request error rates peaked at 0.23% of all requests.
We have so far identified three remediation items here:
- Update the IP Allow List functionality to handle IPv4-mapped addresses.
- Audit the rest of our stack to confirm there are no further places this IPv4-mapped IPv6 addresses flaw exists.
- Improve our testing and monitoring processes to better catch these issues in the future.
Jan 31, 14:56 UTC
Update - We have resolved the issue and confirmed all regions are now operating as expected.
Jan 31, 14:49 UTC
Update - The fix for ip allow lists is currently rolling out; and we are awaiting confirmation from specific geographic regions.
Jan 31, 14:33 UTC
Update - We are rolling out a fix to resolve the issues with IP allow lists. This should be resolved shortly.
Jan 31, 14:14 UTC
Update - Some customers are experiencing issues with IP allow lists.
Jan 31, 14:14 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some latency with Slack]]></title>
        <id>https://status.slack.com//2024-01/0a000b0ad09623a0</id>
        <link href="https://status.slack.com//2024-01/0a000b0ad09623a0"/>
        <updated>2024-01-30T17:58:00.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:37 AM PST to 11:48 AM PST on January 24, 2024, we experienced an unexpected amount of API calls to our servers that occurred within a short window of time. The volume of API calls resulted in some users experiencing latency loading channels and general difficulties connecting to Slack. We investigated the impact with a broad lens and began to observe signs of recovery around 12:09 PM PST.


During this time, we cautiously observed our health metrics to ensure our servers were accurately recovering. As a result of our thorough monitoring, there was a delay before we could confirm the issue was fully resolved. Our teams have also put in measures to help reduce the likelihood of this occurring again in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots Page - Cloud Control Panel]]></title>
        <id>https://status.digitalocean.com/incidents/39r97dl3v2l0</id>
        <link href="https://status.digitalocean.com/incidents/39r97dl3v2l0"/>
        <updated>2024-01-30T16:42:28.000Z</updated>
        <summary type="html"><![CDATA[Jan 30, 16:42 UTC
Resolved - Our Engineering team identified and resolved an issue impacting the Snapshots page in our Cloud Control Panel. 
From 13:00 - 15:00 UTC, users attempting to navigate to https://cloud.digitalocean.com/images/snapshots (via Images -> Snapshots) were unable to access the page, and instead saw an error page returned. 
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Pasting via Cmd/Ctrl + V not working]]></title>
        <id>https://status.slack.com//2024-01/8ad463b92a084387</id>
        <link href="https://status.slack.com//2024-01/8ad463b92a084387"/>
        <updated>2024-01-30T03:44:32.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

From 11:45 AM PST to around 5:20 PM PST on January 29, 2024, some customers experienced problems using keyboard shortcuts to paste text into Slack. 


A code change inadvertently introduced an issue that prevented the use of Cmd/Ctrl + V to paste text into Slack. We reverted this change, then deployed a fix to fully resolve the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2024-002]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2024-002</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2024-002"/>
        <updated>2024-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: We resolved an information disclosure vulnerability in the
hello.ts.net service.
What happened?
On January 15 2024, we became aware of a potential information disclosure
vulnerability in the hello.ts.net service, which could show the identity of a
different Tailscale user when loaded. The hello.ts.net service receives
identity information and public keys of nodes tied to their IP address. On
November 28 2023, we made a change to how IPs are assigned to
Tailscale nodes, making them globally non-unique. When the Tailscale service
assigned the same IP to multiple nodes, hello.ts.net would receive identity
information for one of the nodes at random. We confirmed on January 26 2024
that, if one of the other nodes with that IP loaded hello.ts.net, they would
see another user's name, email, and hostname.
The Tailscale Security Team immediately took hello.ts.net offline while the
fix was in progress. The issue has been fixed and the hello.ts.net service
was restored on January 29 2024.
Who was affected?
The incident was isolated to 10 users across 9 tailnets who could have had
their information leaked to other Tailscale users. We notified the tailnet
security contacts directly in accordance with our obligations under applicable
data privacy laws. Due to the random nature of the vulnerability, we cannot
confirm that all of those users were indeed affected.
Regular shared nodes always see unique node IPs and were not
vulnerable in a manner similar to hello.ts.net.
What was the impact?
A small number of users had their name, email, and hostname potentially exposed
to other Tailscale users that had nodes sharing the same IP.
In addition, the hello.ts.net service was offline between January 26-29
2024. Several users reported being negatively impacted by this.
What do I need to do?
No action is needed at this time.
If you have a dependency on hello.ts.net as a probing target for Tailscale
connectivity, consider using a different probing
mechanism.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNS Resolution in FRA1, AMS3 and LON1 Regions]]></title>
        <id>https://status.digitalocean.com/incidents/r9w0yrbyy9ls</id>
        <link href="https://status.digitalocean.com/incidents/r9w0yrbyy9ls"/>
        <updated>2024-01-29T18:36:45.000Z</updated>
        <summary type="html"><![CDATA[Jan 29, 18:36 UTC
Resolved - Our Engineering team has confirmed the workaround fix is successful and all services should now be operating normally. We will now close this incident and work with the DNS provider separately on the root cause. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Jan 29, 18:18 UTC
Monitoring - Our Engineering team has identified the root cause of the issue with DNS resolution. DigitalOcean resolvers in use in FRA1, AMS3, and LON1 are unable to reach an upstream DNS provider, resulting in resolution for a subset of domain names being unavailable from our resolvers. Our Engineering team is reaching out to the provider for assistance.
In the meantime, our Engineering team has been able to implement a workaround fix by filtering some incorrectly announced network routes. At this time, we are seeing recovery and resolution of hostnames returning to normal in the impacted regions. We'll continue to await an update from the DNS provider. We're now monitoring the workaround fix for stability and will post an update once we are confident it is successful.
Jan 29, 17:27 UTC
Investigating - Our Engineering team is currently investigating issues with DNS resolution in FRA1, AMS3, and LON1. During this time, customers may experience errors trying to resolve domain names from within DigitalOcean services in those regions, including Droplets and Droplet-based services, as well as App Platform. Additionally, App Platform builds may fail or experience delays. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/g6drnqm54qd4</id>
        <link href="https://www.githubstatus.com/incidents/g6drnqm54qd4"/>
        <updated>2024-01-28T14:42:55.000Z</updated>
        <summary type="html"><![CDATA[Jan 28, 14:42 UTC
Resolved - On January 28, 2024, between 01:00 UTC and 14:00 UTC the Avatars service was degraded and could not return all avatar images requested by users, instead it would return a default, fallback avatar image. This incident impacted, at peak time 6% of the requests for viewing Avatars. Requests that were impacted did not prevent the users from continuing to use any GitHub services. This was due to an issue with the Avatars service connecting to a database host.
 We mitigated the incident by restarting the malfunctioning hosts that were not able to return the user avatar images.
 We are working to improve alerting and monitoring of our services to reduce our time to detection and mitigation.
Jan 28, 14:27 UTC
Update - We have mitigated all customer impact. We are no longer serving fallback avatar icons when loading web pages for some customers. We continue to monitor the results.
Jan 28, 13:57 UTC
Update - A fix has been implemented for customers seeing the default avatar (octocat) when loading web pages and we are monitoring the results.
Jan 28, 13:20 UTC
Update - Some requests for getting the Avatars are returning the fallback response instead of the asked avatar since they are having issues connecting to the Mysql host
Jan 28, 13:20 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>