<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2024-02-06T00:21:07.889Z</id>
    <title>osmos::feed</title>
    <updated>2024-02-06T00:21:07.889Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[A number of South Korean users inadvertently banned from using vital APIs in Notion, potentially affecting some functionalities within the app.]]></title>
        <id>https://status.notion.so/incidents/f88y9tn65kl1</id>
        <link href="https://status.notion.so/incidents/f88y9tn65kl1"/>
        <updated>2024-02-06T00:03:08.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 16:03 PST
Update - We are continuing to investigate this issue.
Feb  5, 16:02 PST
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Automation Failures]]></title>
        <id>https://status.notion.so/incidents/w7ppqptykrdz</id>
        <link href="https://status.notion.so/incidents/w7ppqptykrdz"/>
        <updated>2024-02-05T14:56:59.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 06:56 PST
Resolved - Between 3:00 UTC - 14:22 UTC, some users may have experienced database automation failures or delays in actions being triggered.
This is now resolved, and the affected automations have completed. Database automation services are now running as normal. 
Thank you for your patience while we worked through this issue.
Feb  5, 05:30 PST
Monitoring - Notion's database automation service began experiencing problems at approximately 3 AM UTC today, which caused a number of automation actions to fail or experience delays. 
Our engineers have identified a fix, and are now retrying automations that failed to trigger during this period. 
We will continue to monitor the situation and share an update when this is fully resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/3k40h28fkb2r</id>
        <link href="https://www.githubstatus.com/incidents/3k40h28fkb2r"/>
        <updated>2024-02-05T09:53:13.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 09:53 UTC
Resolved - This incident has been resolved.
Feb  5, 09:40 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/1q1jhbr85zvv</id>
        <link href="https://status.digitalocean.com/incidents/1q1jhbr85zvv"/>
        <updated>2024-02-05T05:54:58.000Z</updated>
        <summary type="html"><![CDATA[Feb  5, 05:54 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue impacting Spaces CDN in our SGP1 region.
From 03:02 UTC - 05:15 UTC, users were experiencing errors for objects served over the CDN.
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.
Feb  5, 05:10 UTC
Monitoring - Our Engineering team has applied a fix to mitigate the issue related to the Spaces CDN in the SGP1 region. Users should no longer experience errors for objects served over the CDN. 
We apologize for the inconvenience and will post another update once we're confident that the issue is fully resolved.
Feb  5, 04:52 UTC
Identified - From 03:02 UTC, our Engineering team has identified an issue with the Spaces CDN in our SGP1 region and is actively working on a fix. During this time, users may experience errors for objects served over the CDN. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joining or creating a workspace from sidebar is broken]]></title>
        <id>https://status.notion.so/incidents/q9j3jvg634lx</id>
        <link href="https://status.notion.so/incidents/q9j3jvg634lx"/>
        <updated>2024-02-02T22:45:22.000Z</updated>
        <summary type="html"><![CDATA[Feb  2, 14:45 PST
Resolved - As of 2:44 PM PT, this incident has been resolved.
Feb  2, 11:52 PST
Monitoring - As of 11:44 AM PT, a fix has been implemented and we are monitoring the results.
Feb  2, 11:42 PST
Update - We are continuing to investigate this issue.
Feb  2, 11:42 PST
Update - Users cannot join and create a workspace from their sidebar. In addition, users may experience increased latency across search, viewing and editing content, and syncing content.
We are actively investigating these issues and will follow up here with an update.
Feb  2, 11:36 PST
Investigating - Currently, the "Join or create a workspace" button from the workspace sidebar is broken.
We are actively investigating the issue and will follow up here with an update.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users in Germany having trouble receiving 2FA SMS codes]]></title>
        <id>https://status.slack.com//2024-02/30dfe547672802a4</id>
        <link href="https://status.slack.com//2024-02/30dfe547672802a4"/>
        <updated>2024-02-01T14:43:18.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On February 1, 2024 between 12:57 AM PST and 3:52 AM PST, some users in Germany were having trouble receiving two-factor authentication codes via SMS.


This was caused by an issue with a service provider and has now been resolved.


Users in Germany should no longer be having trouble and may also choose to use an authentication app to receive their 2FA codes instead of SMS.


Thank you for being patient with us, we appreciate it.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/7k5wl5j4f1t4</id>
        <link href="https://www.githubstatus.com/incidents/7k5wl5j4f1t4"/>
        <updated>2024-02-01T04:41:36.000Z</updated>
        <summary type="html"><![CDATA[Feb  1, 04:41 UTC
Resolved - This incident has been resolved.
Feb  1, 04:41 UTC
Update - This issue has been resolved. A reload of your browser window/tab may be required if you continue to experience issues with the collapsable navigation sidebars not loading.
Feb  1, 04:21 UTC
Update - We are in the process of deploying a remediation, and expect to see restoration of impacted functionality within the next hour.
Feb  1, 03:55 UTC
Update - We have identified an issue that is preventing some navigation components from loading while browsing GitHub.com, and are testing a remediation prior to deployment.
Feb  1, 03:14 UTC
Update - We are currently investigating reports of some components of the GitHub.com website not loading for some users.
Feb  1, 03:13 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Search API endpoint is down]]></title>
        <id>https://status.notion.so/incidents/t93zz4ynglj2</id>
        <link href="https://status.notion.so/incidents/t93zz4ynglj2"/>
        <updated>2024-01-31T23:01:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:01 PST
Resolved - This incident has been resolved.
Jan 31, 15:00 PST
Update - We are continuing to work on a fix for this issue.
Jan 31, 13:41 PST
Identified - We identified the root cause and are preparing a hotfix.
Jan 31, 11:05 PST
Update - We are continuing to investigate this issue.
Jan 31, 11:05 PST
Investigating - As of 12:35 AM PT the /search endpoint (https://api.notion.com/v1/search) has been down. 
We are currently investigating the issue and will share an update once the issue has been identified.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues loading threads and sending mesages.]]></title>
        <id>https://status.slack.com//2024-01/8e9a7cb95549d34c</id>
        <link href="https://status.slack.com//2024-01/8e9a7cb95549d34c"/>
        <updated>2024-01-31T18:14:04.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On January 31, 2024 between 7:43 AM PST and 8:27 AM PST, some users were unable to load threads and send messages.


We traced the issue to a backend failure and immediately implemented a change which fixed the issue for all affected users.


We apologize for any disruption to your work day and appreciate your patience while we resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customer Support Ticket Portal]]></title>
        <id>https://status.digitalocean.com/incidents/swc8grzsb33n</id>
        <link href="https://status.digitalocean.com/incidents/swc8grzsb33n"/>
        <updated>2024-01-31T15:26:00.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 15:26 UTC
Resolved - Our team has confirmed the full resolution for the problem with our support portal at https://cloudsupport.digitalocean.com/s/ where customers were unable to create tickets with 'Billing' ticket type. 
We sincerely apologize and thank you for your patience as we worked through this issue. 
In case of any questions or concerns, please open a ticket with our Support team.
Jan 31, 15:12 UTC
Monitoring - Our Engineering team has identified the cause of the issue and implemented a fix to resolve the problem with the Support Portal. Users should now be able to create the tickets in the Support portal with Billing ticket type. 
We are monitoring the situation now and will post an update as soon as the issue is fully resolved.
Jan 31, 14:26 UTC
Investigating - Our Engineering team is investigating an issue with customers being unable to create the support tickets to our support portal for Ticket type "Billing" at https://cloudsupport.digitalocean.com. 
As a temporary workaround, users may still contact us via the form here: https://www.digitalocean.com/company/contact/support
We apologize for the inconvenience and will post an update as soon as further information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/5y8b8lsqbbyq</id>
        <link href="https://www.githubstatus.com/incidents/5y8b8lsqbbyq"/>
        <updated>2024-01-31T14:57:16.000Z</updated>
        <summary type="html"><![CDATA[Jan 31, 14:57 UTC
Resolved - This incident was the result of an infrastructure change that was made to our load balancers to prepare us for IPv6 enablement of GitHub.com. This change was deployed to a subset of our global edge sites.
The change had the unintended consequence of causing IPv4 addresses to start being passed as an IPv4-mapped IPv6-compatible address to our IP Allow List functionality.
For example 10.1.2.3 became ::ffff:10.1.2.3. While our IP Allow List functionality was developed with IPv6 in mind, it wasn't developed to handle these mapped addresses, and hence started blocking requests as it deemed these to be not in the defined list of allowed addresses. Request error rates peaked at 0.23% of all requests.
We have so far identified three remediation items here:
- Update the IP Allow List functionality to handle IPv4-mapped addresses.
- Audit the rest of our stack to confirm there are no further places this IPv4-mapped IPv6 addresses flaw exists.
- Improve our testing and monitoring processes to better catch these issues in the future.
Jan 31, 14:56 UTC
Update - We have resolved the issue and confirmed all regions are now operating as expected.
Jan 31, 14:49 UTC
Update - The fix for ip allow lists is currently rolling out; and we are awaiting confirmation from specific geographic regions.
Jan 31, 14:33 UTC
Update - We are rolling out a fix to resolve the issues with IP allow lists. This should be resolved shortly.
Jan 31, 14:14 UTC
Update - Some customers are experiencing issues with IP allow lists.
Jan 31, 14:14 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some latency with Slack]]></title>
        <id>https://status.slack.com//2024-01/0a000b0ad09623a0</id>
        <link href="https://status.slack.com//2024-01/0a000b0ad09623a0"/>
        <updated>2024-01-30T17:58:00.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:37 AM PST to 11:48 AM PST on January 24, 2024, we experienced an unexpected amount of API calls to our servers that occurred within a short window of time. The volume of API calls resulted in some users experiencing latency loading channels and general difficulties connecting to Slack. We investigated the impact with a broad lens and began to observe signs of recovery around 12:09 PM PST.


During this time, we cautiously observed our health metrics to ensure our servers were accurately recovering. As a result of our thorough monitoring, there was a delay before we could confirm the issue was fully resolved. Our teams have also put in measures to help reduce the likelihood of this occurring again in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots Page - Cloud Control Panel]]></title>
        <id>https://status.digitalocean.com/incidents/39r97dl3v2l0</id>
        <link href="https://status.digitalocean.com/incidents/39r97dl3v2l0"/>
        <updated>2024-01-30T16:42:28.000Z</updated>
        <summary type="html"><![CDATA[Jan 30, 16:42 UTC
Resolved - Our Engineering team identified and resolved an issue impacting the Snapshots page in our Cloud Control Panel. 
From 13:00 - 15:00 UTC, users attempting to navigate to https://cloud.digitalocean.com/images/snapshots (via Images -> Snapshots) were unable to access the page, and instead saw an error page returned. 
We apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Pasting via Cmd/Ctrl + V not working]]></title>
        <id>https://status.slack.com//2024-01/8ad463b92a084387</id>
        <link href="https://status.slack.com//2024-01/8ad463b92a084387"/>
        <updated>2024-01-30T03:44:32.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

From 11:45 AM PST to around 5:20 PM PST on January 29, 2024, some customers experienced problems using keyboard shortcuts to paste text into Slack. 


A code change inadvertently introduced an issue that prevented the use of Cmd/Ctrl + V to paste text into Slack. We reverted this change, then deployed a fix to fully resolve the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2024-002]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2024-002</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2024-002"/>
        <updated>2024-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: We resolved an information disclosure vulnerability in the
hello.ts.net service.
What happened?
On January 15 2024, we became aware of a potential information disclosure
vulnerability in the hello.ts.net service, which could show the identity of a
different Tailscale user when loaded. The hello.ts.net service receives
identity information and public keys of nodes tied to their IP address. On
November 28 2023, we made a change to how IPs are assigned to
Tailscale nodes, making them globally non-unique. When the Tailscale service
assigned the same IP to multiple nodes, hello.ts.net would receive identity
information for one of the nodes at random. We confirmed on January 26 2024
that, if one of the other nodes with that IP loaded hello.ts.net, they would
see another user's name, email, and hostname.
The Tailscale Security Team immediately took hello.ts.net offline while the
fix was in progress. The issue has been fixed and the hello.ts.net service
was restored on January 29 2024.
Who was affected?
The incident was isolated to 10 users across 9 tailnets who could have had
their information leaked to other Tailscale users. We notified the tailnet
security contacts directly in accordance with our obligations under applicable
data privacy laws. Due to the random nature of the vulnerability, we cannot
confirm that all of those users were indeed affected.
Regular shared nodes always see unique node IPs and were not
vulnerable in a manner similar to hello.ts.net.
What was the impact?
A small number of users had their name, email, and hostname potentially exposed
to other Tailscale users that had nodes sharing the same IP.
In addition, the hello.ts.net service was offline between January 26-29
2024. Several users reported being negatively impacted by this.
What do I need to do?
No action is needed at this time.
If you have a dependency on hello.ts.net as a probing target for Tailscale
connectivity, consider using a different probing
mechanism.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNS Resolution in FRA1, AMS3 and LON1 Regions]]></title>
        <id>https://status.digitalocean.com/incidents/r9w0yrbyy9ls</id>
        <link href="https://status.digitalocean.com/incidents/r9w0yrbyy9ls"/>
        <updated>2024-01-29T18:36:45.000Z</updated>
        <summary type="html"><![CDATA[Jan 29, 18:36 UTC
Resolved - Our Engineering team has confirmed the workaround fix is successful and all services should now be operating normally. We will now close this incident and work with the DNS provider separately on the root cause. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Jan 29, 18:18 UTC
Monitoring - Our Engineering team has identified the root cause of the issue with DNS resolution. DigitalOcean resolvers in use in FRA1, AMS3, and LON1 are unable to reach an upstream DNS provider, resulting in resolution for a subset of domain names being unavailable from our resolvers. Our Engineering team is reaching out to the provider for assistance.
In the meantime, our Engineering team has been able to implement a workaround fix by filtering some incorrectly announced network routes. At this time, we are seeing recovery and resolution of hostnames returning to normal in the impacted regions. We'll continue to await an update from the DNS provider. We're now monitoring the workaround fix for stability and will post an update once we are confident it is successful.
Jan 29, 17:27 UTC
Investigating - Our Engineering team is currently investigating issues with DNS resolution in FRA1, AMS3, and LON1. During this time, customers may experience errors trying to resolve domain names from within DigitalOcean services in those regions, including Droplets and Droplet-based services, as well as App Platform. Additionally, App Platform builds may fail or experience delays. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/g6drnqm54qd4</id>
        <link href="https://www.githubstatus.com/incidents/g6drnqm54qd4"/>
        <updated>2024-01-28T14:42:55.000Z</updated>
        <summary type="html"><![CDATA[Jan 28, 14:42 UTC
Resolved - On January 28, 2024, between 01:00 UTC and 14:00 UTC the Avatars service was degraded and could not return all avatar images requested by users, instead it would return a default, fallback avatar image. This incident impacted, at peak time 6% of the requests for viewing Avatars. Requests that were impacted did not prevent the users from continuing to use any GitHub services. This was due to an issue with the Avatars service connecting to a database host.
 We mitigated the incident by restarting the malfunctioning hosts that were not able to return the user avatar images.
 We are working to improve alerting and monitoring of our services to reduce our time to detection and mitigation.
Jan 28, 14:27 UTC
Update - We have mitigated all customer impact. We are no longer serving fallback avatar icons when loading web pages for some customers. We continue to monitor the results.
Jan 28, 13:57 UTC
Update - A fix has been implemented for customers seeing the default avatar (octocat) when loading web pages and we are monitoring the results.
Jan 28, 13:20 UTC
Update - Some requests for getting the Avatars are returning the fallback response instead of the asked avatar since they are having issues connecting to the Mysql host
Jan 28, 13:20 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are unable to upload, download, and view files in Slack.]]></title>
        <id>https://status.slack.com//2024-01/f39851209d6c471a</id>
        <link href="https://status.slack.com//2024-01/f39851209d6c471a"/>
        <updated>2024-01-27T00:22:52.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 1:00pm PST on January 23, 2024 to 2:30pm PST on January 23, 2024, some users encountered trouble uploading, downloading, and viewing files in Slack.


We determined that an API call was not functioning correctly, and made a change to mitigate the issue. The upload problems were caused by a slowing down of responses due to a higher than normal volume of API calls due to the malfunction. This issue has been resolved and steps have been taken to avoid it happening in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Service wide connection issues]]></title>
        <id>https://status.slack.com//2024-01/8b623c0b5640c28f</id>
        <link href="https://status.slack.com//2024-01/8b623c0b5640c28f"/>
        <updated>2024-01-26T14:18:08.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On January 24, 2024 from 5:40 AM PST to around 9:00 AM PST, a small number of users experienced issues connecting to Slack and running workflows.


A change to routing in our servers resulted in requests failing due to a lack of available resources. This change was rolled back and Slack returned to a normal state.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Snapshots are failing in SFO3 and NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/2yxmsbx1r89b</id>
        <link href="https://status.digitalocean.com/incidents/2yxmsbx1r89b"/>
        <updated>2024-01-25T02:56:12.000Z</updated>
        <summary type="html"><![CDATA[Jan 25, 02:56 UTC
Resolved - Our Engineering team has resolved the issue with snapshots taken by customers in the NYC3 and SFO3 regions. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Jan 25, 01:50 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with snapshots taken by customers in the NYC3 and SFO3 regions and are monitoring the situation closely. 
We will post another update once we're confident that the issue is fully resolved.
Jan 25, 00:26 UTC
Identified - Our Engineering team has identified an issue with snapshots taken by customers in the NYC3 and SFO3 regions and is actively working on a fix. We will post an update as soon as additional information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes Cluster in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/s4kt15q6xq19</id>
        <link href="https://status.digitalocean.com/incidents/s4kt15q6xq19"/>
        <updated>2024-01-23T22:50:40.000Z</updated>
        <summary type="html"><![CDATA[Jan 23, 22:50 UTC
Resolved - Our Engineering team has completed mitigation efforts for the issue impacting Managed Kubernetes in the FRA1 region and we are marking this incident as Resolved. 
At this time, functionality to impacted clusters has been restored but customers may need to reconfigure some Kubernetes resources. Customer Support is contacting impacted customers directly with further instructions. 
If you have any questions or concerns regarding this incident, please open a ticket with our support team.
Jan 23, 18:44 UTC
Update - Our Engineering team continues to work on mitigation efforts. An additional small bug has been discovered and remediated. About 10% of clusters have had accessibility restored and restoration efforts are ongoing. 
We will post another update as soon as we have new developments.
Thank you for your patience and we apologize for any inconvenience.
Jan 23, 15:12 UTC
Identified - Our Engineering team has identified the cause of the issue with Managed Kubernetes clusters in the FRA1 region. 200 clusters are impacted by the issue and remain inaccessible to users at this time. 
Our Engineering team is engaged in remediating these clusters to restore accessibility. As soon as we are able to provide an estimated time to restore, we will provide an update.
Jan 23, 13:02 UTC
Investigating - As of 12:18 UTC, our Engineering team is investigating an issue with Kubernetes clusters in the FRA1 region. During this time, users may experience errors while communicating with their clusters in the FRA1 region. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/bsk6hmj0d7nv</id>
        <link href="https://www.githubstatus.com/incidents/bsk6hmj0d7nv"/>
        <updated>2024-01-23T18:53:29.000Z</updated>
        <summary type="html"><![CDATA[Jan 23, 18:53 UTC
Resolved - On January 23, 2024 at 14:36 UTC, our internal metrics began showing an increase in exceptions originating from our live update service. Live updates to Issues, PRs, Actions, and Projects were failing, but refreshing the page successfully updated page content. We resolved the issue by rolling back a problematic dependency update and reenabled live updates at 18:53 UTC. 
We are working to improve alerting and monitoring of our live update service to reduce our time to detection and mitigation.
Jan 23, 18:53 UTC
Update - Live updates have been restored and the system is operating normally.
Jan 23, 18:14 UTC
Update - We have identified and are beginning to roll out a potential fix for issues with live updates to our Web UI that power automatic page updates such as…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some Enterprise Grid users may be seeing unexpected Slackbot responses]]></title>
        <id>https://status.slack.com//2024-01/166eb312bd134031</id>
        <link href="https://status.slack.com//2024-01/166eb312bd134031"/>
        <updated>2024-01-23T03:06:24.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 12:20pm PST until 3:00pm PST on January 22, 2024, some Enterprise Grid users noticed multiple Slackbot responses being triggered unexpectedly.


We determined that the rollout of a fix for an older bug report pertaining to Slackbot responses not working in org-wide or multi-workspace channels, was the root cause. 


Whilst the fix for the bug was intended to improve this features behaviour, ensuring that Slackbot custom responses would work in these channel types, our wider team concluded that the fixed behaviour might not function well for organizations with potentially thousands of custom Slackbot responses.


We rolled back the deployment which caused this behaviour. Customers will no longer see these Slackbot custom responses being triggered unexpectedly.


A discussion is underway about the long-term future of Slackbot custom response behaviour within large organizations.


Thank you for your patience whilst we resolved this.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network connectivity in LON1]]></title>
        <id>https://status.digitalocean.com/incidents/9y133zrfqngf</id>
        <link href="https://status.digitalocean.com/incidents/9y133zrfqngf"/>
        <updated>2024-01-22T18:38:44.000Z</updated>
        <summary type="html"><![CDATA[Jan 22, 18:38 UTC
Resolved - As of 18:37 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. 
Thank you for your patience and we apologize for any inconvenience.
Jan 22, 13:32 UTC
Monitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaas, Managed Kubernetes, and Managed Database. 
We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.
Jan 22, 12:25 UTC
Identified - Our Engineering team has identified the cause of the issue impacting networking in the LON1 region and is actively working on a fix.
During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaaS, Managed Kubernetes, and Managed Databases.
We will post an update as soon as additional information is available
Jan 22, 11:36 UTC
Investigating - Our Engineering team is investigating a networking issue in our LON1 region. At this time, you may experience packet loss or dropped connections.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smily’s enhanced deposit management: seamless synchronization with booking.com]]></title>
        <id>283940</id>
        <link href="https://changelog.bookingsync.com/smily-s-enhanced-deposit-management-seamless-synchronization-with-booking-com-283940"/>
        <updated>2024-01-22T17:40:37.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
Introducing Smily’s enhanced deposit management: seamless synchronization with booking.com
Say goodbye to data discrepancies and hello to effortless property management!
Key Features:
Easier deposit handling: Our system now allows you to manage properly damage deposits directly through Smily, ensuring perfect synchronization with Booking.com.
No more data mismatch: We’ve revolutionized our sync logic to eliminate discrepancies between Smily and Booking.com. This means the information you see on one platform is exactly what you’ll find on the other.
Enhanced user experience: Managing your properties has never been smoother. Our updates are tailored to make your workflow more intuitive and efficient.
Revamped collection & return methods: Say farewell to the hassle of cash deposits and wire returns. With our update, deposit collection and returns are streamlined through credit card transactions, simplifying the process for you and your guests.
Benefits for you:
Peace of mind: With matched data across platforms, reduce the risk of errors and enjoy a more streamlined management experience.
Time savings: Minimize manual work, allowing you to focus on growing your business and enhancing guest experiences.
Consistent information: Rest assured knowing that what you set in Smily is precisely reflected on Booking.com.
-> Update your knowledge with our revised manual. Get all the latest information and tips for maximizing the benefits of this new feature at Smily's updated manual.
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action required for continued Email communication from your own domain]]></title>
        <id>283898</id>
        <link href="https://changelog.bookingsync.com/action-required-for-continued-email-communication-from-your-own-domain-283898"/>
        <updated>2024-01-22T13:29:03.000Z</updated>
        <summary type="html"><![CDATA[Action required
  
We have an important update regarding your email communications with your guests via the notification app.
To maintain uninterrupted service, some adjustments are needed on your domain provider to keep sending notifications from your own domain name.
In case you are confident to do the changes yourself, please follow the instructions on our manual and let us know once it is done so we can verify, otherwise please contact our Customer Support team (Yannick or Pauline) to receive proper instructions and/or schedule a video meeting and we will be happy to help you.
Note: Please make sure you have access to your domain provider before the call.
Your prompt attention to this matter is appreciated to prevent any disruptions. Please note that the changes need to be done before February 13 2024.
Thank you for your cooperation.]]></summary>
        <author>
            <name>Yannick, Customer Care Team Leader - Pro Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces availability in SFO2]]></title>
        <id>https://status.digitalocean.com/incidents/299xh1cfs8s5</id>
        <link href="https://status.digitalocean.com/incidents/299xh1cfs8s5"/>
        <updated>2024-01-22T10:32:23.000Z</updated>
        <summary type="html"><![CDATA[Jan 22, 10:32 UTC
Resolved - Our Engineering team has resolved the issue impacting Spaces API availability in our SFO2 region. From approximately 09:00 UTC - 10:00 UTC, users may have experienced latency or timeouts when trying to access or manage their Spaces buckets. Spaces should now be operating normally.
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Jan 22, 10:10 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting SFO2 Spaces API availability and monitoring the situation. We will post an update as soon as the issue is fully resolved.
Jan 22, 09:59 UTC
Identified - As of 09:50 UTC, our Engineering team has identified the cause of the issue impacting Spaces API availability in our SFO2 region and is actively working on a fix.
We will post an update as soon as additional information is available
Jan 22, 09:13 UTC
Investigating - As of 09:00 UTC, Our Engineering team is investigating an issue impacting Spaces API availability in our SFO2 region.
During this time, users may experience slowness or timeouts when trying to access or manage their Spaces resources in SFO2.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/7ck5966p1073</id>
        <link href="https://www.githubstatus.com/incidents/7ck5966p1073"/>
        <updated>2024-01-21T09:34:34.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 09:34 UTC
Resolved - On 2024-01-21 at 3:38 UTC, we experienced an incident that affected customers using Codespaces. Customers encountered issues creating and resuming Codespaces in multiple regions due to operational issues with compute and storage resources.
Around 25% of customers were impacted, primarily in East US and West Europe. We re-routed traffic for Codespace creations to less impacted regions, but existing Codespaces in these regions may have been unable to resume during the incident.
By 7:30 UTC, we had recovered connectivity to all regions except West Europe, which had an extended recovery time due to increased load in that particular region. The incident was resolved on 2024-01-21 at 9:34 UTC once Codespace creations and resumes were working normally in all regions.
…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/hmvr5kpgzc45</id>
        <link href="https://www.githubstatus.com/incidents/hmvr5kpgzc45"/>
        <updated>2024-01-21T06:19:52.000Z</updated>
        <summary type="html"><![CDATA[Jan 21, 06:19 UTC
Resolved - On 2024-01-21 from 02:05 UTC to 06:19 UTC, GitHub Hosted Runners experienced increased error rates from our main cloud service provider. The errors were initially limited to a single region and we were able to route around the issue by transparently failing over to other regions. However, errors gradually expanded across all regions we deploy to and led to our available compute capacity being exhausted.
During the incident, up to 35% of Actions jobs using Larger Runners and 2% of Actions jobs using GitHub Hosted Runners overall may have experienced intermittent delays in starting. Once the issue was resolved by our cloud service provider, our systems made a full recovery without intervention.
We’re working closely with our service provider to understand the cause of the outage and mitigations we can put in place. We’re also working to increase our resilience to outages of this nature by expanding the regions we deploy to beyond the existing set, especially for Larger Runners.
Jan 21, 05:54 UTC
Update - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.
Jan 21, 05:26 UTC
Update - We have mitigated the issues impacting Actions Larger Runners. We are still experiencing delays starting normal jobs, and are continuing to investigate.
Jan 21, 04:53 UTC
Update - The team has identified the cause of the issues with Actions Larger Runners and has begun mitigation.
Jan 21, 04:16 UTC
Update - The team continues to investigate issues with some Actions jobs being queued for a long time and a percentage of jobs failing. We will continue providing updates on the progress towards mitigation.
Jan 21, 03:45 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet rebuild]]></title>
        <id>https://status.digitalocean.com/incidents/x0f8bvr688rs</id>
        <link href="https://status.digitalocean.com/incidents/x0f8bvr688rs"/>
        <updated>2024-01-17T09:34:03.000Z</updated>
        <summary type="html"><![CDATA[Jan 17, 09:34 UTC
Resolved - As of 09:17 UTC, our Engineering team has confirmed the full resolution of the issue impacting the Droplet rebuild via the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 17, 09:22 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue impacting the Droplet rebuild via Cloud Control Panel and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues when rebuilding Droplets from the Cloud Control Panel. We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 17, 08:26 UTC
Identified - Our Engineering team has identified the cause of the issue impacting the Droplet rebuild via the Cloud Control Panel and is actively working on a fix. During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel. We will post an update as soon as additional information is available.
Jan 17, 07:15 UTC
Investigating - As of 06:50 UTC, our Engineering team is investigating an issue impacting the Droplet rebuild via the Cloud Control Panel.
During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces Functionality]]></title>
        <id>https://status.digitalocean.com/incidents/j4jwmwl3szxz</id>
        <link href="https://status.digitalocean.com/incidents/j4jwmwl3szxz"/>
        <updated>2024-01-16T13:34:24.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 13:34 UTC
Resolved - Our Engineering team has resolved the issue impacting multiple spaces-related functionalities. From approximately 10:30 UTC - 13:30 UTC, users may have experienced issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API. Spaces-related functionalities should now be operating normally.
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.
Jan 16, 12:51 UTC
Monitoring - Our Engineering team has taken actions to mitigate the issue affecting multiple Spaces-related functionalities and is monitoring the situation.
The impact has been subsided and the users should no longer experience issues with Spaces-related functionalities. 
We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.
Jan 16, 12:03 UTC
Identified - Our Engineering team has identified the issue affecting multiple Spaces-related functionalities and is actively working on a fix.
During this time, users may experience issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API.
Additionally, this may also impact Container Registry creation and issues with transferring images between regions. 
We apologize for the inconvenience and will share an update once we have more information.
Jan 16, 10:54 UTC
Investigating - As of 10:30 UTC, our Engineering team is investigating an issue with multiple Spaces functionalities via the Cloud Control Panel. 
During this time, users may experience errors when attempting to delete objects via the Cloud Control Panel. At this moment we are investigating the exact impact and will share more information as soon as we have it.
We apologize for the inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functions]]></title>
        <id>https://status.digitalocean.com/incidents/47dqh6pzqybt</id>
        <link href="https://status.digitalocean.com/incidents/47dqh6pzqybt"/>
        <updated>2024-01-16T09:30:15.000Z</updated>
        <summary type="html"><![CDATA[Jan 16, 09:30 UTC
Resolved - As of 08:50 UTC, our Engineering team has confirmed the full resolution of the issue impacting the ability to access and manage Functions through the Cloud Control Panel.
We appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.
Jan 16, 08:59 UTC
Monitoring - Our Engineering team has been able to mitigate the issue related to the access and operations with Functions through the Cloud Control Panel.
Users should no longer face any problems in accessing or operating Functions using the Cloud Control Panel. 
We apologize for the inconvenience. We are monitoring the situation and will post an update once we confirm this incident is fully resolved.
Jan 16, 08:17 UTC
Investigating - As of 03:00 AM UTC, our Engineering team is investigating an issue impacting Functions. 
During this time, users may experience issues with accessing Functions via the Cloud Control Panel. At this time, API operations shouldn't be impacted and should continue to function as intended. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[🆕 New partnership announcement with KeyNest!]]></title>
        <id>283413</id>
        <link href="https://changelog.bookingsync.com/new-partnership-announcement-with-keynest!-283413"/>
        <updated>2024-01-16T06:58:43.000Z</updated>
        <summary type="html"><![CDATA[New!
  


We are thrilled to announce a new partnership that will revolutionise key management for vacation rental hosts and property managers like you. 🔑
  

💡What is KeyNest Points?
KeyNest Points is a global network of over 5,500 locations where you can securely store and exchange keys with ease. No more hassles of on-site visits or high installation costs.
Most KeyNest Points are open 24/7, ensuring flexibility for key exchanges at your convenience. Discover your nearest Point and its opening hours on the interactive map provided.

 


🏠 KeyNest Points - Your Trusted Partners
These points are typically local businesses such as convenience stores, cafes, hotels, or petrol stations, conveniently situated near your properties.
Each KeyNest Point is managed by trained staff, ensuring instant and secure key exchanges. Your peace of mind is paramount.

🔤 How Does it Work?

Here's a quick overview of how KeyNest Points operate:
Drop your keys at a local KeyNest Point.
Your keys are tagged and logged into the system for real-time tracking.
Staff at the Point securely stores your keys.
Send the location of the key and the code to your guests within your automated Smily messages.
Guests or cleaners visit the Point, show their safety code, and receive the key.
Upon return, they drop the key back at the same Point, and you are notified.
 
👉 Install the Keynest app here
 
💌 Learn more on our dedicated manual page.
  
If you have any questions, please don't hesitate to contact support@keynest.com.]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may not be able to configure 2FA]]></title>
        <id>https://status.slack.com//2024-01/9301a4fb7cc577ea</id>
        <link href="https://status.slack.com//2024-01/9301a4fb7cc577ea"/>
        <updated>2024-01-15T23:51:07.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 11:26 AM PST on January 12, 2024 to 10:04 AM PST on January 15, 2024, some users were unable to configure 2FA on their accounts. We were made aware of this after a spike in reports early in the morning of Monday, January 15.


Upon investigation, this issue was traced back to a recent code change which we discovered was preventing users from being redirected back to the 2FA configuration page after entering their password during the 2FA setup process. We immediately reverted this change which fully resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degraded behavior when moving pages in bulk]]></title>
        <id>https://status.notion.so/incidents/wn8zmpphxwr9</id>
        <link href="https://status.notion.so/incidents/wn8zmpphxwr9"/>
        <updated>2024-01-15T19:19:20.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 11:19 PST
Resolved - We've pushed a fix for this issue now and users can move pages in bulk again without errors. We apologize for the earlier disruption and thank you for bearing with us through this. 
Please open Notion and press Cmd/Ctrl + Shift + R to reload the latest changes before trying to move pages again.
Jan 15, 10:24 PST
Identified - Our team has identified the cause of problems when moving pages in bulk across Notion databases and is working on a fix. We will share further updates as soon as the problem is resolved.
Jan 15, 07:53 PST
Investigating - Users may be experiencing degraded behavior when moving pages in bulk across Notion databases. We are investigating this issue and will share an update as soon as possible.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monday app not working]]></title>
        <id>https://status.make.com/incidents/h3gvmlbldj1q</id>
        <link href="https://status.make.com/incidents/h3gvmlbldj1q"/>
        <updated>2024-01-15T15:18:40.000Z</updated>
        <summary type="html"><![CDATA[Jan 15, 16:18 CET
Resolved - We have reactivated the affected scenarios. Please note that this reactivation will not be visible in the scenario logs. Currently, the Monday app is fully operational.
Jan 15, 11:42 CET
Update - A fix has been rolled and we are investigating options to re-enable affected scenarios automatically.
Jan 15, 10:27 CET
Monitoring - A fix has been implemented and we are monitoring the results.
Jan 15, 09:47 CET
Identified - The issue has been identified and a fix is being implemented.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue with MFA Login]]></title>
        <id>https://status.notion.so/incidents/qbkb93dwrm9r</id>
        <link href="https://status.notion.so/incidents/qbkb93dwrm9r"/>
        <updated>2024-01-12T06:56:42.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 22:56 PST
Resolved - This incident has been resolved.
Jan 11, 20:52 PST
Identified - We are experiencing issues with MFA  login method. The team has identified the cause and is working on the fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Networking]]></title>
        <id>https://status.digitalocean.com/incidents/33vqf05m8396</id>
        <link href="https://status.digitalocean.com/incidents/33vqf05m8396"/>
        <updated>2024-01-11T00:18:25.000Z</updated>
        <summary type="html"><![CDATA[Jan 11, 00:18 UTC
Resolved - Our Engineering team has confirmed full resolution of this incident. 
From approximately 20:15 - 21:45 UTC, DigitalOcean experienced a global networking issue that impacted multiple services and products. Users saw increased error rates and latency for event processing, accessing our Cloud Control Panel/API, applying Cloud Firewall policies, accessing www.digitalocean.com and our Community site, and DNS resolution. Additionally, users saw timeouts/increased latency for networking requests to Droplets and Droplet-based services, as well as connections to existing App Platform Apps. 
We sincerely apologize for the disruption. If you continue to experience issues or have questions, please reach out to our Support team by opening a ticket from within your account. …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New fix for your booking.com reservations]]></title>
        <id>283019</id>
        <link href="https://changelog.bookingsync.com/new-fix-for-your-booking-com-reservations-283019"/>
        <updated>2024-01-10T17:36:04.000Z</updated>
        <summary type="html"><![CDATA[New!
 
Improvement
  
We're thrilled to share a new feature we’ve just released for booking.com properties.
What's New?
The rate rule Booking at least 'x' day ahead is now synchronised on booking.com channel;
Why Does This Matter?
You were facing the obstacle where Guests were making a booking for the same day or 'x' days (based on your settings) even though the setting to prevent that was activated;
-> It won’t happen anymore.
What's Next?
If you face any obstacle with this setting, reach out to us; 
We are and will be working on more improvements to make your day-to-day easier :)
--
💡 For more detailed information, simply click here;
Let's embark on this journey together to make your vacation rental dreams a reality 🚀
Have a great day,
Your Smily team :)]]></summary>
        <author>
            <name>Basile, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues, API Requests, Pull Requests, Actions, Pages, Git Operations, Webhooks, Packages and Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/pxg3dz4yg7lp</id>
        <link href="https://www.githubstatus.com/incidents/pxg3dz4yg7lp"/>
        <updated>2024-01-09T14:40:45.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 14:40 UTC
Resolved - On January 9 between 12:45 and 13:56 UTC, services in one of our three sites experienced elevated latency for connections.  This led to a sustained period of timed out requests across a number of services, including but not limited to our git backend.  An average of 5% and max of 10% of requests failed with a 5xx response or timed out during this period.  This was caused by a combination of events that led to connection limits being hit in load balancer proxies in that site.  An upgrade of hosts was in flight, which meant a subset of proxy hosts were draining and coming offline as the upgrade rolled through the fleet.  A config change event also triggered a connection reset across all services in that site.  These events are commonplace, but led to a spike in c…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Database Operations]]></title>
        <id>https://status.digitalocean.com/incidents/8fm83lgfsdhm</id>
        <link href="https://status.digitalocean.com/incidents/8fm83lgfsdhm"/>
        <updated>2024-01-09T12:55:23.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 12:55 UTC
Resolved - As of 12:30  UTC, our Engineering team has resolved the issue impacting CRUD (create, read, update, delete) operations for Managed Database Clusters in all the regions.
Everything should now be functioning normally. We appreciate your patience throughout the process.
If you continue to experience problems, please open a ticket with our support team.
Jan  9, 10:17 UTC
Monitoring - Our Engineering team has confirmed that the action taken to mitigate the recurrence of the issue is successful. Users should no longer experience errors in CRUD (create, read, update, delete) operations for Managed Database Clusters in all regions, via both the Cloud Control Panel and API requests.
We are actively monitoring the situation to ensure stability and will provide an update …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/dxthh9653wkk</id>
        <link href="https://www.githubstatus.com/incidents/dxthh9653wkk"/>
        <updated>2024-01-09T05:44:37.000Z</updated>
        <summary type="html"><![CDATA[Jan  9, 05:44 UTC
Resolved - On January 9 between 1:06 and 5:43 UTC, no audit log events were streamed for customers that have that enabled. All events that happened during this time were delivered after the issue was mitigated. The event delivery was failing due to a data shape issue, with a streaming configuration linked to a soft-deleted Enterprise causing a runtime error for a backend service.  This was mitigated by removing that configuration.  Since the incident, we have improved our detection of these errors and ensured support for similar scenarios.
Jan  9, 05:40 UTC
Update - Audit logs streaming is currently unavailable due to a misconfiguration issue. The root cause is understood and our engineers will be updating the broken configuration shortly.
Jan  9, 04:59 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/35pgg0gc75nv</id>
        <link href="https://www.githubstatus.com/incidents/35pgg0gc75nv"/>
        <updated>2024-01-08T23:41:03.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 23:41 UTC
Resolved - From January 5th to January 8th, Codespaces experienced issues with port forwarding when connecting from a web browser. During this incident 100% of operations to forward ports and connect to a forwarded port failed for customers located in US West and Australia. The cause was due to a cross origin API change in our port forwarding service. After detecting the issue, we mitigated the impact by rolling back the change that caused the discrepancy in the cross origin rules. We have improved the way we are detecting issues like this one including implementing automated alerts during and after the rollout process so that we get signals as early as possible.
Jan  8, 22:37 UTC
Update - The initial mitigation we attempted did not fully resolve this issue.  We are continuing to investigate and will provide an update as soon as possible.
Jan  8, 21:55 UTC
Update - We are engaged on the issue and continuing to work toward a mitigation. Please continue to fallback to VS Code desktop for port forwarding workflows.
Jan  8, 21:03 UTC
Update - Mitigation of degraded Codespaces port forwarding in is progress.  In the meantime, please continue to use VS Code Desktop for port forwarding.
Jan  8, 20:22 UTC
Update - We are actively mitigating degraded performance of Codespaces port forwarding in the web and GitHub CLI.  Codespaces port forwarding on VS Code Desktop is unaffected.
Jan  8, 20:22 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/crg2qtnbmb1t</id>
        <link href="https://status.rippling.com/incidents/crg2qtnbmb1t"/>
        <updated>2024-01-08T18:26:01.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 18:26 UTC
Resolved - The Rippling application was inaccessible from 9:39am to 10:06am PST; it has recovered and is now performing normally. The root cause is an incident with Cloudflare: https://www.cloudflarestatus.com/
Jan  8, 18:11 UTC
Monitoring - The issue accessing Rippling has been mitigated, and we are monitoring the results.
Jan  8, 17:56 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry and App Platform in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/88sl0dldq6dh</id>
        <link href="https://status.digitalocean.com/incidents/88sl0dldq6dh"/>
        <updated>2024-01-08T15:57:06.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 15:57 UTC
Resolved - As of 15:00 UTC, Our engineering team has resolved the issue impacting Container Registry  and App platform builds in various regions including AMS3, LON1 and FRA1 regions. 
Everything should now be functioning normally. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Jan  8, 15:13 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting Container Registry App platform builds in AMS3, LON1 and FRA1 regions. User should not be facing any issues while interacting with their Container registries and also while building their Apps. 
We are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. 
Thank you for your patience and we apologize for the inconvenience.
Jan  8, 14:15 UTC
Investigating - Our Engineering team is investigating an issue with DigitalOcean Container Registry service in our AMS3 and LON1 regions. 
During this time a subset of customers may experience latency while interacting with the Container Registries. This is also impacting App Platform builds and users may encounter delays in while building their Apps and could potentially experience timeout errors in builds as a result. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2024, here we are! Happy New Year from all of us at Smily! 🎉]]></title>
        <id>282774</id>
        <link href="https://changelog.bookingsync.com/2024-here-we-are!-happy-new-year-from-all-of-us-at-smily!-282774"/>
        <updated>2024-01-08T09:34:22.000Z</updated>
        <summary type="html"><![CDATA[Communications
  

As we step into this new year, we want to take a moment to express our heartfelt gratitude for your trust and partnership. Your journey is at the heart of everything we do, and it's been an incredible experience growing alongside each of you.
This past year has been filled with challenges but also triumphs, and through it all, our Property Managers and Owners have always been our main inspiration. We're honored to be a part of your story, helping you manage and grow your vacation rental business.
Looking ahead, we're excited to continue our journey together. With every challenge comes an opportunity, and we're here to support you every step of the way. Let's make this year one of progress, success, and shared smiles!
Hereby some of Smily's milestones and snapshot of our 2023 key achievements:
Increase efficient communication: enhanced Inbox filters for efficient guest communication, enabling quick searches by conversation status, assignee, guest, booking stage, and more.
Powerful partnerships: Collaborations with Maeva and PlumGuide have opened new horizons and more OTA distribution channels!
Guest experience & security: From synchronized check-in types to robust website https updates, we're elevating the guest journey.
Sustainable visibility: Synchronization of our eco-amenities to Holidu to boost visibility and conversion rates!
Reviewed payment processes: With easier Stripe account onboarding and better integration but also by streamlining our billing system, managing finances should be smoother than ever.
Review management made easy: Our AI-powered Review Response and Automated Guest Reviews are setting new standards in reputation management.
Tech Marvels: Our focus on modernization and security has supercharged system stability and performance. Don't miss the insights on our new Tech Blog.
Once again, here's to a year filled with happiness, health, and prosperity. Let's keep building dreams and sharing smiles with every soul.
Happy New Year! 🌟
With love and warmth,
Your Smily Team]]></summary>
        <author>
            <name>Ella, Chief Customer Officer (CCO) &amp; Cofounder</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Clusters - Authentication Errors]]></title>
        <id>https://status.digitalocean.com/incidents/x0l7xgm3bc91</id>
        <link href="https://status.digitalocean.com/incidents/x0l7xgm3bc91"/>
        <updated>2024-01-08T00:32:13.000Z</updated>
        <summary type="html"><![CDATA[Jan  8, 00:32 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue with authentication failures on Managed Kubernetes Clusters.
From 19:26 - 23:29 UTC, users saw errors accessing and performing actions on Managed Kubernetes Clusters, due to authentication failures. All services are now operating normally.  
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!
Jan  8, 00:01 UTC
Monitoring - Our Engineering team has confirmed that the action taken to mitigate the impact of this incident is successful and customers are no longer seeing issues accessing or performing actions on Managed Kubernetes Clusters. 
We'll monitor the situation for a short while and will post a final update once we confirm full resolution.
Jan  7, 23:32 UTC
Identified - Our Engineering team has taken action to mitigate the impact of this incident and internal tests on impacted clusters are now passing. At this time, users should start to see recovery and be able to access clusters normally. 
We are still investigating root cause and ensuring this mitigation will continue to be successful. We'll provide another update as soon as possible.
Jan  7, 22:02 UTC
Investigating - Our Engineering team is investigating customer reports of errors when trying to perform actions on Managed Kubernetes Clusters using kubectl. At this time, customers may see errors indicating that the API token is unauthorized or missing credentials. We will provide an update as soon as we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2024-001]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2024-001</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2024-001"/>
        <updated>2024-01-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: On Windows before Tailscale version 1.52 and on Linux before
Tailscale 1.54, the tailscale serve and tailscale funnel features allowed
users to serve the contents of directories that their user account could not
access, but which the tailscaled service process could.
What happened?
A user could escalate their own file read access by running, for example,
tailscale.exe serve http / C:\, and then browsing to the local HTTP endpoint.
The issue can also occur on Linux if the local administrator enabled an operator
user ID with tailscale up --operator=$USER, as the $USER account could
serve itself files that it could not normally read.
Who is affected?
Owners of Windows deployments for which the users of Tailscale nodes do not also
have OS-level administrative access, and owners of Linux deployments where the
administrator enabled non-root --operator access.
This issue can only be triggered by a local user and cannot be triggered
remotely.
What is the impact?
This issue enables local privilege escalation (file read access). Access to
certain system files (such as /etc/shadow on Linux) can then be used to obtain
full administrative control over the host.
What do I need to do?
On Windows 10 and later, upgrade to Tailscale 1.52 (released 30 October
2023) or later, which resolves the issue.
On Windows 7 and 8, upgrade to Tailscale 1.44.3 (released 8 Jan
2024), which resolves the issue.
On Linux, upgrade to 1.54 (released 15 November 2023) or later,
which resolves the issue.
The best practice is to run the latest stable version, which as of this writing
is 1.56.1. Consider turning on automatic updates.
Use Tailscale ACLs to control the availability of Funnel.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
</feed>