<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-09-16T00:19:25.513Z</id>
    <title>osmos::feed</title>
    <updated>2023-09-16T00:19:25.513Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Planned Maintenance for Flex Insights Historical Reporting]]></title>
        <id>https://status.twilio.com/incidents/fj2h091ltc8q</id>
        <link href="https://status.twilio.com/incidents/fj2h091ltc8q"/>
        <updated>2023-09-16T08:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Sep 16, 01:00 - 04:00 PDT
Sep  7, 06:27 PDT
Scheduled - The Flex Insights dashboards visualization partner in the Chicago, U.S. is conducting a planned maintenance on September 16th, 2023, at 1:00 AM Pacific Time. The maintenance window is planned to take up to 3 hours. During the maintenance window, loads will be impacted while Flex Insights dashboard will be available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Failures To Access API GET List For Account AvailablePhoneNumber TollFree]]></title>
        <id>https://status.twilio.com/incidents/1w5p1b9lmq5s</id>
        <link href="https://status.twilio.com/incidents/1w5p1b9lmq5s"/>
        <updated>2023-09-15T23:29:23.000Z</updated>
        <summary type="html"><![CDATA[Sep 15, 16:28 PDT
Update - 1. inventory-search-service (the service used to look up the numbers in Elastic Search) is returning some numbers that are not present inventory_dids_to_did_tags table. This seems to be related to an onboarding issue with a subset of tollfree numbers.
This is causing as to use our fallback and go to the database. This is happening for most tollfree requests to the available phone numbers page.
2. When the pageSize is set in the request, the query issued to the database is faster than when the pageSize is not included. That's why the issue only shows up for the requests without a pageSize.
We expect to provide another update in 8 hours or as soon as more information becomes available.
Sep 15, 14:26 PDT
Identified - Our engineers have identified the issue with the AvailablePhoneNumbers/TollFree api returning 500 errors and are working to deploy a fix. As a workaround please include a PageSize parameter in your requests at this time. We expect to provide another update in 4 hours or as soon as more information becomes available.
Sep 15, 13:00 PDT
Update - We are experiencing failures to access GET API List for Account, AvailablePhoneNumber and TollFree. Our engineering team has been alerted and is actively investigating. We expect to provide another update in 2 hours or as soon as more information becomes available.
Sep 15, 12:03 PDT
Update - We are experiencing failures to access GET API List for Account, AvailablePhoneNumber and TollFree. Our engineering team has been alerted and is actively investigating. We expect to provide another update in 1 hour or as soon as more information becomes available.
Sep 15, 11:47 PDT
Investigating - Our monitoring systems have detected a potential issue with access GET API List for Account, AvailablePhoneNumber and TollFree. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Report Delays to CTBC Network in Brazil]]></title>
        <id>https://status.twilio.com/incidents/jtpv63yv4plr</id>
        <link href="https://status.twilio.com/incidents/jtpv63yv4plr"/>
        <updated>2023-09-15T23:01:56.000Z</updated>
        <summary type="html"><![CDATA[Sep 15, 16:01 PDT
Update - We are experiencing SMS delivery delays when sending messages to  CTBC Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Sep 15, 14:57 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to  CTBC Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Studio Messaging Scheduler Service 500 Errors]]></title>
        <id>https://status.twilio.com/incidents/cxcyf3q6hdrv</id>
        <link href="https://status.twilio.com/incidents/cxcyf3q6hdrv"/>
        <updated>2023-09-15T22:48:21.000Z</updated>
        <summary type="html"><![CDATA[Sep 15, 15:48 PDT
Resolved - Twilio messaging-scheduler-service and messaging-scheduler-service-mms was degraded for 11 minutes between 3:14 pm and 3:25 pm Pacific Time on 09/12/2023. During this period of time customers may have experienced a high rate of 5xx failures, affecting the Studio and MMS products, respectively. The issue has now been resolved.
Sep 15, 15:23 PDT
Monitoring - Our engineers are currently investigating the customer impact of a retroactive issue with Studio 500 errors. We will follow up and provide customer impact details within 2 hours.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays To Multiple Networks In Multiple Countries]]></title>
        <id>https://status.twilio.com/incidents/3c15zxr9096v</id>
        <link href="https://status.twilio.com/incidents/3c15zxr9096v"/>
        <updated>2023-09-15T15:06:24.000Z</updated>
        <summary type="html"><![CDATA[Sep 15, 08:06 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to multiple networks in multiple countries. This incident has been resolved.
Sep 15, 06:15 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to multiple networks in multiple countries. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Sep 15, 05:49 PDT
Update - We are experiencing SMS delivery delays when sending messages to multiple networks in multiple countries. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 30 minutes or as soon as more information becomes available.
Sep 15, 05:35 PDT
Investigating - We've become aware of a potential issue with SMS delivery delays to multiple networks in multiple countries. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kazakhstan SMS Carrier Maintenance - K-Cell]]></title>
        <id>https://status.twilio.com/incidents/0dm2h6zdpyxc</id>
        <link href="https://status.twilio.com/incidents/0dm2h6zdpyxc"/>
        <updated>2023-09-15T15:00:56.000Z</updated>
        <summary type="html"><![CDATA[Sep 15, 08:00 PDT
Completed - The scheduled maintenance has been completed.
Sep 13, 19:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Sep 13, 08:35 PDT
Scheduled - The K-Cell network in Kazakhstan is conducting a planned maintenance from 13 September 2023 at 19:00 PDT until 15 September 2023 at 08:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to K-Cell Kazakhstan handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Checkbox creation and updates down]]></title>
        <id>https://status.notion.so/incidents/5xq6jc04sf0b</id>
        <link href="https://status.notion.so/incidents/5xq6jc04sf0b"/>
        <updated>2023-09-14T18:31:05.000Z</updated>
        <summary type="html"><![CDATA[Sep 14, 11:31 PDT
Resolved - This incident has been resolved. Formulas and checkboxes should now work for all public API users.
Sep 14, 11:24 PDT
Monitoring - At 11:00 AM PT on September 13th, 2023, some API calls for formulas and checkboxes were failing. 
A server rollback has been sent. We are currently monitoring the results.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDATE: We're investigating reports of an issue with Google Drive. We will provide more information shortly. The affected users are able to access Google Drive, but are seeing error messages, high latency, and/or other unexpected behavior.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/yqfXS87CRC6TNqfmSzLi</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/yqfXS87CRC6TNqfmSzLi"/>
        <updated>2023-09-14T15:40:14.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-09-14 13:23</strong> and ended at <strong>2023-09-14 14:33</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><span>  The problem with Google Drive has been resolved. We apologize for the inconvenience and thank you for your patience and continued support.  </span><div> <p>Customers that still see the banner should refresh their browser.</p>
  </div></div><hr><p>Affected products: Google Drive</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/8rwpr0c6hvbc</id>
        <link href="https://www.githubstatus.com/incidents/8rwpr0c6hvbc"/>
        <updated>2023-09-14T10:21:49.000Z</updated>
        <summary type="html"><![CDATA[Sep 14, 10:21 UTC
Resolved - This incident has been resolved.
Sep 14, 10:01 UTC
Update - We have applied a mitigation to handle elevated errors in a provider. Customers will see increased latency creating and resuming Codespaces instead of failures.
Sep 14, 09:35 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform in NYC1 Region]]></title>
        <id>https://status.digitalocean.com/incidents/5w2hdj0bnc08</id>
        <link href="https://status.digitalocean.com/incidents/5w2hdj0bnc08"/>
        <updated>2023-09-14T09:00:37.000Z</updated>
        <summary type="html"><![CDATA[Sep 14, 09:00 UTC
Resolved - Our Engineering team has confirmed that the problem affecting the App Platform in the NYC1 region has been completely resolved. Users should now be able to access their apps as usual.
If you continue to experience problems, please open a ticket with our support team.
Thank you for your patience and we apologize for any inconvenience.
Sep 14, 07:18 UTC
Monitoring - Our Engineering team has deployed a fix to resolve the issue with App Platform in the NYC1 region. Users should now have access to their apps. We are monitoring the situation closely and will share an update once the issue is resolved completely.
Sep 14, 05:47 UTC
Identified - As of 05:06 UTC, our Engineering team has observed a reoccurrence of the issues that were impacting the App Platform in the NYC1 region. As a result, a subset of our customers may still intermittently experience problems with reaching their apps. Our team is actively investigating the issue and is in the process of implementing a solution. We apologize for any inconvenience caused and will provide updates as the situation progresses.
Sep 14, 04:42 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with App Platform in the NYC1 region. Users should now have the capability to reach their apps. We are monitoring the situation closely and will share an update once the issue is resolved completely.
Sep 14, 04:22 UTC
Investigating - As of 2:30 UTC, our Engineering team is actively investigating issues with App Platform in the NYC1 region. During this time a subset of our customers may intermittently experience problems with reaching apps. We apologize for the inconvenience and will share an update once we have more information]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking - Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/tymj4w28h54w</id>
        <link href="https://status.digitalocean.com/incidents/tymj4w28h54w"/>
        <updated>2023-09-14T03:18:13.000Z</updated>
        <summary type="html"><![CDATA[Sep 14, 03:18 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services. 
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Sep 14, 00:53 UTC
Monitoring - At this time, all services and regions impacted by this incident are recovered. 
We're now monitoring to ensure stability. 
If you are still experiencing any issues, please let our Support team know.
Sep 13, 21:56 UTC
Update - Our Engineering team has confirmed that additional network routes are impacted, alongside the im…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/ty1p9pd6pncl</id>
        <link href="https://www.githubstatus.com/incidents/ty1p9pd6pncl"/>
        <updated>2023-09-13T07:16:13.000Z</updated>
        <summary type="html"><![CDATA[Sep 13, 07:16 UTC
Resolved - This incident has been resolved.
Sep 13, 05:02 UTC
Update - We are continuing to work with a third party provider to restore the ability to resume for some Codespaces in the West US region. When we have more detail we will provide another update.
Sep 13, 04:02 UTC
Update - Customers in the West US region may not be able to resume some Codespaces. We have failed over creation for new code spaces and are investigating the issue.
Sep 13, 03:45 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues creating reservations on test listings]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/43dzv9cm8bpk</id>
        <link href="https://airbnbapi.statuspage.io/incidents/43dzv9cm8bpk"/>
        <updated>2023-09-13T06:17:55.000Z</updated>
        <summary type="html"><![CDATA[Sep 12, 23:17 PDT
Resolved - This incident has been resolved.
Sep 12, 15:38 PDT
Monitoring - We have deployed a fix for this issue, and you can now create reservations on test listings. We will monitor the results for the next couple of hours. Apologies for the inconvenience.
Sep 11, 20:03 PDT
Update - The teams are actively working on the resolution and will require more time than initially expected.
We apologise for the inconvenience caused due to this issue and hope to provide a positive update as soon as we can.
Thank you for your patience and understanding.
Sep  7, 16:31 PDT
Identified - We have identified the issue causing the errors, however, the resolution is not trivial. The ETA for resolution is Monday, Sep 11, 2023 the latest. Apologies for the inconvenience.
Sep  7, 12:12 PDT
Investigating - We are investigating an issue where you can't create reservations on test listings. When you select dates to book a test listing, the following message appears "Those dates are not available", although the dates are available. We are actively looking into this issue and will provide updates as soon as possible,]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subsea Fiber Faults]]></title>
        <id>https://status.digitalocean.com/incidents/0yj0wwt12hfc</id>
        <link href="https://status.digitalocean.com/incidents/0yj0wwt12hfc"/>
        <updated>2023-09-12T15:41:22.000Z</updated>
        <summary type="html"><![CDATA[Sep 12, 15:41 UTC
Resolved - Our Engineering team has observed stability for network routes and App Platform builds and deployments over the last week. Given this, we will proceed with closing out this incident. 
If the situation changes or we receive any important updates regarding the repair of the fiber faults, we will communicate as needed. 
If you are still experiencing any issues or have any questions, please feel free to open a support ticket from within your account. Thank you for your patience.
Sep  2, 04:32 UTC
Monitoring - Our Engineering team has observed some improvement in the networking routes between NYC and the Western EU, including FRA, AMS, and LON.
Due to the general nature of internet outages, DigitalOcean users may still experience degraded network performance in the …]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Billing system improvements]]></title>
        <id>273866</id>
        <link href="https://changelog.bookingsync.com/billing-system-improvements-273866"/>
        <updated>2023-09-11T07:16:47.000Z</updated>
        <summary type="html"><![CDATA[New!
 
Improvement
  
Tired of billing problems taking up a lot of your time? We understand!
Not long ago, we introduced some changes to our pricing model, which now takes into account your Gross Booking Value (GBV) and adjusts your monthly fee accordingly. Since this change, we understand that you’ve experienced some issues with your monthly payments to Smily and that this has been incredibly frustrating for you.
We are working hard to improve your payments experience, so that you no longer have to spend your time and energy solving payment issues.
We’re very happy to announce the first in a series of changes to the way in which you pay for Smily.
We are moving towards a billing system where:
For every payment that is due on your account, we will immediately issue an invoice.


The invoic…]]></summary>
        <author>
            <name>Megan, Product Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Loading Insights Graphs]]></title>
        <id>https://status.digitalocean.com/incidents/l2szqpvkw4s1</id>
        <link href="https://status.digitalocean.com/incidents/l2szqpvkw4s1"/>
        <updated>2023-09-09T06:41:14.000Z</updated>
        <summary type="html"><![CDATA[Sep  9, 06:41 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with loading the Insights graphs. As of 06.23 UTC, users should be able to access the Insights graphs, and these graphs will display the expected data. 
If you continue to experience problems, please open a ticket with our support team. 
Thank you for your patience and we apologize for any inconvenience.
Sep  9, 06:23 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue with loading the Insights graphs. Users should now be able to access the Insights graphs, and these graphs will display the expected data. We are monitoring the situation closely and will share an update once the issue is resolved completely.
Sep  9, 06:10 UTC
Identified - Our Engineering team has identified the cause of the issue with loading the Insights graphs and is actively working on a fix. During this time, you might encounter errors when trying to access Insights graphs, and the graphs may not display any data. We will post an update as soon as additional information is available.
Sep  9, 05:52 UTC
Investigating - As of 05.15 UTC, our Engineering team is investigating issues with viewing Insights graphs. During this time, you might encounter errors when trying to access Insights graphs, and the graphs may not display any data. We regret any inconvenience this may cause and will provide further information as soon as we have more details.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some Admins are missing DMs in Single User Exports]]></title>
        <id>https://status.slack.com//2023-09/5b5bbd0b1b68e294</id>
        <link href="https://status.slack.com//2023-09/5b5bbd0b1b68e294"/>
        <updated>2023-09-08T19:42:47.000Z</updated>
        <summary type="html"><![CDATA[We've determined this issue impacts a small subset of admins on Enterprise Grid. We'll work to fix this internally, and provide further updates through customer support tickets. We apologize for any disruptions to your day.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users are receiving an error while attempting to load app configurations]]></title>
        <id>https://status.slack.com//2023-09/d0d43b69112068ec</id>
        <link href="https://status.slack.com//2023-09/d0d43b69112068ec"/>
        <updated>2023-09-08T01:38:58.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From September 6, 2023 at 9:50 PM PDT until September 7, 2023 at 3:18 AM PDT, some users may have encountered a 500 error when attempting to access their app configuration pages. 


We determined that a code change inadvertently omitted some recently updated parameters, causing these pages to return errors and preventing customers from accessing or modifying their app configurations.


We implemented an initial fix to rectify the 500 errors for most pages, and then a second fix to address any outstanding issues. With both fixes in place, we restored full access to all app configuration pages.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/2gy2gddtv23d</id>
        <link href="https://www.githubstatus.com/incidents/2gy2gddtv23d"/>
        <updated>2023-09-07T10:38:12.000Z</updated>
        <summary type="html"><![CDATA[Sep  7, 10:38 UTC
Resolved - This incident has been resolved.
Sep  7, 10:02 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/n8wpsvv7wt6x</id>
        <link href="https://www.githubstatus.com/incidents/n8wpsvv7wt6x"/>
        <updated>2023-09-06T21:58:38.000Z</updated>
        <summary type="html"><![CDATA[Sep  6, 21:58 UTC
Resolved - This incident has been resolved.
Sep  6, 21:51 UTC
Update - We have mitigated the issue with webhooks and are working to confirm that customers are no longer seeing issues verifying payload signatures.
Sep  6, 20:36 UTC
Update - We are investigating reports of issues with verifying webhook payload signatures. We will provide updates as we have them.
Sep  6, 20:32 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/njb4tgwrkv34</id>
        <link href="https://www.githubstatus.com/incidents/njb4tgwrkv34"/>
        <updated>2023-09-06T11:41:38.000Z</updated>
        <summary type="html"><![CDATA[Sep  6, 11:41 UTC
Resolved - This incident has been resolved.
Sep  6, 10:07 UTC
Update - We identified the issue affecting Git Operations and applied the mitigation. We expect service to fully recover in 30 min.
Sep  6, 09:03 UTC
Update - Operations that download Git archives and raw files are seeing highly elevated response times (up to 4+ seconds) for users. Other Git operations are not impacted. We are working on mitigation.
Sep  6, 08:53 UTC
Update - We are investigating reports of issues with service(s): Git Operations. We will continue to keep users updated on progress towards mitigation.
Sep  6, 08:21 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporary Outage]]></title>
        <id>https://status.notion.so/incidents/d091c2f258j9</id>
        <link href="https://status.notion.so/incidents/d091c2f258j9"/>
        <updated>2023-09-06T01:54:44.000Z</updated>
        <summary type="html"><![CDATA[Sep  5, 18:54 PDT
Resolved - From 6:25pm PDT - 6:46pm PDT, we experienced an issue that prevented users from accessing Notion. The issue has been resolved, so users should be able to access Notion again. We'll continue to monitor performance and provide updates if we experience other issues.
Root cause: 
One of our memcache clusters crashed and caused our API cluster to get stuck. Engineering resolved the issue by restarting them. We’ll continue to investigate on what caused the cluster to crash.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: A small number of users are experiencing issues loading Slack.]]></title>
        <id>https://status.slack.com//2023-09/a6b0cdff3fbe130e</id>
        <link href="https://status.slack.com//2023-09/a6b0cdff3fbe130e"/>
        <updated>2023-09-05T22:50:42.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From 6:00 PM PST on September 3, 2023 to 12:40 PM PST on September 5, 2023, some users could not launch their Slack desktop apps.


We determined that a code change caused the app to call deleted workspaces for users, resulting in an error and an app loading loop.


Once the root cause was identified, we immediately made an adjustment to the code so that it did not call deleted teams for users, which resulted in the app loading properly once more.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some customers are unable to switch channels in the web browser.]]></title>
        <id>https://status.slack.com//2023-09/6bb97b903beac5b6</id>
        <link href="https://status.slack.com//2023-09/6bb97b903beac5b6"/>
        <updated>2023-09-05T21:53:09.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 12:24 PM PDT on August 30, 2023 to 12:44 PM PDT on September 5, 2023, some users were unable to switch channels using Slack in the browser.


We determined that the clicking on a channel in the sidebar ended up in a codepath which was requesting focus of the main Slack window. So in any case where the Slack app was opened from another page, the users encountered an issue.


The code change was reverted and a fix was deployed which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, Pages, Pull Requests, Codespaces, API Requests, Issues, Webhooks and Packages]]></title>
        <id>https://www.githubstatus.com/incidents/smdz34v7j8q0</id>
        <link href="https://www.githubstatus.com/incidents/smdz34v7j8q0"/>
        <updated>2023-09-05T17:01:49.000Z</updated>
        <summary type="html"><![CDATA[Sep  5, 17:01 UTC
Resolved - From 16:24-16:43 UTC, multiple GitHub services were down or degraded due to an outage in one of our primary databases. 
The primary host for a shared datastore for GitHub experienced an underlying file system write error which affected availability for the majority of public-facing GitHub services. SAML login was affected, as was access to Actions, Issues, Pull Requests, Pages, API, Webhooks, Codespaces, and Packages. In this case, our automatic failover was unable to handle the partial file system failure mode. We mitigated by manually performing a forced failover, initiated 17 minutes after our first alert and completed 2 minutes later.
With the incident mitigated, we are working to assess more detailed impact and resilience improvements to each impacted serv…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/frdfpnnt85s8</id>
        <link href="https://www.githubstatus.com/incidents/frdfpnnt85s8"/>
        <updated>2023-09-05T14:19:40.000Z</updated>
        <summary type="html"><![CDATA[Sep  5, 14:19 UTC
Resolved - Between 06:59 UTC to 13:18 UTC GitHub customers experienced intermittent increased latency when using our git archive and raw file downloading capability. This capability is typically used for larger CI/CD systems and bulk download of repository data.  This also caused intermittent periods of Actions workflow failures.
During this incident we saw bursts of increased unique traffic on specific endpoints that overloaded our caching capability for this system. In addition, the traffic caused saturation of the service that exposed a bug in downloading archived files at high load which caused some archives to be downloaded with incomplete data. This caused intermittent failure of workflows for GitHub Actions, which primarily downloads actions data via these endpoint…]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests, Pull Requests and Actions]]></title>
        <id>https://www.githubstatus.com/incidents/c8zjb9351hlc</id>
        <link href="https://www.githubstatus.com/incidents/c8zjb9351hlc"/>
        <updated>2023-09-05T05:10:01.000Z</updated>
        <summary type="html"><![CDATA[Sep  5, 05:10 UTC
Resolved - This incident has been resolved.
Sep  5, 05:09 UTC
Update - API Requests is operating normally.
Sep  5, 05:09 UTC
Update - Actions is operating normally.
Sep  5, 04:52 UTC
Update - We are investigating reports of issues with service(s): Actions, API Requests, Pull Requests. We will continue to keep users updated on progress towards mitigation.
Sep  5, 04:51 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Sep  5, 04:17 UTC
Investigating - We are investigating reports of degraded performance for API Requests and Pull Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/w01m57lwx8r3</id>
        <link href="https://www.githubstatus.com/incidents/w01m57lwx8r3"/>
        <updated>2023-09-05T03:27:28.000Z</updated>
        <summary type="html"><![CDATA[Sep  5, 03:27 UTC
Resolved - This incident has been resolved.
Sep  5, 03:18 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/tqrmcdmghldw</id>
        <link href="https://www.githubstatus.com/incidents/tqrmcdmghldw"/>
        <updated>2023-09-04T21:56:49.000Z</updated>
        <summary type="html"><![CDATA[Sep  4, 21:56 UTC
Resolved - This incident has been resolved.
Sep  4, 21:28 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users in India are having trouble sending DMs.]]></title>
        <id>https://status.slack.com//2023-09/64d234fca6bc1e06</id>
        <link href="https://status.slack.com//2023-09/64d234fca6bc1e06"/>
        <updated>2023-09-04T15:24:43.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 11:00 PM PDT on September 1, 2023 to 01:00 AM PDT on September 2, 2023, some users in India may have had issues with the messaging functionality in Slack.


We determined that the issue was caused by too many requests causing our database to be overwhelmed.


To resolve the issue, we created additional hosts and redistributed the requests which fixed the issue for affected users. 


Thank you for your patience while we sorted this out.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/76xp2jd3px64</id>
        <link href="https://www.githubstatus.com/incidents/76xp2jd3px64"/>
        <updated>2023-09-04T14:48:44.000Z</updated>
        <summary type="html"><![CDATA[Sep  4, 14:48 UTC
Resolved - This incident has been resolved.
Sep  4, 14:21 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users are unable to post to the #general channel]]></title>
        <id>https://status.slack.com//2023-08/414e590c93cb5636</id>
        <link href="https://status.slack.com//2023-08/414e590c93cb5636"/>
        <updated>2023-09-01T18:36:44.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 23, 2023 from 4:30 PM PDT to August 31, 2023 11:26 PM PDT, some users experienced an issue posting messages in the designated #general channel of their workspace. 


Our team identified that recent changes in our code broke some conditional logic and deprecated the preference that allows users to post in the #general channel.


We pushed out a fix and asked affected users to reload their Slack apps. This combination of steps resolved the issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/wz41ffzj8937</id>
        <link href="https://www.githubstatus.com/incidents/wz41ffzj8937"/>
        <updated>2023-09-01T15:06:06.000Z</updated>
        <summary type="html"><![CDATA[Sep  1, 15:06 UTC
Resolved - This incident has been resolved.
Sep  1, 14:51 UTC
Update - A limited number of users in the UK and Western Europe are experiencing issues starting or reconnecting to GitHub Codespaces. We are investigating the issue.
Sep  1, 14:35 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MongoDB Cluster Creation Failure]]></title>
        <id>https://status.digitalocean.com/incidents/t4hymf61fn71</id>
        <link href="https://status.digitalocean.com/incidents/t4hymf61fn71"/>
        <updated>2023-08-31T19:27:11.000Z</updated>
        <summary type="html"><![CDATA[Aug 31, 19:27 UTC
Resolved - The maintenance that was scheduled to prevent the recurrence of this issue has been completed as of 19:00 UTC and our Engineering team has confirmed that the issue affecting the creation of Mongo Managed Database clusters is fully resolved. 
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Aug 31, 17:03 UTC
Update - Our Engineering team has identified that urgent maintenance is needed in order to remediate this incident and prevent a recurrence. 
From 18:00 -19:00 UTC, we’ll be performing that maintenance. Further details and updates throughout the maintenance can be found here:  https://status.digitalocean.com/incidents/cq5jj3snkp64 
Once completed, we will update this incident after confirming the issue is fully resolved.
Aug 31, 11:44 UTC
Monitoring - Our engineering team has implemented a fix to resolve the issue that was preventing users from creating a MongoDB cluster in any region and is monitoring the situation. We will post an update as soon as the issue is fully resolved.
Aug 31, 10:53 UTC
Identified - Our engineering team has identified the cause of the issue that was preventing users from creating a MongoDB cluster in any region and is working on a fix. We will post an update as soon as additional information is available.
Aug 31, 09:56 UTC
Investigating - As of 09:30 UTC, our engineering team is investigating an issue that prevents creating a MongoDB cluster in any region. During this time, you may face issues when creating a MongoDB cluster. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/cq5jj3snkp64</id>
        <link href="https://status.digitalocean.com/incidents/cq5jj3snkp64"/>
        <updated>2023-08-31T19:00:04.000Z</updated>
        <summary type="html"><![CDATA[Aug 31, 19:00 UTC
Completed - The scheduled maintenance has been completed.
Aug 31, 18:25 UTC
Update - The failover process is complete and teams have verified services are running as expected. During the failover, we did observe a small amount of failed requests for the services indicated, but overall impact was very minimal. We aren't expecting any further customer impact. 
Our team is now performing the final actions in this maintenance and we're continuing to monitor.
Aug 31, 18:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Aug 31, 16:51 UTC
Scheduled - Start Time: 18:00 UTC
End Time: 19:00 UTC
During this time, our Engineering Team will be performing maintenance to failover an internal service from one cluster to another. 
T…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/cbhwcvtkfr34</id>
        <link href="https://www.githubstatus.com/incidents/cbhwcvtkfr34"/>
        <updated>2023-08-30T21:04:08.000Z</updated>
        <summary type="html"><![CDATA[Aug 30, 21:04 UTC
Resolved - This incident has been resolved.
Aug 30, 20:56 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/f7lz4nc72bvp</id>
        <link href="https://www.githubstatus.com/incidents/f7lz4nc72bvp"/>
        <updated>2023-08-30T20:53:20.000Z</updated>
        <summary type="html"><![CDATA[Aug 30, 20:53 UTC
Resolved - This incident has been resolved.
Aug 30, 20:42 UTC
Update - Webhook delays have been reduced significantly. We anticipate returning to nominal service soon.
Aug 30, 19:13 UTC
Update - The remediation for Webhook delivery delays has been implemented. We will continue to keep users updated on Webhook latency improvements.
Aug 30, 18:10 UTC
Update - We are addressing the source of Webhook delays, and will continue to keep users updated on progress towards mitigation.
Aug 30, 17:37 UTC
Update - We have identified the source of delay in webhook deliveries and are in the process of implementing a fix.
Aug 30, 16:49 UTC
Update - We are investigating increased delivery time issues with Webhooks. We will continue to keep users updated on progress towards mitigation.
Aug 30, 16:45 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with workflows and app modals]]></title>
        <id>https://status.slack.com//2023-08/52eff24d7806f1cf</id>
        <link href="https://status.slack.com//2023-08/52eff24d7806f1cf"/>
        <updated>2023-08-30T14:22:50.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 11:32 PM PDT on August 29, 2023 to 02:06 AM PDT on August 30, 2023, some users may have seen an internal server or connection error in workflow and app modals.


We determined that a code change inadvertently caused an issue where incorrect data was stored in our database. When a modal was opened it used this incorrect data and displayed a connection error to users.


The code change was reverted and a fix was deployed which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/wqv4yvlgtq62</id>
        <link href="https://www.githubstatus.com/incidents/wqv4yvlgtq62"/>
        <updated>2023-08-29T21:58:10.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 21:58 UTC
Resolved - This incident has been resolved.
Aug 29, 21:52 UTC
Update - GitHub webhooks are fully functional but can take up to 20 minutes to deliver. We are working on identifying the root cause of the increased latency.
Aug 29, 20:55 UTC
Update - The Webhooks backlog continues to process, and we anticipate continued latency while it clears.
Aug 29, 19:38 UTC
Update - The Webhooks backlog continues to process, and we anticipate continued latency while it clears.
Aug 29, 18:24 UTC
Update - We’ve mitigated the issue causing Webhooks latency and are monitoring processing of the backlog of queued requests.
Aug 29, 17:31 UTC
Update - We are investigating increased delivery time issues with Webhooks. We will continue to keep users updated on progress towards mitigation.
Aug 29, 17:29 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Less than 1% of users may be experiencing slowness when loading Slack]]></title>
        <id>https://status.slack.com//2023-08/094f489e39b57d9f</id>
        <link href="https://status.slack.com//2023-08/094f489e39b57d9f"/>
        <updated>2023-08-29T16:54:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 28, 2023 from 4:30 AM PDT to 10:30 AM PDT, less than 1% of Slack users encountered slowness when loading channels, DMs, and threads. Some impacted users were also unable to load the sidebar.


We found that a host that caches workspace member, channel, and integration data was impacted by timeout errors. This prevented users on some workspaces from routing to the host and, in turn, reaching their cached data. 


We restarted the affected host, cleared the associated cache for impacted workspaces, and asked affected users to reload their Slack apps. This combination of steps resolved the issue for all users. 


We're continuing to investigate what caused the timeout errors to prevent future reoccurrence.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/99rhjnxq99tp</id>
        <link href="https://www.githubstatus.com/incidents/99rhjnxq99tp"/>
        <updated>2023-08-29T16:16:46.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 16:16 UTC
Resolved - This incident has been resolved.
Aug 29, 16:05 UTC
Update - Intermittent failures with our resource provider prevented around 200 Codespaces from launching or resuming in Europe West and Southeast Asia. We have mitigated impact and continue to monitor.
Aug 29, 16:05 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/yyr2ghh113dj</id>
        <link href="https://www.githubstatus.com/incidents/yyr2ghh113dj"/>
        <updated>2023-08-29T15:35:49.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 15:35 UTC
Resolved - This incident has been resolved.
Aug 29, 15:25 UTC
Update - We are investigating issues with Codespaces in the Europe West and Southeast Asia geographic areas. Some users may not be able to connect to their Codespaces at this time. We will update on progress.
Aug 29, 15:13 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, Pull Requests and Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/spz29wshw9mh</id>
        <link href="https://www.githubstatus.com/incidents/spz29wshw9mh"/>
        <updated>2023-08-29T02:36:47.000Z</updated>
        <summary type="html"><![CDATA[Aug 29, 02:36 UTC
Resolved - This incident has been resolved.
Aug 29, 02:34 UTC
Update - Webhooks is operating normally.
Aug 29, 02:34 UTC
Update - Pull Requests is operating normally.
Aug 29, 02:34 UTC
Update - Actions is operating normally.
Aug 29, 02:15 UTC
Update - GitHub webhooks are delayed, and required pull-request checks and reviews are not triggered.
Aug 29, 02:14 UTC
Update - Webhooks is experiencing degraded performance. We are continuing to investigate.
Aug 29, 02:07 UTC
Update - Newly triggered Actions workflows are stalled. We identified the problem, and we are working on a mitigation.
Aug 29, 02:02 UTC
Update - Pull Requests is experiencing degraded performance. We are continuing to investigate.
Aug 29, 01:59 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App platform deployment and rollbacks in NYC region]]></title>
        <id>https://status.digitalocean.com/incidents/5zn0vljq3hnd</id>
        <link href="https://status.digitalocean.com/incidents/5zn0vljq3hnd"/>
        <updated>2023-08-27T12:22:38.000Z</updated>
        <summary type="html"><![CDATA[Aug 27, 12:22 UTC
Resolved - As of 12:22 UTC, our Engineering team has resolved the issue impacting the builds and rollbacks in our NYC region. Everything should be functioning normally and users should no longer experience issues with the builds and rollbacks being stuck.
We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.
Aug 27, 11:29 UTC
Monitoring - As of 11:28 UTC, our Engineering team has implemented a fix to resolve the issue with the App platform impacting the builds and rollbacks in our NYC region and monitoring the situation. At this time, users should no longer experience issues with active deployment and rollbacks being stuck. 
We will share another update once the matter is fully resolved.
Aug 27, 09:32 UTC
Update - As of 09:32 UTC, our Engineering team has identified the cause of the issue with the App platform impacting the builds and rollbacks in our NYC region and has implemented a fix that has partially mitigated the issue. Currently, our Engineering team is working on investigating the issue further and implementing a permanent fix.
We will post an update as soon as additional information is available.
Aug 27, 06:32 UTC
Identified - As of 06:21 UTC, our Engineering team has identified the cause of the issue with the App platform impacting the builds and rollbacks in our NYC region and is actively working on a fix. We will post an update as soon as additional information is available.
Aug 27, 05:35 UTC
Investigating - Our Engineering team is investigating an issue with App Platform builds and rollbacks in our NYC region. At this time, users may experience issues with active deployment and rollbacks being stuck.
We will post an update as soon as additional information is available.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be missing sidebar sections in Slack]]></title>
        <id>https://status.slack.com//2023-08/dd4ce480d5ff7e49</id>
        <link href="https://status.slack.com//2023-08/dd4ce480d5ff7e49"/>
        <updated>2023-08-25T21:31:10.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On August 24, 2023 between 10:41 AM PDT to 4:10 PM PDT, channel sections were missing from some users' sidebars.


We determined that a recent update in code failed to process an older format of data objects coming from our cache.


While we reverted this change and saw the majority of the issue resolved, this action caused an issue localized to the new data format and some users were still affected. 


Once we pushed a fix to normalize and handle all objects in the cache, channel sections were restored for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues uploading files.]]></title>
        <id>https://status.slack.com//2023-08/8fd09ebe40c64df0</id>
        <link href="https://status.slack.com//2023-08/8fd09ebe40c64df0"/>
        <updated>2023-08-25T20:32:40.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 4, 2023 from 8:04 AM PDT to 11:14 AM PDT, some users were unable to upload files in Slack.


We determined that a recent backend code change was erroneously detecting certain users as deactivated, which in turn was causing these users requests to upload files to be rejected. We reverted this change, which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DigitalOcean Docs (https://docs.digitalocean.com) Unavailable]]></title>
        <id>https://status.digitalocean.com/incidents/2z768rr0lytf</id>
        <link href="https://status.digitalocean.com/incidents/2z768rr0lytf"/>
        <updated>2023-08-24T17:14:15.000Z</updated>
        <summary type="html"><![CDATA[Aug 24, 17:14 UTC
Resolved - Our Engineering team identified an issue with the DigitalOcean Docs site (https://docs.digitalocean.com), resulting in all users attempting to view the main site or any docs pages seeing 500 errors. 
Beginning around 16:55 UTC, the Docs site became unavailable following a complication with a deploy. At 17:16, our Engineering team triggered a new deploy and the site came back online. 
We apologize for the disruption. If you have any questions or concerns, please reach out through a Support ticket from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[delayed webhook response time on eu1.make.com]]></title>
        <id>https://status.make.com/incidents/l2625994sxpw</id>
        <link href="https://status.make.com/incidents/l2625994sxpw"/>
        <updated>2023-08-24T10:12:34.000Z</updated>
        <summary type="html"><![CDATA[Aug 24, 12:12 CEST
Resolved - The incident has been resolved.
Aug 24, 11:39 CEST
Monitoring - A fix was applied and webhook response time is back at expected level. We are closely monitoring the situation.
Aug 24, 11:16 CEST
Investigating - Webhook response time might be delayed on eu1.make.com. Our team is currently investigating the issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Webhooks]]></title>
        <id>https://www.githubstatus.com/incidents/w86r7g2x32j1</id>
        <link href="https://www.githubstatus.com/incidents/w86r7g2x32j1"/>
        <updated>2023-08-23T16:23:11.000Z</updated>
        <summary type="html"><![CDATA[Aug 23, 16:23 UTC
Resolved - This incident has been resolved.
Aug 23, 15:56 UTC
Update - We are investigating and working towards mitigating slight delays in Webhook deliveries.
Aug 23, 15:38 UTC
Investigating - We are investigating reports of degraded performance for Webhooks]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HEIC Images Cannot be Rendered]]></title>
        <id>https://status.notion.so/incidents/bn3xy3vc4n90</id>
        <link href="https://status.notion.so/incidents/bn3xy3vc4n90"/>
        <updated>2023-08-23T15:30:00.000Z</updated>
        <summary type="html"><![CDATA[Aug 23, 08:30 PDT
Resolved - Between August 22nd at 11:25 AM PT and August 23rd at 8:39 AM PT, HEIC images were unable to render in Notion.
This incident has now been resolved. Users should now be able to render HEIC images.
Affected users will need to hard refresh their local cache to successfully load their images.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unveiling the newest upgrades to Smily's Automated Discount Codes 🚀]]></title>
        <id>272959</id>
        <link href="https://changelog.bookingsync.com/unveiling-the-newest-upgrades-to-smily's-automated-discount-codes-272959"/>
        <updated>2023-08-23T13:33:56.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
👉 After releasing the first version of the discount code feature, your invaluable feedback has spoken volumes, We want to express our gratitude for your involvement and your contribution 
Understanding the significance of the discount codes for you, we hosted dynamic workshops and engaged in insightful discussions with users like yourself. Your input has been important to add the following features designed to cater to your needs.
📈 Tracking discount codes:
Seamlessly track discount code usage and view the discounted booking list, using our discount code filtering feature!

📅 Eligible date:
Unlock the power of time-limited offers: Set eligible dates for booking discount codes to drive sales for specific seasons, create urgency, and maximize guest Engagement!

We invite you to discover all these new features, and we hope it will help you manage better your bookings, save time, and book more!
If you have any questions or need assistance, please don't hesitate to contact us through this form.
Wishing you all a pleasant day!]]></summary>
        <author>
            <name>Zahra, Product manager, PMS team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Login issues for Android customers using Intune and Edge]]></title>
        <id>https://status.slack.com//2023-08/cecb6f52bb0ace68</id>
        <link href="https://status.slack.com//2023-08/cecb6f52bb0ace68"/>
        <updated>2023-08-22T21:00:44.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

Beginning August 16th 2023 at 11:00 AM PDT, some Android users signing into the Slack via Microsoft Intune and Microsoft Edge, saw the error "For reasons unknown, we were unable to sign you into your workspace" or "Drat, your login didn't work".


We found the error is related to a web redirect issue with Slack and Microsoft Edge. 


We'll work to fix this internally through a bug ticket that has been filed as a result of this incident. Though we haven't resolved this issue yet, we will provide further updates through customer support tickets with the impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/smms25d68d09</id>
        <link href="https://www.githubstatus.com/incidents/smms25d68d09"/>
        <updated>2023-08-22T17:01:49.000Z</updated>
        <summary type="html"><![CDATA[Aug 22, 17:01 UTC
Resolved - This incident has been resolved.
Aug 22, 16:14 UTC
Update - Actions Larger Runners are experiencing delayed queue times. A mitigation is being worked on and will be applied shortly.
Aug 22, 15:59 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Workspace name appearing next to channel names]]></title>
        <id>https://status.slack.com//2023-08/91459fdf86fd71f7</id>
        <link href="https://status.slack.com//2023-08/91459fdf86fd71f7"/>
        <updated>2023-08-22T11:57:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 21, 2023 from 11:30 AM PDT to August 22, 2023 around 2:30 AM PDT, customers may have noticed the workspace name appearing beside the channel names.


The issue was caused by a product change that impacted our channel name display logic, causing workspace names to appear next to the channel names in the sidebar. 


We identified and introduced a fix, resolving the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some iOS users are experiencing errors when sending messages]]></title>
        <id>https://status.slack.com//2023-08/0e77cc5cab99dd94</id>
        <link href="https://status.slack.com//2023-08/0e77cc5cab99dd94"/>
        <updated>2023-08-22T04:22:28.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On August 21, 2023 from 3:33 PM PDT to around 5:41 PM PDT, customers using the iOS app may have experienced issues sending messages. 


A code change inadvertently introduced an error in our message validation logic, preventing the iOS app from sending messages on a first attempt. Some customers may have been able to resend messages successfully.


We identified and reverted the culprit code change, resolving the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-006]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-006/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-006/"/>
        <updated>2023-08-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: An issue in the Tailscale client, combined with a behavior
of the UPnP implementations in some routers, could expose all UDP ports of a
node to external networks (usually the internet).
As of 2023-08-22 2:30 AM UTC, we have changed the Tailscale coordination server
to advise nodes to stop using UPnP for port mapping. In some cases this can
degrade NAT traversal and may cause some connections to route through DERP.
This may increase node-to-node latency and decrease throughput. Version 1.48.1
resolves the issue and re-enables port mapping via UPnP.
What happened?
Tailscale nodes use UPnP as one of the mechanisms to open UDP port forwarding
in routers to help with NAT traversal. Tailscale picks a node port and an
external router port and requests forwarding between them. On firs…]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be having trouble loading Slack.]]></title>
        <id>https://status.slack.com//2023-08/d67213498dd5cc16</id>
        <link href="https://status.slack.com//2023-08/d67213498dd5cc16"/>
        <updated>2023-08-21T23:07:59.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 21, 2023 from 8:00 AM PDT to 8:51 AM PDT, less than 1% of users experienced trouble loading Slack.


We determined that a recent code change on our back end was resulting in a higher than expected memory footprint, and that this combined with high traffic loads was causing these issues.


We immediately rolled back this change, which resolved the issue for all affected users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/q8swpy90g6pp</id>
        <link href="https://www.githubstatus.com/incidents/q8swpy90g6pp"/>
        <updated>2023-08-21T18:41:06.000Z</updated>
        <summary type="html"><![CDATA[Aug 21, 18:41 UTC
Resolved - This incident has been resolved.
Aug 21, 18:23 UTC
Update - Mitigation is in place for delays in Actions workflows and should be fully recovered in the next 30 minutes.
Aug 21, 17:41 UTC
Update - We’re currently investigating reports of delays in Actions workflows.
Aug 21, 17:27 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Trouble loading Slack]]></title>
        <id>https://status.slack.com//2023-08/56ebd3ce13963c6a</id>
        <link href="https://status.slack.com//2023-08/56ebd3ce13963c6a"/>
        <updated>2023-08-18T01:00:12.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On August 17, 2023 from 9:20 AM PDT to 9:37 AM PDT, some users encountered issues with messages not sending and channels and threads failing to load.


We determined that a recent indexing change caused a higher than expected level of requests to our database. The resulting high load errors were responsible for the impacted users' inability to load messages and channels.


We immediately reverted this change, which resolved the issue for affected users. We then deployed a fix to prevent this from reoccurring in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We are investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/nj39bg7wbgdq</id>
        <link href="https://www.githubstatus.com/incidents/nj39bg7wbgdq"/>
        <updated>2023-08-17T19:28:50.000Z</updated>
        <summary type="html"><![CDATA[Aug 17, 19:28 UTC
Resolved - This incident has been resolved.
Aug 17, 19:12 UTC
Update - Intermittent large runners and self hosted runners issue has been mitigated
Aug 17, 19:12 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/jg4xwzwflb3r</id>
        <link href="https://www.githubstatus.com/incidents/jg4xwzwflb3r"/>
        <updated>2023-08-17T19:09:11.000Z</updated>
        <summary type="html"><![CDATA[Aug 17, 19:09 UTC
Resolved - This incident has been resolved.
Aug 17, 18:44 UTC
Update - Customers using large runners and self hosted runners may see delays over 10 minutes for actions runs. We are working on mitigating. This effects approximately 1% of customers.
Aug 17, 18:36 UTC
Investigating - We are investigating reports of degraded performance for Actions]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>