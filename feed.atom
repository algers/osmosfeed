<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-07-24T01:37:27.229Z</id>
    <title>osmos::feed</title>
    <updated>2023-07-24T01:37:27.229Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Maintenance - AT&T]]></title>
        <id>https://status.twilio.com/incidents/tcrh757jvtc1</id>
        <link href="https://status.twilio.com/incidents/tcrh757jvtc1"/>
        <updated>2023-07-24T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Jul 23, 20:00 PDT  -  Jul 28, 02:00 PDT
Jul 20, 08:55 PDT
Scheduled - The AT&T network in the US is conducting a series of planned maintenance from 23 July 2023 at 20:00 PDT until 28 July 2023 at 02:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from AT&T US handsets.

Note, the maintenance will be carried out on each of the following dates and times:

23 July 2023 at 20:00 PDT until 24 July 2023 at 02:00 PDT
24 July 2023 at 20:00 PDT until 25 July 2023 at 02:00 PDT
25 July 2023 at 17:00 PDT until 26 July 2023 at 00:00 PDT
26 July 2023 at 20:00 PDT until 27 July 2023 at 02:00 PDT
27 July 2023 at 20:00 PDT until 28 July 2023 at 02:00 PDT]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[US SMS and MMS Carrier Maintenance - T-Mobile US]]></title>
        <id>https://status.twilio.com/incidents/k85v7wcvcl89</id>
        <link href="https://status.twilio.com/incidents/k85v7wcvcl89"/>
        <updated>2023-07-24T01:01:18.000Z</updated>
        <summary type="html"><![CDATA[Jul 23, 18:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jul 22, 08:53 PDT
Scheduled - The T-Mobile network in the US is conducting an emergency maintenance from 23 July 2023 at 18:00 PDT until 24 July 2023 at 04:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS and MMS to and from T-Mobile US handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes]]></title>
        <id>https://status.twilio.com/incidents/mmc0p1yl14rb</id>
        <link href="https://status.twilio.com/incidents/mmc0p1yl14rb"/>
        <updated>2023-07-23T21:18:35.000Z</updated>
        <summary type="html"><![CDATA[Jul 23, 14:18 PDT
Resolved - We are no longer experiencing MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes. This incident has been resolved.
Jul 23, 13:15 PDT
Monitoring - We are observing a recovery in MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes.  We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Jul 23, 11:21 PDT
Update - We are still experiencing MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes. Our engineers are working with our carrier partner to resolve this issue. We will provide another update in 2 hours or as soon as more information becomes available.
Jul 23, 10:40 PDT
Update - We are experiencing MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.
Jul 23, 10:36 PDT
Investigating - Our monitoring systems have detected a potential issue with MMS Delivery Delays To AT&T in the US From a Subset of Shortcodes. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delayed scenario executions in the EU1 zone]]></title>
        <id>https://status.make.com/incidents/b17vq1m1ztj4</id>
        <link href="https://status.make.com/incidents/b17vq1m1ztj4"/>
        <updated>2023-07-23T14:00:00.000Z</updated>
        <summary type="html"><![CDATA[Jul 23, 16:00 CEST
Resolved - Executions of scheduled scenarios in the EU1 zone are delayed.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Option to unarchive private channels is missing]]></title>
        <id>https://status.slack.com//2023-07/87cc7f492c7c074a</id>
        <link href="https://status.slack.com//2023-07/87cc7f492c7c074a"/>
        <updated>2023-07-21T23:04:07.000Z</updated>
        <summary type="html"><![CDATA[We have identified a possible cause for the missing unarchive option for channels. We are pushing a fix and will update once we have confirmation that this option is working again.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some user unable to load profiles]]></title>
        <id>https://status.slack.com//2023-07/ca6b134d9fd2d5a4</id>
        <link href="https://status.slack.com//2023-07/ca6b134d9fd2d5a4"/>
        <updated>2023-07-21T16:12:39.000Z</updated>
        <summary type="html"><![CDATA[We’ve identified the cause of this issue. Any users encountering a REB9FC2A4F43 error can reload Slack (Command/Ctrl + Shift + R) in order to resolve this error. Thank you for your patience while we sorted out this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests, Git Operations, Packages, Pages and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/6503rcn8s34s</id>
        <link href="https://www.githubstatus.com/incidents/6503rcn8s34s"/>
        <updated>2023-07-21T14:01:48.000Z</updated>
        <summary type="html"><![CDATA[Jul 21, 14:01 UTC
Resolved - This incident has been resolved.
Jul 21, 14:01 UTC
Update - Actions is operating normally.
Jul 21, 14:01 UTC
Update - Packages is operating normally.
Jul 21, 14:01 UTC
Update - API Requests is operating normally.
Jul 21, 14:01 UTC
Update - Git Operations is operating normally.
Jul 21, 14:01 UTC
Update - Pages is operating normally.
Jul 21, 14:00 UTC
Update - Networking issues are fully mitigated and service operation is restored. We will continue to work on follow ups internally but all services are operating normally.
Jul 21, 13:45 UTC
Update - We have mitigated the network power issue and are watching services recover. We will continue to monitor recovery and return status to green when the issue is fully mitigated.
Jul 21, 13:36 UTC
Update - API Requests is experiencing degraded performance. We are continuing to investigate.
Jul 21, 13:34 UTC
Update - We are experiencing a power failure at a datacenter related to our networking infrastructure that is impacting a subset of customers using our services. We are working on mitigating the issue.
Jul 21, 13:27 UTC
Update - Pull Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
Jul 21, 13:22 UTC
Update - Pages is experiencing degraded performance. We are still investigating and will provide an update when we have one.
Jul 21, 13:21 UTC
Update - Packages is experiencing degraded performance. We are still investigating and will provide an update when we have one.
Jul 21, 13:15 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Jul 21, 13:12 UTC
Investigating - We are investigating reports of degraded performance for Git Operations.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some users prevented from accessing Notion due to change in request cookie consent]]></title>
        <id>https://status.notion.so/incidents/ps6b5r9s5sfk</id>
        <link href="https://status.notion.so/incidents/ps6b5r9s5sfk"/>
        <updated>2023-07-20T23:05:12.000Z</updated>
        <summary type="html"><![CDATA[Jul 20, 16:05 PDT
Resolved - This incident has been resolved.
Jul 20, 15:30 PDT
Monitoring - We published a fix and validating the problem is fixed
Jul 20, 14:46 PDT
Identified - A change to request cookie consent is preventing users from accessing Notion. We have identified the issue and are working on a fix.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces directories creation]]></title>
        <id>https://status.digitalocean.com/incidents/hwy11pmr6hgl</id>
        <link href="https://status.digitalocean.com/incidents/hwy11pmr6hgl"/>
        <updated>2023-07-20T22:31:03.000Z</updated>
        <summary type="html"><![CDATA[Jul 20, 22:31 UTC
Resolved - As of 21:52 UTC, our Engineering team has confirmed the full resolution of the issue impacting the creation of directories/folders/files in Spaces buckets via the Cloud Control Panel. 
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jul 20, 22:05 UTC
Monitoring - As of 21:32 UTC, our Engineering team has implemented a fix for the issue impacting the creation of directories/folders/files in Spaces buckets in the Cloud Control Panel.
We're now monitoring the situation closely. We will post an update as soon as the issue is fully resolved.
Jul 20, 21:39 UTC
Update - Our Engineering team is investigating an issue with creating directories/folders/files in Spaces buckets using the Cloud Control Panel.
During this time, users should still be able to create the Spaces Bucket using Control Panel and API.
We will share an update as soon as possible.
Jul 20, 21:38 UTC
Investigating - Our Engineering team is investigating an issue with creating directories/folders/files in Spaces buckets using the Cloud Control Panel.
During this time, users should still be able to create the Spaces Bucket using Control Panel and API.
We will share an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Users cannot access any exported files from pages or workspaces]]></title>
        <id>https://status.notion.so/incidents/g2ywgyryp910</id>
        <link href="https://status.notion.so/incidents/g2ywgyryp910"/>
        <updated>2023-07-20T22:07:47.000Z</updated>
        <summary type="html"><![CDATA[Jul 20, 15:07 PDT
Resolved - This incident has been resolved.
Jul 20, 14:57 PDT
Investigating - Users are unable to export workspace and page files. We are currently investigating the issue]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We're investigating reports of an issue with Gmail. We will provide more information shortly. The affected users are able to access Gmail, but are seeing error messages, high latency, and/or other unexpected behavior.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/aw35PEC9zRoTE8rNHWL6</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/aw35PEC9zRoTE8rNHWL6"/>
        <updated>2023-07-20T01:31:08.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-07-06 22:22</strong> and ended at <strong>2023-07-11 02:19</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Incident Report</h1>
<h2>Summary</h2>
<p>On Thursday, 6 July 2023, Gmail experienced elevated errors in some of its core functionalities due to internal task issues. This occurred for 1 hour and 27 minutes starting at 15:22 PT. On Monday, 10 July 2023, the impact reoccurred for a period of 42 minutes between 18:37 to 19:19. During the period of impact,  some Gmail users experienced intermittent unavailability to a number of services, including syncing email to their clients, uploading attachments, initial page loads, and email deliveries. To our Gmail users who were impacted during this disruption, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform’s performance and availability.</p>
<h2>Root Cause</h2>
<p>Gmail uses an internal triggering process system, built on an internal Spanner queue receiver, to perform asynchronous processing of the incoming messages in the Gmail backend.</p>
<p>The root cause of the incident is due to an unintended configuration change made in one of the internal proxy layers that handles Spanner queue receivers. However, it inadvertently reduced the callback size limit of the Spanner queue receiver resulting in throttling of the queue, which caused elevated memory pressure of the backend servers while handling the backlog of messages.</p>
<p>Google engineers mitigated the issue by rolling back the incorrect change and performed a controlled release of the messages to ensure the backend servers were resourced.</p>
<h2>Remediation and Prevention</h2>
<p>On Thursday July 6, 2023 at 15:22, Google engineers were alerted to the issue from the monitoring tools in place, and started working on multiple mitigation strategies to restore the service.</p>
<p>There was a spike in traffic from a single problematic trigger, which caused memory pressures on the backend servers. Google engineers paused the processing of this trigger and implemented temporary throttles and began to slowly ramp up the processing of this trigger to clean up the backlog of tasks.</p>
<p>On Monday July 10, 2023, Google engineers noticed that an incorrect configuration change had been rolled out after the issue on Thursday that caused a recurrence. Google engineers implemented a controlled ramp of the processing of the trigger, similar to the mitigation that was performed on July 6. However, one partition of production traffic was missed during this procedure and caused follow-on impact starting at 18:47. To prevent further user impact, engineers paused the processing of the trigger again, and as a result user impact began to subside at around 19:15. Google engineers performed cleanup of this trigger at a controlled pace over the course of the next few weeks.</p>
<p>Google is committed to preventing a repeat of this issue in the future. Google engineers have set up a conservative limit for throttling the incoming load. This is to ensure the memory utilization is within the limits.</p>
<p>Google is also completing the following actions:</p>
<ul>
<li>Implement additional safeguards around change management while system improvements are being developed</li>
</ul>
<h2>Detailed Description of Impact</h2>
<p>On Thursday, 6 July 2023 between 15:15 and 16:47 PT, a portion of Gmail users experienced some IMAP unavailability, which resulted in asynchronous syncing issues between their clients and their mailboxes. Users may have experienced errors when syncing mail and with settings in their clients, issues with initial page loads, one or more errors accessing Gmail’s external API through the API Server, an inbound delivery delay, or an outbound delay.</p>
<p>On Monday, 10 July 2023 between 18:37 and 19:19, a portion of Gmail users received errors attempting to sync their clients to their mailboxes, upload attachments and load ads, or load their web client for the first time. Meanwhile some users experienced IMAP errors, errors from Sync Server, and unavailability when accessing the API Server. Email delivery was also impacted, with some inbound emails temporarily delayed.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/shbq71s0g00z</id>
        <link href="https://www.githubstatus.com/incidents/shbq71s0g00z"/>
        <updated>2023-07-18T16:44:11.000Z</updated>
        <summary type="html"><![CDATA[Jul 18, 16:44 UTC
Resolved - This incident has been resolved.
Jul 18, 16:43 UTC
Update - Pull Requests is operating normally.
Jul 18, 16:34 UTC
Update - We have mitigated the issue causing slow git operations for some customers and operations should now be performing normally.
Jul 18, 16:05 UTC
Update - Some customers may be experiencing slow git operations. We have identified the cause of the issue and are working on a mitigation.
Jul 18, 15:51 UTC
Update - Pull Requests is experiencing degraded performance. We are still investigating and will provide an update when we have one.
Jul 18, 15:44 UTC
Investigating - We are investigating reports of degraded performance for Git Operations.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Clusters]]></title>
        <id>https://status.digitalocean.com/incidents/04y93nmb1blj</id>
        <link href="https://status.digitalocean.com/incidents/04y93nmb1blj"/>
        <updated>2023-07-14T15:25:24.000Z</updated>
        <summary type="html"><![CDATA[Jul 14, 15:25 UTC
Resolved - From 14:53 - 15:26 UTC, we experienced an issue with Kubernetes clusters and our Kubernetes API. During that time, users would have experienced errors while communicating with their clusters , as well as seen errors while sending requests to API endpoints. Errors to the API manifested as 404's on /v2/kubernetes/clusters endpoints. 
Our Engineering team was able to take swift action to roll back a breaking change, remediating the incident. 
We apologize for the disruption in service. If you are still experiencing any problems or have additional questions, then please open a support ticket within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with Facebook Lead Ads - New Event instant trigger]]></title>
        <id>https://status.make.com/incidents/lpshgnkxkz7n</id>
        <link href="https://status.make.com/incidents/lpshgnkxkz7n"/>
        <updated>2023-07-14T04:49:33.000Z</updated>
        <summary type="html"><![CDATA[Jul 14, 06:49 CEST
Resolved - This incident was resolved by hotfix.
Jul 13, 15:39 CEST
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with loading threads and accessing Slack]]></title>
        <id>https://status.slack.com//2023-07/c77cf2eeb9a222df</id>
        <link href="https://status.slack.com//2023-07/c77cf2eeb9a222df"/>
        <updated>2023-07-13T21:25:19.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

On July 13, 2023 between 12:19 PM PDT and 12:36 PM PDT, some users encountered errors which led to being unable to load threads, send DMs, joining channels, and accessing Slack overall.


This issue was a result of a failed host in our database infrastructure. We quickly replaced this host, fixing the problem for all impacted users.


Users should no longer be experiencing any trouble accessing Slack.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Email sent erroneously regarding work email domain verification]]></title>
        <id>https://status.rippling.com/incidents/nxj258xcyfmh</id>
        <link href="https://status.rippling.com/incidents/nxj258xcyfmh"/>
        <updated>2023-07-13T20:31:10.000Z</updated>
        <summary type="html"><![CDATA[Jul 13, 20:31 UTC
Resolved - This incident has been resolved.
Jul 13, 18:45 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Jul 13, 18:31 UTC
Identified - Select customers have received an email with subject "Domain Verification failed". This email was sent erroneously and isn't applicable to any customer who has received it, and it can be ignored.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservations Missing Thread Id Values]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/l6f622b5k1t3</id>
        <link href="https://airbnbapi.statuspage.io/incidents/l6f622b5k1t3"/>
        <updated>2023-07-13T16:22:18.000Z</updated>
        <summary type="html"><![CDATA[Jul 13, 09:22 PDT
Resolved - We have backfilled message threads for the reservations that were affected during this time window. You should now be able to retrieve the `thread_id` associated with a reservation using the Reservations API. Apologies for the inconvenience.
If you have any further queries related to this incident, please submit a ticket via the Partner Support Portal and our team will get back to you as soon as possible.
Jul 12, 12:25 PDT
Identified - We have identified an issue that caused reservations to be created without associated threads. This occurred between 10:15 PM and 11:29 PM on July 12th (PST), so notifications for reservations created during that time were sent with a null `thread_id` value.
We are actively working on creating threads for reservations that do not have them, and will update here afterwards.
Once this is done, you will be able to retrieve the `thread_id` value for a reservation using the Reservations API.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with executing scenarios]]></title>
        <id>https://status.make.com/incidents/ym8fpgs8rb0j</id>
        <link href="https://status.make.com/incidents/ym8fpgs8rb0j"/>
        <updated>2023-07-13T16:08:31.000Z</updated>
        <summary type="html"><![CDATA[Jul 13, 18:08 CEST
Resolved - This incident has been resolved.
Jul 13, 15:45 CEST
Monitoring - We have applied the fix to reactivate the affected scenarios. 
Please note that this reactivation will not be visible in the scenario logs.
Currently the platfrom is fully operational but we will continue to monitor situation for the next couple of hours.
Jul 13, 13:11 CEST
Identified - The issue has been identified and we are currently working on implementing the fix to reactivate the affected scenarios.
We'll post the next update once the fix is rolled out.
Jul 13, 12:10 CEST
Update - We have identified the cause of the issue. Any newly executed scenarios and scheduled scenarios should work correctly. However, as part of the investigation we noticed the issue affected other zones: us1.make.com and eu2.make.com. 
We are currently working on reactivating the affected scenarios.
Jul 13, 11:22 CEST
Investigating - We are facing issues with scenario execution affecting users on eu1.make.com. Scenarios might unexpectedly fail, users might receive notification about scenarios being stopped, We are currently investigating the cause of the issue, we will provide more details within the next hour.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Billing page on the Cloud Panel]]></title>
        <id>https://status.digitalocean.com/incidents/k6db523qf5gw</id>
        <link href="https://status.digitalocean.com/incidents/k6db523qf5gw"/>
        <updated>2023-07-13T02:33:27.000Z</updated>
        <summary type="html"><![CDATA[Jul 13, 02:33 UTC
Resolved - Our engineering team has resolved the issue with the Billing page for Team accounts in the Cloud Control Panel. 
Thank you for your patience. If you continue to experience any problems, please open a support ticket from within your account.
Jul 13, 01:33 UTC
Monitoring - As of 01:20 UTC, Our Engineering team has implemented a fix regarding the issue with the Billing page for Team accounts in the Cloud Control Panel. We are monitoring the situation to ensure there is no recurrence. 
We will post another update once we confirm the issue is fully resolved.
Jul 13, 01:27 UTC
Investigating - As of 01:05 UTC, Our Engineering team is investigating an issue with the Billing page for Teams on the Cloud Control Panel. During this time, users on Team accounts may experience errors while accessing the Billing page. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/x4llkxfv8kn4</id>
        <link href="https://www.githubstatus.com/incidents/x4llkxfv8kn4"/>
        <updated>2023-07-12T18:08:00.000Z</updated>
        <summary type="html"><![CDATA[Jul 12, 18:08 UTC
Resolved - This incident has been resolved.
Jul 12, 17:59 UTC
Update - We have identified a problematic configuration change that caused Codespaces creations and resumes to fail. The change has been reverted and users should start seeing improvements soon.
Jul 12, 17:38 UTC
Investigating - We are investigating reports of degraded performance for Codespaces.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Websites security improvement]]></title>
        <id>269393</id>
        <link href="https://changelog.bookingsync.com/websites-security-improvement-269393"/>
        <updated>2023-07-12T08:31:36.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
We would like to inform you that we will deploy a nice improvement to your websites.
Starting today, all Smily websites will be under SSL certificates.
There is nothing you have to do on your side, this change will be automatic and shall not impact your website’s accessibility.
What does it mean ?
Your website’s URL will be starting with https:// and no longer http://.
What are the benefits of this change ?
Websites are more secured
Better user experience
Users stay longer and buy more on secured websites
Sites with HTTPS load faster
HTTPS leads to accurate SEO reporting
Better SEO : SSL is a positive ranking factor for Google. Google’s team has expressed the need for HTTPS time and time again. So much so that they’ve released an algorithm update based on it—causing sites without HTTPS security to struggle on their quest to rank highly in the search engines results.
Feel free to reach out to us if you have any question !]]></summary>
        <author>
            <name>Yannick, Customer Care Team Leader - Pro Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smily Summer Success Strategies: Tips and Insights for a memorable Season! ☀️]]></title>
        <id>269321</id>
        <link href="https://changelog.bookingsync.com/smily-summer-success-strategies-tips-and-insights-for-a-memorable-season!-269321"/>
        <updated>2023-07-11T16:33:43.000Z</updated>
        <summary type="html"><![CDATA[Communications
  
As the temperature rises and your guests start packing, the Smily team wanted to take a moment to wish you a wonderful summer season. Our dedicated team is here to support you at every step during the 2023 summer season! 🎉
At Smily, we value your partnership and understand the challenges that come with managing vacation rentals. As we enter this busy period together, we would like to share some last-minute tips that can help ensure a successful season for all.
Embrace multidiffusion:
As many of you may have noticed, the number of reservations received from Airbnb has been on the decline recently. This trend has been noticed by many in the industry. To counter this we highly recommend taking advantage of our integrations with Holidu, Maeva, Locasun/Leboncoin, Cdiscount, a…]]></summary>
        <author>
            <name>Ella, Head of Customer Care and OnBoarding team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble connecting to Slack and performing some actions]]></title>
        <id>https://status.slack.com//2023-07/0f34cdaa00a02b51</id>
        <link href="https://status.slack.com//2023-07/0f34cdaa00a02b51"/>
        <updated>2023-07-11T07:04:29.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On July 10, 2023 from 7:08 PM PDT to 7:58 PM PDT, some customers may have experienced various errors while using Slack.


A code change inadvertently added extra load to our server infrastructure. We increased our server capacity to handle this extra load and error rates returned to normal.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/hz717kx2gsvq</id>
        <link href="https://www.githubstatus.com/incidents/hz717kx2gsvq"/>
        <updated>2023-07-07T17:15:00.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 17:15 UTC
Resolved - This incident has been resolved.
Jul  7, 16:36 UTC
Update - The fix has been confirmed to work in production and is in the process of being deployed to all users.
Jul  7, 16:14 UTC
Update - We have identified a bug with Pull Requests where "change base branch" dropdown menu is not responding. We are in the process of shipping a fix.
Jul  7, 15:53 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues with text previews]]></title>
        <id>https://status.slack.com//2023-07/b0781e1094d3cd69</id>
        <link href="https://status.slack.com//2023-07/b0781e1094d3cd69"/>
        <updated>2023-07-06T22:58:08.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On July 6, 2023 between 06:21 AM PDT and 10:58 AM PDT, some users were experiencing issues with messages sent with an attached file and app mention. They were incorrectly displaying an error "This message contains interactive elements".


As we looked into it further, we traced the problem to a formatting issue, which resulted in missing metadata for the messages sent. Reverting a recent change fixed the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signups affected for some new users]]></title>
        <id>https://status.notion.so/incidents/zqrrhnc8xcpb</id>
        <link href="https://status.notion.so/incidents/zqrrhnc8xcpb"/>
        <updated>2023-07-06T19:48:27.000Z</updated>
        <summary type="html"><![CDATA[Jul  6, 12:48 PDT
Resolved - A recent change affected signups for some new users. This has been rolled back and signups for new users should work correctly. This incident has been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Creation in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/m4nw5210sd5t</id>
        <link href="https://status.digitalocean.com/incidents/m4nw5210sd5t"/>
        <updated>2023-07-05T19:58:43.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 19:58 UTC
Resolved - Our Engineering team have confirmed the issue with creation of Droplets and Droplet-based products is fully resolved. As of 18:15 UTC the fix was applied to resolve the problem and impact was subsided. All the events should now be processing normally in NYC3 region as the services are completely restored. 
Thank you for your patience. If you continue to experience any problems, please open a support ticket from within your account.
Jul  5, 18:47 UTC
Monitoring - Our Engineering team identified the root cause and implemented a fix to resolve the issue with Droplet creation in our NYC3 region. Users should now be able to create Droplets and Droplet-based services in the region. 
We are monitoring the situation closely and will post an update once the matter is completely resolved.
Jul  5, 17:23 UTC
Identified - Our Engineering team has identified the cause of the issue impacting Droplet creation in the NYC3 region and is actively working on a fix.
During this time, users may still experience latency and related issues while deploying Droplets and Droplet-based services in the NYC3 region. We will post an update as soon as additional information is available.
Jul  5, 16:27 UTC
Investigating - Our Engineering team is investigating an issue with Droplet creation in our NYC3 region. 
As of 15:50 UTC, users may see latency or errors when creating Droplets and Droplet-based services like Load Balancers, Managed Databases, and Kubernetes clusters, via Cloud Control Panel. Users will also be experiencing errors or slow processing of the API calls as well.
 We apologize for inconvenience and will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRA1 Power Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/ydd314cpt9fk</id>
        <link href="https://status.digitalocean.com/incidents/ydd314cpt9fk"/>
        <updated>2023-07-05T14:00:50.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 14:00 UTC
Completed - The scheduled maintenance has been completed.
Jul  5, 10:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jul  4, 16:53 UTC
Scheduled - Start: 2023-07-05  10:00 AM UTC
End: 2023-07-05   14:00 PM UTC 
Hello, 
During the above window, our Engineering team will be performing maintenance on redundant upstream electrical systems in order to improve reliability in our FRA1 region.
Expected Impact:
We don't expect to see any downtime as the power to hypervisors are redundant. In a highly unlikely situation, where we lose power to both the supplies, the Droplets in FRA1 may experience downtime for a brief period. We will endeavor to keep this to a minimum for the duration of the change.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket
Thank you,
Team DigitalOcean]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/pr0ptqcw6q8d</id>
        <link href="https://www.githubstatus.com/incidents/pr0ptqcw6q8d"/>
        <updated>2023-07-05T11:37:28.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 11:37 UTC
Resolved - This incident has been resolved.
Jul  5, 11:35 UTC
Update - Pull Requests is continuing to operate normally following earlier issues.
We are synchronising impacted pull requests in the background. If there are any further issues we will status again.
Jul  5, 11:22 UTC
Update - Reprocessing of unprocessed pushes from earlier continues. We will continue to provide progress updates.
Reminder that all new pushes are working as usual.
Jul  5, 10:30 UTC
Update - We have identified impacted pushes to repositories and are reprocessing these. We will provide updates as we progress.
As previously stated, all new pushes will be processed as usual.
Jul  5, 09:55 UTC
Update - We are working to identify all impacted pushes. Once identified, we will reprocess these. We will provide updates as we progress.
As previously stated, all new pushes will be processed as usual.
Jul  5, 07:09 UTC
Update - We have identified an issue that caused pull requests to not reflect additional git pushes over the last several hours. New pushes will now be processed as normal.
Jul  5, 05:54 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in AMS and FRA]]></title>
        <id>https://status.digitalocean.com/incidents/dpfhm7hf06s3</id>
        <link href="https://status.digitalocean.com/incidents/dpfhm7hf06s3"/>
        <updated>2023-07-03T16:52:21.000Z</updated>
        <summary type="html"><![CDATA[Jul  3, 16:52 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting networking in our AMS and FRA regions. If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jul  3, 13:25 UTC
Investigating - Our Engineering team is investigating an issue impacting networking in our AMS and FRA regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in these regions.
At the moment, all Droplet-based services appear to be impacted and users can expect to see brief connectivity issues and interrupted traffic flows.  We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may experience slow-to-load threads]]></title>
        <id>https://status.slack.com//2023-06/838a7ee19a81940e</id>
        <link href="https://status.slack.com//2023-06/838a7ee19a81940e"/>
        <updated>2023-07-03T15:53:07.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 

On Wednesday June 28th, 2023 from 10:35 AM PDT to 1:55 PM PDT, some customers experienced delays in loading threads. This was caused by a strain on our databases due to some unusually data-heavy tasks running in the background.


We stopped the tasks that caused the strain — which resolved the problem for impacted users. Thank you for your patience while we fixed this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with Slack API Website]]></title>
        <id>https://status.slack.com//2023-06/469a7534a9fd990a</id>
        <link href="https://status.slack.com//2023-06/469a7534a9fd990a"/>
        <updated>2023-07-03T15:33:08.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 

On June 29, 2023 between 12:12 PM PDT and 1:47 PM PDT some users had trouble loading api.slack.com and API documentation was unavailable. This issue was caused due to a code change that resulted in api.slack.com to be broken for users that were not logged in. We quickly reverted the change — resolving the issue for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/mc06mgh6qv9r</id>
        <link href="https://www.githubstatus.com/incidents/mc06mgh6qv9r"/>
        <updated>2023-07-03T13:47:36.000Z</updated>
        <summary type="html"><![CDATA[Jul  3, 13:47 UTC
Resolved - This incident has been resolved.
Jul  3, 13:22 UTC
Update - We are aware of an increase in 5xx errors for Pull Requests. Our team has been working on mitigation for the past 40 minutes and expects to see full recovery in the next 20 minutes.
Jul  3, 13:18 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing increased loading time or delays when switching between channels]]></title>
        <id>https://status.slack.com//2023-06/a8931305c8cbcf0e</id>
        <link href="https://status.slack.com//2023-06/a8931305c8cbcf0e"/>
        <updated>2023-07-03T09:54:10.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 09:27 PDT on Friday June 23th 2023 to 14:23 PDT the same day, some customers experienced delays in loading messages and conversations. This was caused by a slight strain on our databases due to some unusually data heavy tasks running in the background.


We stopped the tasks that caused that strain, which resolved the problem for impacted users.


Thank you for your patience while we fixed this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble logging in]]></title>
        <id>https://status.slack.com//2023-06/14c5fc2b58f7c4eb</id>
        <link href="https://status.slack.com//2023-06/14c5fc2b58f7c4eb"/>
        <updated>2023-07-03T05:30:19.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On June 30, 2023 from 8:12 AM PDT to 4:23 PM PDT, some users may have had trouble signing in to Slack with magic login codes or via the 'Sign In With Email' button. 


A code change inadvertently introduced a logic error in the sign-in flow. We identified the error and deployed an initial fix to correct the flawed logic. This resolved the problem with the 'Sign In With Email' button. After further investigation, we deployed a second correction to address the trouble with magic login codes, resolving the issue for all affected customers.


We believed the first fix had resolved the problem for all customers, and we updated the status site accordingly. However, we discovered that the issue with magic login codes had not been addressed, and we posted a separate notice on the status site while we investigated further. We apologize for any confusion and for calling the all clear too quickly. 


We have updated the titles for both entries so that they match.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble logging in]]></title>
        <id>https://status.slack.com//2023-06/e4f9c32806cc233f</id>
        <link href="https://status.slack.com//2023-06/e4f9c32806cc233f"/>
        <updated>2023-07-03T05:28:56.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On June 30, 2023 from 8:12 AM PDT to 4:23 PM PDT, some users may have had trouble signing in to Slack with magic login codes or via the 'Sign In With Email' button. 


A code change inadvertently introduced a logic error in the sign-in flow. We identified the error and deployed an initial fix to correct the flawed logic. This resolved the problem with the 'Sign In With Email' button. After further investigation, we deployed a second correction to address the trouble with magic login codes, resolving the issue for all affected customers.


We believed the first fix had resolved the problem for all customers, and we updated the status site accordingly. However, we discovered that the issue with magic login codes had not been addressed, and we posted a separate notice on the status site while we investigated further. We apologize for any confusion and for calling the all clear too quickly. 


We have updated the titles for both entries so that they match.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple services down and API availability]]></title>
        <id>https://status.digitalocean.com/incidents/k8ll5g9988gt</id>
        <link href="https://status.digitalocean.com/incidents/k8ll5g9988gt"/>
        <updated>2023-07-02T10:01:03.000Z</updated>
        <summary type="html"><![CDATA[Jul  2, 10:01 UTC
Resolved - Our Engineering team has confirmed that the issue impacting Public API and multiple services has been fully resolved. 
Users should no longer experience issues with Billing services, new User registrations, and Droplet event processing. All services related to the App platform, Managed Kubernetes, Container Registry, and Block Storage Volumes should be functioning normally.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. Thank you for your patience.
Jul  2, 09:38 UTC
Update - Our Engineering team had already deployed a fix earlier and that has mitigated the issue impacting Public API and multiple services. 
As of 09:20 UTC, our Engineering team found issues related to Billing services whic…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Webhook Migration Enforcement - Phase 3]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/3f2h5b7dp3vj</id>
        <link href="https://airbnbapi.statuspage.io/incidents/3f2h5b7dp3vj"/>
        <updated>2023-07-01T07:01:10.000Z</updated>
        <summary type="html"><![CDATA[Jul  1, 00:01 PDT
Completed - The scheduled maintenance has been completed.
Jun 26, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 23, 08:32 PDT
Scheduled - The deprecation date for API Webhook Version 2021.12.31 was May 31, 2023. Any applications that have not fully migrated webhooks will be automatically migrated to a newer version, which can result in application breakage.
Automatic webhook migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 3 will occur during the week of June 26th, 2023 and will automatically migrate all Critical booking sync check webhooks:
- check_availability
- make_reservation
- update_reservation]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces availability in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/qj8dv6r1wdc8</id>
        <link href="https://status.digitalocean.com/incidents/qj8dv6r1wdc8"/>
        <updated>2023-06-30T05:32:48.000Z</updated>
        <summary type="html"><![CDATA[Jun 30, 05:32 UTC
Resolved - Our engineering team has resolved the issue impacting Spaces in our NYC3 region. From approximately 3:41 UTC - 5:10 UTC, users may have encountered errors with API or object requests, experienced difficulties in creating new buckets in NYC3, or faced issues with loading Spaces in the Cloud Control Panel. Spaces should now be operating normally. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Jun 30, 05:13 UTC
Identified - Our engineering team has identified the cause of the issue with Spaces availability in our NYC3 region and is actively working on a fix. We will post an update as soon as additional information is available.
Jun 30, 03:41 UTC
Investigating - As of 03:15 UTC, Our Engineering team is investigating a drop in the availability for Spaces in our NYC region. During this time, some users may experience errors with API or object requests, be unable to create new buckets in NYC3, and/or see issues with loading Spaces in the Cloud Control Panel. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support Center]]></title>
        <id>https://status.digitalocean.com/incidents/tgqy43vl1mhf</id>
        <link href="https://status.digitalocean.com/incidents/tgqy43vl1mhf"/>
        <updated>2023-06-29T22:31:18.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 22:31 UTC
Resolved - Our vendor has confirmed resolution of this issue and the Support Center is functioning normally as of approximately 11:00 UTC. 
Users can access the Support Center and submit tickets normally. We appreciate your patience as we worked through this issue and apologize for the disruption. 
If you continue to experience problems, please open a ticket with our support team from the contact form.
Jun 29, 13:19 UTC
Update - Our Engineering team has confirmed with our vendor that the Support Center is accessible once more. At this time, users should be able to access the Support Center normally through their DigitalOcean account. 
Our team and the vendor are continuing to monitor this issue. We will post further updates as we have new information or once we confirm th…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[macOS agent installs will fail when Find My Mac is enabled]]></title>
        <id>https://status.rippling.com/incidents/r0c40mv4dpzs</id>
        <link href="https://status.rippling.com/incidents/r0c40mv4dpzs"/>
        <updated>2023-06-29T22:25:26.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 22:25 UTC
Resolved - Service is operating normally
Jun 29, 22:15 UTC
Monitoring - We are rolling out the fix to production and normal operation is resuming.
Jun 29, 20:33 UTC
Identified - The issue has been identified and a fix is being implemented.
Jun 29, 20:31 UTC
Investigating - Existing agents will be slow to update app.rippling.com when Find My Mac is enabled]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/jxchgkbpq0pg</id>
        <link href="https://status.digitalocean.com/incidents/jxchgkbpq0pg"/>
        <updated>2023-06-29T19:03:43.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 19:03 UTC
Resolved - As of 18:36 UTC, GitHub has recovered and is operating normally. App Platform users with source code hosted in GitHub should no longer experience any errors while deploying their Apps.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
Jun 29, 18:09 UTC
Monitoring - As of 17:52 UTC, our App Platform service might experience errors due to an ongoing GitHub incident. During this time, App Platform users with source code hosted in GitHub will have their App builds failed and users will not be able to create new Apps based on GitHub sources. Additionally, users who reference GitHub source code amongst their dependencies may see those dependencies fail to fetch.
Please visit the below link for more information on the GitHub incident.
https://www.githubstatus.com/incidents/gqx5l06jjxhp
We're monitoring Github's incident and will post any relevant updates here.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident On 2023-06-29]]></title>
        <id>https://www.githubstatus.com/incidents/gqx5l06jjxhp</id>
        <link href="https://www.githubstatus.com/incidents/gqx5l06jjxhp"/>
        <updated>2023-06-29T18:36:19.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 18:36 UTC
Resolved - From 17:39-18:12 UTC GitHub was down in parts of North America, particularly the US East coast, and South America. 
GitHub takes measures to ensure that we have redundancy in our system for various disaster scenarios. We have been working on building redundancy to an earlier single point of failure in our network architecture at a second Internet edge facility. This second Internet edge facility was completed in January and has been actively routing production traffic since then. Today we were performing a live failover test to validate that we could in fact use this second Internet edge facility if the primary were to fail. Unfortunately, during this failover we inadvertently caused a production outage.
During the test we exposed that the secondary site had a …]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/xqskfvqm07w3</id>
        <link href="https://www.githubstatus.com/incidents/xqskfvqm07w3"/>
        <updated>2023-06-29T17:19:31.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 17:19 UTC
Resolved - This incident has been resolved.
Jun 29, 16:30 UTC
Update - The third party provider GitHub uses is continuing to work through the issue that is causing delays for some customers with Actions Larger Runners and has started applying mitigations.
Jun 29, 15:45 UTC
Update - The third party provider GitHub uses is continuing to work through the issue that is causing delays for some customers with Actions Larger Runners and is seeking to provide a mitigation soon.
Jun 29, 15:05 UTC
Update - A third party provider GitHub uses has identified an issue and is working on recovery over the next hour. Standard Hosted Runners and Self Hosted Runners should not be impacted.
Jun 29, 15:02 UTC
Update - GitHub Actions customers using Windows and Linux Larger Runners may be experiencing longer than normal queue times.
Jun 29, 14:51 UTC
Investigating - We are investigating reports of degraded performance for Actions.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests and Pages]]></title>
        <id>https://www.githubstatus.com/incidents/ws2l6203cs7g</id>
        <link href="https://www.githubstatus.com/incidents/ws2l6203cs7g"/>
        <updated>2023-06-29T06:43:57.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 06:43 UTC
Resolved - This incident has been resolved.
Jun 29, 06:41 UTC
Update - With the fix deployed, we are now seeing recovery across impacted scenarios.
Jun 29, 06:05 UTC
Update - We are continuing to rollout the fix for failing Actions jobs using the deployment environments feature. We are expecting the remaining rollout to take another hour
Jun 29, 05:24 UTC
Update - We are continuing to monitor the rollout of the fix for failing Actions jobs using the deployment environments feature. We are expecting the rollout to take up to another hour
Jun 29, 04:54 UTC
Update - We are continuing to monitor the rollout of the fix for failing Actions jobs using the deployment environments feature. We are expecting the rollout to take up to another 1 to 1.5 hours.
Jun 29, 04:19 UTC
Update …]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/yr8dbc2p1ny4</id>
        <link href="https://www.githubstatus.com/incidents/yr8dbc2p1ny4"/>
        <updated>2023-06-28T11:37:54.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 11:37 UTC
Resolved - This incident has been resolved.
Jun 28, 11:24 UTC
Update - We are investigating reports of issues with service(s): Issues, Pull Requests, Actions, Releases. We will continue to keep users updated on progress towards mitigation.
Jun 28, 11:21 UTC
Investigating - We are investigating reports of degraded performance for Issues and Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/xg8t6gl5shmp</id>
        <link href="https://status.make.com/incidents/xg8t6gl5shmp"/>
        <updated>2023-06-28T09:00:21.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 11:00 CEST
Completed - The scheduled maintenance has been completed.
Jun 28, 09:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 22, 17:01 CEST
Scheduled - Please note that we will be undergoing scheduled infrastructure maintenance on Wednesday June 28th between 09:00 AM and 11:00 AM CEST on European zones. During this maintenance you can expect few short service interruptions.
During the maintenance the us1.make.celonis.com, us1.make.com websites login might be temporarily affected.
Some webhooks and shared hooks processed at that time might be temporarily affected as well.
There will be no impact on eu1.make.com, eu2.make.com and eu1.make.celonis.com.
We do not expect any data loss.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/jq61fcn60dfm</id>
        <link href="https://status.make.com/incidents/jq61fcn60dfm"/>
        <updated>2023-06-28T05:00:03.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 07:00 CEST
Completed - The scheduled maintenance has been completed.
Jun 28, 05:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 22, 16:59 CEST
Scheduled - Please note that we will be undergoing scheduled infrastructure maintenance on Wednesday June 28th between 05:00 AM and 07:00 AM CEST on European zones. During this maintenance you can expect few short service interruptions.
During the maintenance the eu1.make.celonis.com, eu1.make.com and eu2.make.com websites login might be temporarily affected.
Some webhooks and shared hooks processed at that time might be temporarily affected as well.
There will be no impact on us1.make.com and us1.make.celonis.com.
We do not expect any data loss.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues loading threads or connecting to Slack.]]></title>
        <id>https://status.slack.com//2023-06/511f645055892966</id>
        <link href="https://status.slack.com//2023-06/511f645055892966"/>
        <updated>2023-06-27T22:55:04.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 12:49pm PDT on Tuesday June 27th 2023 to 13:20 PDT on Tuesday, June 27th, 2023, approximately 1% of users were seeing issues loading Slack.


The issue was caused by Slack not allocating resources effectively in our backend database. Once resources were reallocated, the issue was resolved. Our engineering team is taking a look at the cause to make sure it does not occur in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users unable to create user groups]]></title>
        <id>https://status.slack.com//2023-06/e9d38ae3f06e5cfe</id>
        <link href="https://status.slack.com//2023-06/e9d38ae3f06e5cfe"/>
        <updated>2023-06-27T20:50:05.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From June 26, 2023 at 8:23 AM PDT to June 27, 2023 2:03 AM PDT, some users encountered an error message when creating user groups.


This issue was a result of a change to the user group creation flow. Once this change was reverted, the ability to create user groups was restored for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues sending messages with files]]></title>
        <id>https://status.slack.com//2023-06/40f7ccc7ae160447</id>
        <link href="https://status.slack.com//2023-06/40f7ccc7ae160447"/>
        <updated>2023-06-27T20:42:29.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From 00:00 AM PDT on Saturday, June 24th, 2023 to Monday, June 26th, 2023 10:18 AM PDT, some users on the desktop and browser apps encountered a 'failed to send' error message when trying to send messages with files attached. 


The issue was caused by a recent change to how file permissions are read when files are shared. This change was reverted, which resolved the problem for impacted users. 


Our team is making efforts to prevent issues like this from reoccurring in the future. Thank you for your patience while we sorted out this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Google Analytics 4 ⚡]]></title>
        <id>268097</id>
        <link href="https://changelog.bookingsync.com/introducing-google-analytics-4-268097"/>
        <updated>2023-06-27T08:11:46.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
We would like to inform you that we have recently updated our data tracking system through our Website app to adjust to Google Analytics 4.
This new version offers improved features for tracking and analyzing the performance of your website.
However, for this update to be effective and to ensure your data’s tracking, we kindly ask you to replace the Google Analytics key in the preferences of your Website application.
This key is essential for your website data to be properly collected and analyzed.
What should I do to update my key?
We invite you to follow the instructions below to make this update:
Log in to your Google Analytics account.
Go to the “Admin” section of your account.
Select the property of your website.
Click on “Data Stream” and then open the window to get your Google Analytics 4 key code.
Copy the new Google Analytics 4 key and replace the old key in the Preferences of your Website application.
If needed, you can access the video instructions here.
Thank you for your cooperation in this important update. For your information, your GA3 key is still valid until the 1st of July 2023.
If you have any questions or need any help, please do not hesitate to contact us via this form.]]></summary>
        <author>
            <name>Ella, Head of Customer Care and OnBoarding team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in AMS3]]></title>
        <id>https://status.digitalocean.com/incidents/506776wyjn35</id>
        <link href="https://status.digitalocean.com/incidents/506776wyjn35"/>
        <updated>2023-06-25T18:12:51.000Z</updated>
        <summary type="html"><![CDATA[Jun 25, 18:12 UTC
Resolved - As of 15:31 UTC our Engineering team has confirmed that the issue impacting Networking in AMS3 region has been fully resolved.
From 07:40 UTC to 15:31 UTC, Users may have experienced network timeouts, packet loss, and/or increased latency interacting with the resources in the AMS3 region. The impact has been completely subsided and the network connectivity is back to normal for all the services in AMS3 region. 
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jun 25, 15:51 UTC
Monitoring - The network issues affecting our AMS3 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in those regions, including Droplets, Managed Kubernetes, and Managed Database. 
We will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.
Jun 25, 15:18 UTC
Investigating - Our Engineering team is investigating an issue impacting the networking in our AMS3 region. During this time, a subset of users may experience intermittent packet loss or increased latency while interacting with the resources in AMS3 region.
At the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. This will also be impacting services including Spaces, Managed Kubernetes and Managed Databases as well. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/tcp7119jvq7y</id>
        <link href="https://www.githubstatus.com/incidents/tcp7119jvq7y"/>
        <updated>2023-06-25T07:01:11.000Z</updated>
        <summary type="html"><![CDATA[Jun 25, 07:01 UTC
Resolved - This incident has been resolved.
Jun 25, 06:41 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Jun 25, 06:40 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>