<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-11-01T00:22:03.897Z</id>
    <title>osmos::feed</title>
    <updated>2023-11-01T00:22:03.897Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[US SMS Carrier Maintenance - T-Mobile]]></title>
        <id>https://status.twilio.com/incidents/ft5xxtxvld67</id>
        <link href="https://status.twilio.com/incidents/ft5xxtxvld67"/>
        <updated>2023-11-01T03:00:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Oct 31, 20:00 PDT Â -Â  Nov 1, 04:00 PDT
Oct 30, 18:22 PDT
Scheduled - The T-Mobile network in the US is conducting an emergency maintenance from 31 October 2023 at 20:00 PDT until 01 November 2023 at 04:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from T-Mobile US handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United Kingdom Voice Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/76fn1xd3zmxf</id>
        <link href="https://status.twilio.com/incidents/76fn1xd3zmxf"/>
        <updated>2023-11-01T00:30:00.000Z</updated>
        <summary type="html"><![CDATA[THIS IS A SCHEDULED EVENT Oct 31, 17:30 - 21:30 PDT
Oct 18, 03:57 PDT
Scheduled - Our Voice carrier partner in the UK is conducting a planned maintenance from 31 October 2023 at 17:30 PDT until 31 October 2023 at 21:30 PDT. During the maintenance window, there could be call failures from a subset of UK phone numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Call Engineers are Investigating an issue]]></title>
        <id>https://status.twilio.com/incidents/lx1xv4bnlm0f</id>
        <link href="https://status.twilio.com/incidents/lx1xv4bnlm0f"/>
        <updated>2023-11-01T00:21:18.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 17:21 PDT
Investigating - Our monitoring systems have detected a potential issue SMS Delivery Delays when sending to the Telenor Network in Serbia via pre-registered sender-IDs. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Surf Network in Brazil for a Subset of Short Codes]]></title>
        <id>https://status.twilio.com/incidents/bj4t6qzzcyb3</id>
        <link href="https://status.twilio.com/incidents/bj4t6qzzcyb3"/>
        <updated>2023-11-01T00:10:08.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 17:10 PDT
Update - We are continuing to investigate the issue causing SMS Delivery Delays when sending to the Surf Telecom Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 31, 16:13 PDT
Update - We are experiencing SMS Delivery Delays to Surf Telecom Network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hours or as soon as more information becomes available.
Oct 31, 16:07 PDT
Investigating - Our monitoring systems have detected a potential issue with SMS delivery delays when sending to the Surf Telecom network in Brazil. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United Kingdom Account Security Carrier Partner Maintenance - Vodafone]]></title>
        <id>https://status.twilio.com/incidents/sljkmg32xfj2</id>
        <link href="https://status.twilio.com/incidents/sljkmg32xfj2"/>
        <updated>2023-10-31T23:00:26.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 16:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 27, 16:47 PDT
Scheduled - Our carrier partner Vodafone UK is conducting a planned maintenance from 31 October 2023 at 16:00 PDT until 31 October 2023 at 22:40 PDT. During the maintenance window, there could be intermittent API request failures for Vodafone UK customers.
Impacted Products: 
Lookup Identity Match 
Legacy Identity MatchAndAttributes]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[United Kingdom Account Security Carrier Partner Maintenance - EE]]></title>
        <id>https://status.twilio.com/incidents/bf0rlj4jn2lj</id>
        <link href="https://status.twilio.com/incidents/bf0rlj4jn2lj"/>
        <updated>2023-10-31T21:50:03.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 14:50 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 30, 15:16 PDT
Scheduled - Our carrier partner EE UK is conducting an emergency maintenance from 31 October 2023 at 14:50 PDT until 01 November 2023 at 00:06 PDT. During the maintenance window, there could be intermittent API request failures for EE UK customers.


Impacted Products: Verify Silent Network Auth, Lookup SIM Swap, Lookup Identity Match.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays to Vinaphone Network in Vietnam]]></title>
        <id>https://status.twilio.com/incidents/vh0wdktmql2k</id>
        <link href="https://status.twilio.com/incidents/vh0wdktmql2k"/>
        <updated>2023-10-31T21:25:44.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 14:25 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to the Vinaphone Network in Vietnam. This incident has been resolved.
Oct 31, 12:27 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to the Vinaphone Network in Vietnam. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 31, 09:55 PDT
Update - We are experiencing SMS delivery delays when sending messages to Vinaphone Network in Vietnam. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.
Oct 31, 08:09 PDT
Update - We are experiencing SMS delivery delays when sending messages to Vinaphone Network in Vietnam. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Oct 31, 07:35 PDT
Investigating - We are experiencing SMS delivery delays when sending messages to Vinaphone Network in Vietnam. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue Affecting Webhooks]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/875yz70rnb1p</id>
        <link href="https://airbnbapi.statuspage.io/incidents/875yz70rnb1p"/>
        <updated>2023-10-31T17:00:19.000Z</updated>
        <summary type="html"><![CDATA[Oct 31, 10:00 PDT
Resolved - This incident has been resolved.
Oct 30, 18:14 PDT
Monitoring - Today (Oct 30, 2023) between 1:00 PM and 3:00 PM PDT we encountered an incident that impacted our webhooks functionality. As a result, some reservations may have been missing webhooks during this time period. The issue has been resolved, and our team is actively monitoring the results to ensure the stability of our systems. If you have noticed any reservations missing webhooks, we recommend using the GET Reservations API to retrieve the details of those reservations. Alternatively, you can contact us with a list of affected reservations, and we will assist you in backfilling the missing information.
We apologize for any inconvenience this may have caused and appreciate your understanding.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Admin account not visible in the account dropdown for Super Admins]]></title>
        <id>https://status.rippling.com/incidents/fjw9lbztvc7f</id>
        <link href="https://status.rippling.com/incidents/fjw9lbztvc7f"/>
        <updated>2023-10-27T15:51:44.000Z</updated>
        <summary type="html"><![CDATA[Oct 27, 15:51 UTC
Resolved - The issue has now been resolved. All admins are able to access their Admin Account view as expected.
Oct 27, 14:50 UTC
Identified - We are aware of an issue where 'Admin Account' is not visible in the account dropdown for Super Admins. After they navigate to their 'Employee Account' view they can't navigate back to their admin view. 
The root cause has been identified and the issue should be resolved in the next hour.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Storage - SGP1 and SFO3]]></title>
        <id>https://status.digitalocean.com/incidents/nwm1sn0gjfc3</id>
        <link href="https://status.digitalocean.com/incidents/nwm1sn0gjfc3"/>
        <updated>2023-10-27T14:47:58.000Z</updated>
        <summary type="html"><![CDATA[Oct 27, 14:47 UTC
Resolved - As of 14:30 UTC, our Engineering team has resolved the issue impacting Spaces in our SGP1 and SFO3 regions. Users should no longer experience slowness or timeouts when trying to create or access their Spaces resources in the SGP1 and SFO3 regions. 
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Oct 27, 14:03 UTC
Monitoring - Our Engineering team has implemented a fix to resolve the issue impacting Spaces in our SGP1 and SFO3 regions and is monitoring the situation closely. We will post an update as soon as the issue is fully resolved.
Oct 27, 12:04 UTC
Update - Our Engineering team is continuing to investigate an issue impacting Object Storage in our SGP1 region. Additionally, we have become aware that this issue has also impacted Object Storage in the SFO3 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 27, 11:25 UTC
Investigating - As of 10:22 UTC, our Engineering team is investigating an issue with Object Storage in our SGP1 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in LON1]]></title>
        <id>https://status.digitalocean.com/incidents/1zckjwhpq1xy</id>
        <link href="https://status.digitalocean.com/incidents/1zckjwhpq1xy"/>
        <updated>2023-10-26T15:52:41.000Z</updated>
        <summary type="html"><![CDATA[Oct 26, 15:52 UTC
Resolved - As of 15:45 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. 
Thank you for your patience and we apologize for any inconvenience.
Oct 26, 15:18 UTC
Monitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. 
We will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.
Oct 26, 14:25 UTC
Identified - Our Engineering team has identified the cause of the issue impacting the networking in the LON1 region and is actively working on a fix. During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. 
We will post an update as soon as additional information is available.
Oct 26, 12:50 UTC
Investigating - As of 11:40 UTC, our Engineering team is investigating an issue impacting the networking in the LON1 region. During this time, a subset of users may experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. 
We will share an update once we have further information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-2023-007]]></title>
        <id>https://tailscale.com/security-bulletins/#ts-2023-007/</id>
        <link href="https://tailscale.com/security-bulletins/#ts-2023-007/"/>
        <updated>2023-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Description: Microsoft Defender is flagging Tailscale 1.46.1 as malware.
These classifications are false positives, and we are working with Microsoft to
resolve the situation.
As of 2023-10-27 1:05 AM UTC, we have confirmed that Microsoft have addressed
the false positive, meaning Defender no longer flags Tailscale 1.46.1 as
malware. A rescan of tailscaled.exe 1.46.1 on VirusTotal confirms this.
What happened?
Microsoft Defender was flagging Tailscale 1.46.1 as malware. This caused
Defender to quarantine the binaries, meaning they could not run.
We submitted Tailscale 1.46.1 to Microsoft to investigate the false positive,
who then updated Defender to avoid flagging this release as malware at
2023-10-27 1:05 AM UTC.
Who is affected?
People using Defender and Tailscale 1.46.1.
What is the impact?
Tailscale will not run on affected machines.
What do I need to do?
To resolve this issue on your own tailnet, you can take either or both of 2
approaches:
Update to a newer version of Tailscale. Newer versions are not affected by this problem.
Create an exception in Microsoft Defender. Microsoft has published instructions explaining how to do this.
Update Microsoft Defender.]]></summary>
        <author>
            <name>Security Bulletins on Tailscale</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/sbjdwn6mvht7</id>
        <link href="https://www.githubstatus.com/incidents/sbjdwn6mvht7"/>
        <updated>2023-10-25T22:15:54.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 22:15 UTC
Resolved - This incident has been resolved.
Oct 25, 21:37 UTC
Update - The observed Copilot API error rate is around 5% of the requests. As a result, some of the Copilot code suggestions are skipped or not delivered on time.
Oct 25, 21:19 UTC
Update - We are seeing an impact in the US region as well. We continue the investigation.
Oct 25, 20:53 UTC
Update - Copilot is experiencing intermittent issues in our Japan region. Engineers are currently investigating.
Oct 25, 20:50 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues loading Rippling]]></title>
        <id>https://status.rippling.com/incidents/vnm5lc6f0b91</id>
        <link href="https://status.rippling.com/incidents/vnm5lc6f0b91"/>
        <updated>2023-10-25T21:10:01.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 21:10 UTC
Resolved - This incident has been resolved.
Oct 25, 21:02 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 25, 20:52 UTC
Identified - An issue has been identified and we are working on a fix.
Oct 25, 20:46 UTC
Update - We are continuing to monitor for any further issues.
Oct 25, 20:22 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 25, 20:17 UTC
Identified - The issue has been identified and a fix is being implemented.
Oct 25, 20:14 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/p351mbywbp0t</id>
        <link href="https://www.githubstatus.com/incidents/p351mbywbp0t"/>
        <updated>2023-10-25T13:02:51.000Z</updated>
        <summary type="html"><![CDATA[Oct 25, 13:02 UTC
Resolved - This incident has been resolved.
Oct 25, 12:56 UTC
Update - We have applied a fix to help with Copilot performance. Initial signals show good recovery. We will continue to monitor for the time being and resolve when confident the issue has been resolved.
Oct 25, 12:29 UTC
Update - We are investigating degraded performance in Europe for Copilot. We will continue to keep users updated on progress towards mitigation.
Oct 25, 12:10 UTC
Investigating - We are investigating reports of degraded performance for Copilot]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core Infrastructure Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/y5rwfhsl0gqw</id>
        <link href="https://status.digitalocean.com/incidents/y5rwfhsl0gqw"/>
        <updated>2023-10-24T23:00:07.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 23:00 UTC
Completed - The scheduled maintenance has been completed.
Oct 24, 21:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Oct 20, 19:40 UTC
Update - After a thorough review by the team performing this maintenance, we have determined that our initial messaging does not convey the complete scope and potential impact of this event. Existing infrastructure will continue running without issue during this maintenance window. However, users may experience increased latency with some platform operations, including: 
Cloud Control Panel and API operations
Event processing
Droplet creates, resizes, rebuilds, and power events
Managed Kubernetes reconciliation and scaling
Load Balancer operations
Container Registry operations
App â€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with single sign-on from Rippling to third-party applications]]></title>
        <id>https://status.rippling.com/incidents/vlnmlxcw85r8</id>
        <link href="https://status.rippling.com/incidents/vlnmlxcw85r8"/>
        <updated>2023-10-24T22:51:30.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 22:51 UTC
Resolved - This incident has been resolved.
Oct 24, 22:08 UTC
Update - We are continuing to monitor for any further issues.
Oct 24, 22:07 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Oct 24, 21:58 UTC
Identified - The issue has been identified and a fix is being implemented.
Oct 24, 21:57 UTC
Investigating - Customers are experiencing intermittent issues using single sign-on from Rippling (IdP-initiated SAML) to third-party applications. We are investigating this issue.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: A small number of users are having problems loading Slack.]]></title>
        <id>https://status.slack.com//2023-10/2ef86432e31615ea</id>
        <link href="https://status.slack.com//2023-10/2ef86432e31615ea"/>
        <updated>2023-10-24T21:08:02.000Z</updated>
        <summary type="html"><![CDATA[Customers should no longer be experiencing any connection issues with Slack. Apologies for the trouble today and thank you for your patience.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in multiple regions.]]></title>
        <id>https://status.digitalocean.com/incidents/gp9bzm1hnlk4</id>
        <link href="https://status.digitalocean.com/incidents/gp9bzm1hnlk4"/>
        <updated>2023-10-24T09:15:10.000Z</updated>
        <summary type="html"><![CDATA[Oct 24, 09:15 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Oct 24, 09:03 UTC
Monitoring - Our Engineering team has received communication from the upstream provider that a fix to resolve the networking issue has been implemented. We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.
Oct 24, 07:41 UTC
Identified - Our Engineering team has identified the cause of issues impacting networking in multiple regions. The issues are a direct result of traffic congestion from our upstream providers, which is in the process of being repaired.
At this time, a subset of users will continue to experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.
We apologize for the inconvenience and will share an update once we have more information.
Oct 24, 06:11 UTC
Investigating - As of 05:30 UTC, our Engineering team is investigating an issue impacting the networking in multiple regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.
At the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Git Operations]]></title>
        <id>https://www.githubstatus.com/incidents/lw4dvwltm025</id>
        <link href="https://www.githubstatus.com/incidents/lw4dvwltm025"/>
        <updated>2023-10-22T16:07:58.000Z</updated>
        <summary type="html"><![CDATA[Oct 22, 16:07 UTC
Resolved - This incident has been resolved.
From 11:21 to 16:07 UTC some GitHub customers experienced errors cloning via workflows or via the command line.
A third-party configuration change resulted in an unexpected behavior to our systems that resulted in Git clone failures. Once we detected the change we were able to disable it, and our systems started operating normally.
With the incident mitigated, we are working with our third-party provider to improve subsequent configuration change rollouts.
Oct 22, 15:58 UTC
Update - We have mitigated the cause of the issue and are awaiting positive confirmation from impacted customers that the issue is fully resolved.
Oct 22, 15:34 UTC
Update - We are currently investigating reports from some customers encountering errors when cloning repositories via workflows or via the command line. We do not currently have an ETA for resolution. Next update in 30 minutes.
Oct 22, 15:16 UTC
Investigating - We are investigating reports of degraded performance for Git Operations]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing service degradation related to notifications]]></title>
        <id>https://status.notion.so/incidents/d38fkw0dxb29</id>
        <link href="https://status.notion.so/incidents/d38fkw0dxb29"/>
        <updated>2023-10-17T18:59:19.000Z</updated>
        <summary type="html"><![CDATA[Oct 17, 11:59 PDT
Resolved - Notification health should be back to normal. Engineering will continue to monitor system health.
Oct 17, 11:40 PDT
Investigating - We are experiencing service degradation related to notifications. Notifications may be delayed, and notification badge counts may not be accurate.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Accessibility via Cloud]]></title>
        <id>https://status.digitalocean.com/incidents/f38zfs58jkl8</id>
        <link href="https://status.digitalocean.com/incidents/f38zfs58jkl8"/>
        <updated>2023-10-17T17:33:44.000Z</updated>
        <summary type="html"><![CDATA[Oct 17, 17:33 UTC
Resolved - As of 16:10 UTC, our Engineering team has confirmed the full resolution of the issue impacting our App platform service where users were not able to access their Apps via Cloud Panel. Apps should be loading fine https://cloud.digitalocean.com/apps without any errors. 
We appreciate your patience during the process and if you continue to experience any issues  please open a ticket with our support team.
Oct 17, 16:51 UTC
Monitoring - Our Engineering team has deployed a fix to resolve the ongoing issue with accessing Apps list via Cloud panel for our App platform service. As of 16:10 UTC the situation started to improve and the users should be able to access their apps via Cloud panel UI without any issues. 
We are monitoring the situation closely and will post an update once the issue is completely resolved.
Oct 17, 16:19 UTC
Identified - Our Engineering team has identified an issue impacting App platform service in all regions. During this time users may experience issues while loading apps via https://cloud.digitalocean.com/apps and the requests appear to be timing out. New App deployments via API or doctl and the existing deployed apps are not impacted by this incident.  
Our team is working to mitigate the issue and we will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Codespaces]]></title>
        <id>https://www.githubstatus.com/incidents/gkrfrz6r7flc</id>
        <link href="https://www.githubstatus.com/incidents/gkrfrz6r7flc"/>
        <updated>2023-10-17T13:49:00.000Z</updated>
        <summary type="html"><![CDATA[Oct 17, 13:49 UTC
Resolved - From 10:59 UTC to 13:48 UTC, GitHub Codespaces service was down or degraded due to an outage in our authentication service.This issue impacted 67% of users over this time period.
Our service auth layer experienced throttling with our third party dependencies due to higher load. This impacted all user facing scenarios for Codespaces service. Our automated regional failover kicked in, but it failed to mitigate as this issue impacted the service globally. We mitigated manually by reducing load on external dependency.
With the incident mitigated, we are working to assess and implement scaling improvements to make our service more resilient with increasing load.
Oct 17, 13:46 UTC
Update - Codespaces is experiencing degraded performance. We are continuing to investigate.
Oct 17, 13:24 UTC
Update - We are continuing with efforts to mitigate Codespaces issues   and are beginning to see some Codespace creations succeed.
Oct 17, 12:18 UTC
Update - We have identified an issue impacting most Codespaces operations and are working on a mitigation.
Oct 17, 11:18 UTC
Update - Codespaces is experiencing degraded availability. We are continuing to investigate.
Oct 17, 11:14 UTC
Investigating - We are investigating reports of degraded performance for Codespaces]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Database Cluster]]></title>
        <id>https://status.digitalocean.com/incidents/7548dwrnthz8</id>
        <link href="https://status.digitalocean.com/incidents/7548dwrnthz8"/>
        <updated>2023-10-17T06:48:06.000Z</updated>
        <summary type="html"><![CDATA[Oct 17, 06:48 UTC
Resolved - As of 06:25 UTC, our Engineering team has confirmed that the issue impacting the newly created Managed Database Clusters has been fully resolved. 
Users will no longer experience issues with the newly created Managed Database Cluster.
If you continue to experience any issues with Managed Database Clusters please open a ticket with our support team. Thank you for your patience.
Oct 17, 06:24 UTC
Monitoring - As of 06:15 UTC, our Engineering team has identified the issue impacting our Managed Database product and a fix has been implemented to mitigate the issue. Currently, new users may no longer experience issues with the newly created Managed Database Cluster due to hostname resolution.
Users who already have a Managed Database cluster are not impacted by this incident.
Thank you for your patience and we apologize for the inconvenience caused. We are monitoring the situation and will post an update once the incident is completely resolved.
Oct 17, 04:22 UTC
Investigating - Our Engineering team is currently investigating reports of issues impacting a subset of users using our Managed Database Clusters. 
During this time, hostnames for the newly created Managed Database Cluster are not resolving. However, previously created Clusters are not impacted due to the same.
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DigitalOcean Control Panel and API]]></title>
        <id>https://status.digitalocean.com/incidents/yxfmyjl0kmpr</id>
        <link href="https://status.digitalocean.com/incidents/yxfmyjl0kmpr"/>
        <updated>2023-10-16T23:14:32.000Z</updated>
        <summary type="html"><![CDATA[Oct 16, 23:14 UTC
Resolved - From 22:45 to 23:00 UTC, our Engineering team has reported an issue with DigitalOcean Control Panel and API.
 During that time, Customers may have experienced intermittent timeout errors while using DigitalOcean Control Panel and API. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Latency in SFO Region]]></title>
        <id>https://status.digitalocean.com/incidents/k58m2k4qcchf</id>
        <link href="https://status.digitalocean.com/incidents/k58m2k4qcchf"/>
        <updated>2023-10-15T01:04:23.000Z</updated>
        <summary type="html"><![CDATA[Oct 15, 01:04 UTC
Resolved - Our Engineering team has confirmed the full resolution of the networking issue in the SFO regions. If you continue to see issues with latency or packet loss in the SFO region please reach out directly to our support team for assistance.
Oct 14, 20:30 UTC
Update - Our Engineering team is continuing to monitor the networking issue in the SFO regions. So far, we haven't observed any major spike in latency or packet loss with network connections going in or out of the SFO regions. However, we will post an update as soon as the issue is fully resolved.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 14, 15:25 UTC
Monitoring - Our Engineering team has detected a recurrence of the networking issue identified in a previous incident today: 
https://status.digitalocean.com/incidents/bhpslzd37517
Our team is actively monitoring the situation and has implemented traffic routing changes where applicable to alleviate the latency. Some users may still experience packet loss or increased latency accessing the resources in the SFO region from certain ISPs.
We apologize for any inconvenience caused and will provide updates as the situation progresses.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Latency in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/bhpslzd37517</id>
        <link href="https://status.digitalocean.com/incidents/bhpslzd37517"/>
        <updated>2023-10-14T11:46:50.000Z</updated>
        <summary type="html"><![CDATA[Oct 14, 11:46 UTC
Resolved - Our Engineering team has confirmed full resolution of the issue with networking in affected regions. Users should no longer experience timeouts or delays when connecting to or from these regions. 
If you continue to experience problems, please open a ticket with our support team.
Thank you for your patience and we apologize for any inconvenience.
Oct 14, 11:10 UTC
Monitoring - As of 10:55 UTC, our Engineering team has implemented a fix to address the networking problem in the affected regions and is currently monitoring the situation. Users should no longer face timeouts or encounter delays when connecting to or from these regions. We will post an update as soon as the issue is fully resolved.
Oct 14, 10:30 UTC
Update - As of 10:27 UTC, our Engineering team is continuing to investigate an issue with networking in our SFO regions. Additionally, it has come to our attention that this issue has affected other regions, specifically SGP1, SYD1, and NYC3. Users may encounter timeouts or experience delays in network connections going in and out of these regions. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.
We apologize for the inconvenience and thank you for your patience and continued support.
Oct 14, 09:47 UTC
Update - We are continuing to investigate this issue.
Oct 14, 09:12 UTC
Investigating - As of 8:30 UTC, our Engineering team is investigating an issue with networking in our SFO regions. During this time, users may experience timeouts or latency with network connections going in or out of the SFO regions. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces CDN in Multiple Regions]]></title>
        <id>https://status.digitalocean.com/incidents/lh3j71zp57hx</id>
        <link href="https://status.digitalocean.com/incidents/lh3j71zp57hx"/>
        <updated>2023-10-12T19:17:29.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 19:17 UTC
Resolved - Our Engineering team has confirmed the resolution of the issue that impacted the Spaces CDN. Objects should be accessible over the CDN endpoint without any issues. However, HTTP/2 is temporarily unavailable due to upstream issues, and due to this HTTP/2 requests should automatically be re-negotiated to 1.1, but in case you experience failures please open a ticket with our support team.
Thank you for your patience and we apologize for any inconvenience.
Oct 12, 16:10 UTC
Update - After our upstream provider implemented a remediation step to resolve the issue with the Spaces CDN, the Spaces CDN is serving the objects stored in the Spaces bucket without any errors or performance issues. However, we are still monitoring the situation closely and we'll share more inâ€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing an issue with SAML SSO login]]></title>
        <id>https://status.notion.so/incidents/55frj2c42yvq</id>
        <link href="https://status.notion.so/incidents/55frj2c42yvq"/>
        <updated>2023-10-12T18:14:45.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 11:14 PDT
Resolved - Users were unable to log in using the Notion mobile and desktop apps to enterprise workspaces requiring SAML SSO login. This issue is resolved now.
Oct 12, 11:10 PDT
Update - We are experiencing an issue with SAML SSO login in the Notion mobile and desktop apps, and we are investigating the cause.
Oct 12, 11:05 PDT
Investigating - We are experiencing an issue with SAML SSO and we are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NEW - Plum Guide is now available on Smily! ðŸŽ‰]]></title>
        <id>276591</id>
        <link href="https://changelog.bookingsync.com/new-plum-guide-is-now-available-on-smily!-276591"/>
        <updated>2023-10-12T16:07:05.000Z</updated>
        <summary type="html"><![CDATA[New!
Â Â 
We are delighted to announce our new partnership with Plum Guide. ðŸ¥³
Plum Guide is a curated online travel agency that showcases the worldâ€™s best homes - they're like the Michelin Guide, but for homes.
They do this by independently vetting every home and property manager in each market and selecting only the best to feature in our collection.
They were recently named â€˜Best OTAâ€™ as the 2023 Shortyz awards. ðŸ†





ðŸ’¡ How can Plum Guide help you?
Incremental revenue thanks to unique guests
Editorial style listings built by their Merchandising team
Dedicated functions for individual Hosts & Property Management Companies



ðŸ¤© Benefits:
2.1x longer stays than on Airbnb
Over 50% of bookings occur outside of high season
Dedicated AM, Sales & Onboarding teams for both individual Hosts and Property Management Companies
Majority of Plum Guide Guests have never booked on other OTAs
High Quality Guests (avg. age is 45)
Guest Cancellation Rate is <0.5%



Interested in advertising your properties on Plum Guide?
ðŸ‘‰ Install the app here

dedicated manual page or book a call with your Account Manager here.  
We hope that this new channel will help you get more bookings from travelers around the world!
Donâ€™t hesitate to contact the Plum Guide team supply@plumguide.com or our support team for any questions.


Best regards,]]></summary>
        <author>
            <name>Maud , Partnership Manager</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Users cannot clone nor move scenarios]]></title>
        <id>https://status.make.com/incidents/2c52p236t4rp</id>
        <link href="https://status.make.com/incidents/2c52p236t4rp"/>
        <updated>2023-10-12T15:40:47.000Z</updated>
        <summary type="html"><![CDATA[Oct 12, 17:40 CEST
Resolved - This incident has been resolved.
Oct 12, 16:21 CEST
Monitoring - We implemented workaround and we are currently monitoring the issue.
Oct 12, 16:09 CEST
Investigating - We are experiencing issues within our scenario features i.e. users cannot clone scenario nor move it to a folder inside list of scenarios. This is still possible when user open particular scenario.
We are currently working on implementing a quick workaround, we should publish the relevant steps within the next 30 minutes.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Managed Kubernetes Service]]></title>
        <id>https://status.digitalocean.com/incidents/fsfsv9fj43w7</id>
        <link href="https://status.digitalocean.com/incidents/fsfsv9fj43w7"/>
        <updated>2023-10-11T07:26:53.000Z</updated>
        <summary type="html"><![CDATA[Oct 11, 07:26 UTC
Resolved - Our Engineering team has resolved the issue with the Managed Kubernetes Service. A daemonset has been released to all existing clusters eliminating the auto-update process and it will be removed again going forward. If you find worker nodes that could still be affected by a prior occurrence of this incident, please replace them for permanent mitigation.
If you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.
Oct 11, 04:06 UTC
Monitoring - Our Engineering team has completed the work for both items. New images have been released that eliminate the auto-update process and the daemonset has been applied to all existing clusters. 
Given this, the team is not expecting to see a recurrence of this incidenâ€¦]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/gtsz1l2jc96n</id>
        <link href="https://www.githubstatus.com/incidents/gtsz1l2jc96n"/>
        <updated>2023-10-09T15:18:49.000Z</updated>
        <summary type="html"><![CDATA[Oct  9, 15:18 UTC
Resolved - On October 9, 2023 at 14:24 UTC, a noticeable delay in commits appearing in pull requests was detected.  During the incident, approximately 9% of pull requests (less than the 20% first reported) experienced staleness of up to 7m. The root cause was identified to be an increase in the latency of a downstream dependency causing pull request workers to saturate their available capacity, resulting in delayed updates to PRs - no data was lost during this incident.
	We mitigated this by adding additional capacity to the affected worker pool at 15:02 UTC. This allowed our background jobs to catch up with the backlog of updates and provide relief to our customers. Additionally, we have significantly increased the performance of the downstream service to prevent recurrence
Oct  9, 14:52 UTC
Update - We are investigating delays for commits showing up on Pull Requests page loads in the web UI. As a result of this,  about 20% of pull requests are currently showing stale data of up-to 7m. We are currently investigating contributing factors right now.
Oct  9, 14:51 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: **Summary**
Google Keep users are experiencing issues with notification emails when a note is shared.
**Description:**
We are experiencing an issue with Google Keep beginning on Wednesday, 2023-09-27. Mitigation work is underway by our engineering team. We do not have an ETA for mitigation at this point. We will provide more information by Wednesday, 2023-10-04 10:00 US/Pacific.
**Diagnosis**
Google Keep users are experiencing an issue where notification emails about note sharing are ending up in receiver's Spam folder
**Workaround**
None at this time]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM"/>
        <updated>2023-10-09T11:49:19.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-09-28 07:00</strong> and ended at <strong>2023-10-04 18:22</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced an impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 28 Sept 2023 00:00</p>
<p><strong>Incident End:</strong> 4 Oct 2023 11:22</p>
<p><strong>Duration:</strong>  6 days, 11 hours, 22 minutes</p>
<p><strong>Affected Services and Features:</strong></p>
<p>Google Keep - Email notifications</p>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Google Keep experienced an issue where notification emails about note sharing ended up in the receiver&#39;s Spam folder for a duration of 6 days, 11 hours and 22 minutes. From preliminary analysis, the root cause is an unexpected increase in notifications.</p>
<p><strong>Customer Impact:</strong></p>
<p>Google Keep users incorrectly received email notifications in their spam folder.</p>
</div><hr><p>Affected products: Google Keep</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces listing in FRA1]]></title>
        <id>https://status.digitalocean.com/incidents/08q0d3g8dtp3</id>
        <link href="https://status.digitalocean.com/incidents/08q0d3g8dtp3"/>
        <updated>2023-10-07T20:01:58.000Z</updated>
        <summary type="html"><![CDATA[Oct  7, 20:01 UTC
Resolved - As of 19:52 UTC, our Engineering team has confirmed the full resolution of the issue impacting the listing of Spaces buckets in the FRA1 region via the Cloud Control Panel. 
If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Oct  7, 19:38 UTC
Monitoring - Our engineering team has implemented a fix to resolve the issue with listing Spaces in our FRA1 region via the Cloud Panel and is monitoring the situation. We will post an update as soon as the issue is fully resolved.
Oct  7, 19:15 UTC
Investigating - Our Engineering team is investigating an issue with Spaces in our FRA1 region. As of 13:40 UTC, users may experience issues listing the Spaces created in the FRA1 region via the Cloud Panel. We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outage: Users are having trouble connecting to Slack]]></title>
        <id>https://status.slack.com//2023-10/ad8f0e62516e8812</id>
        <link href="https://status.slack.com//2023-10/ad8f0e62516e8812"/>
        <updated>2023-10-06T15:28:47.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:


On Friday, October 6 2023, from 1:52 AM PDT to 2:12 AM PDT, some users were unable to load Slack. This was caused by an issue where certain backend Slack processes were pulling data from our database rather than their cache which caused strain on our servers.


We rolled out a change to how these processes load data, and the issue was resolved for all users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with API Requests]]></title>
        <id>https://www.githubstatus.com/incidents/w554d74dv18j</id>
        <link href="https://www.githubstatus.com/incidents/w554d74dv18j"/>
        <updated>2023-10-06T02:59:36.000Z</updated>
        <summary type="html"><![CDATA[Oct  6, 02:59 UTC
Resolved - We deployed a new configuration to improve our network availability. This resulted in a small percentage of user traffic getting incorrectly blocked, but missed by our automated detections. We mitigated this with a change to the configuration, rolled out slowly over the last hour of this incident time for safe deployment. Beyond the learnings related to the config, we are analyzing how we can more quickly detect this kind of impact as part of future configuration rollouts.
Oct  6, 02:42 UTC
Update - We have confirmed that the fix has resolved the issue in the subset of regions where it has been deployed. We are now continuing the deployment to the remaining regions.
Oct  6, 02:03 UTC
Update - We are monitoring the rollout of the fix and are beginning to see signs of improvement. We will send another update shortly.
Oct  6, 01:31 UTC
Update - A small number of customers are experiencing 403 errors when attempting to access repository data via the API. We have found what we believe to be the cause of the issue and are deploying a fix.
Oct  6, 01:12 UTC
Investigating - We are investigating reports of degraded performance for API Requests]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Session expiration issues]]></title>
        <id>https://status.make.com/incidents/jjzrxzv3vfj5</id>
        <link href="https://status.make.com/incidents/jjzrxzv3vfj5"/>
        <updated>2023-10-05T16:58:09.000Z</updated>
        <summary type="html"><![CDATA[Oct  5, 18:58 CEST
Resolved - This incident has been resolved.
Oct  5, 13:23 CEST
Monitoring - We have identified that cause of the issue, users should no longer experience unexpected session drops. We will be monitoring the situation for the next few hours.
Oct  5, 12:05 CEST
Investigating - We encountered issues related to user sessions, so users may experience that their session might be disconnected after a few minutes. We are currently investigating the issue and we will provide a new update in the next hour.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/9fk4c2w43clz</id>
        <link href="https://www.githubstatus.com/incidents/9fk4c2w43clz"/>
        <updated>2023-10-05T16:42:15.000Z</updated>
        <summary type="html"><![CDATA[Oct  5, 16:42 UTC
Resolved - On October 5, 2023 at 13:40 UTC, our monitoring systems observed an increase in the time it was taking for Git pushes to become visible when viewing commits in Pull Requests. Under normal operating conditions, a series of asynchronous jobs runs in response to every push and within a few seconds applies a number of side-effects in Pull Requests such as requesting reviews, marking Pull Requests as merged, and showing new commits. During the incident, jobs were entering the queue faster than we could process them, resulting in processing delays as high as 75 seconds on average, and as much as 15 minutes in the worst case. About 10% of all Pull Request page loads were showing out-of-date data during this time.
We had recently created a dedicated worker pool for proâ€¦]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble connecting to Slack]]></title>
        <id>https://status.slack.com//2023-10/6d592537eb7bd98e</id>
        <link href="https://status.slack.com//2023-10/6d592537eb7bd98e"/>
        <updated>2023-10-05T13:53:12.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On Oct 4, 2023 from 11:15 PM PDT to around 11:58 PM PDT a small subset of users experienced trouble connecting to Slack and sending and receiving messages.


A code change optimizing resource usage caused a core service to restart causing brief connectivity issues for a few users.


The service recovered automatically and weâ€™ve put measures in place to prevent this from happening in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues with loading some items on Slack]]></title>
        <id>https://status.slack.com//2023-10/ee32d13012fb29d5</id>
        <link href="https://status.slack.com//2023-10/ee32d13012fb29d5"/>
        <updated>2023-10-04T18:53:01.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On October 3, 2023 from 4:00 PM PDT to 4:20 PM PDT, some customers had trouble loading threads, mentions, reactions and search results in Slack.


We determined that this was caused during the process of changing our database hosts on our backend. To mitigate the issue, we stopped the change which fixed the issue for affected customers.


We're sorry for any disruption this may have caused and thank you for your patience while we looked into this]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/18rxsrdk6zkz</id>
        <link href="https://status.notion.so/incidents/18rxsrdk6zkz"/>
        <updated>2023-10-04T17:52:33.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 10:52 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  4, 09:28 PDT
Monitoring - We've rolling out a fix and will continue monitoring.
Oct  4, 09:04 PDT
Identified - We have implemented a fix and rolling out.
Oct  4, 08:32 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Error on GET Opportunities API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/kkvp54r30jdn</id>
        <link href="https://airbnbapi.statuspage.io/incidents/kkvp54r30jdn"/>
        <updated>2023-10-04T16:46:35.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 09:46 PDT
Resolved - This incident has been resolved.
Oct  3, 22:04 PDT
Monitoring - There was an increased number of 500 errors on the GET Opportunities API endpoint. We've mitigated the issue and will continue to monitor it. The time window for these errors was between 9:00 PM and 9:50 PM (PDT) today (Oct 3, 2023)]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/dknkp34qzxg0</id>
        <link href="https://status.notion.so/incidents/dknkp34qzxg0"/>
        <updated>2023-10-04T13:49:09.000Z</updated>
        <summary type="html"><![CDATA[Oct  4, 06:49 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  4, 05:31 PDT
Monitoring - We're rolling out a fix and will continue monitoring.
Oct  4, 05:12 PDT
Update - We have implemented a fix and rolling out soon.
Oct  4, 04:58 PDT
Identified - We have identified the issue and are working on a fix.
Oct  4, 04:46 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue Delaying Webhook Deliveries]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/rhkqlr25pfcs</id>
        <link href="https://airbnbapi.statuspage.io/incidents/rhkqlr25pfcs"/>
        <updated>2023-10-03T17:21:45.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:21 PDT
Resolved - This incident has been resolved.
Oct  2, 10:44 PDT
Monitoring - We're currently investigating an issue that's causing spikes in our delivery timelines for asynchronous webhook notifications. It began around 5PM PST on Friday, September 29, and briefly reoccured on both Saturday and Sunday afternoon. We've mitigated the issue for now, but will continue to monitor this over the next few days.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basic Auth Errors for Synchronous Availability Checks and Webhook Notifications]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/6zcksbcfmvsb</id>
        <link href="https://airbnbapi.statuspage.io/incidents/6zcksbcfmvsb"/>
        <updated>2023-10-03T17:21:27.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:21 PDT
Resolved - This incident has been resolved.
Oct  2, 14:30 PDT
Monitoring - There was an issue with the generation of Basic Auth headers for synchronous availability checks and asynchronous webhook notifications between 11:45 AM and 2:00 PM PDT today. This resulted in errors for both availability checks and webhooks.
The issue has been resolved, and our team will be monitoring any further errors.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevated Server Error Rates on Async Calendars API]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/7d87njvtk91g</id>
        <link href="https://airbnbapi.statuspage.io/incidents/7d87njvtk91g"/>
        <updated>2023-10-03T17:20:56.000Z</updated>
        <summary type="html"><![CDATA[Oct  3, 10:20 PDT
Resolved - This incident has been resolved. Error rates have returned to normal levels. Please retry any failed requests.
Oct  2, 19:51 PDT
Monitoring - We have mitigated the issue and will continue monitoring it. The time window for the errors was between 5:45 PM and 7:00 PM PDT.
Oct  2, 18:34 PDT
Investigating - We are currently investigating elevated error rates when processing the Async Calendars API update requests.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/gxjt1jbwqs7m</id>
        <link href="https://status.notion.so/incidents/gxjt1jbwqs7m"/>
        <updated>2023-10-03T00:13:08.000Z</updated>
        <summary type="html"><![CDATA[Oct  2, 17:13 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  2, 16:54 PDT
Identified - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notion is experiencing degraded performance]]></title>
        <id>https://status.notion.so/incidents/b3g5shlj6hhl</id>
        <link href="https://status.notion.so/incidents/b3g5shlj6hhl"/>
        <updated>2023-10-02T23:03:47.000Z</updated>
        <summary type="html"><![CDATA[Oct  2, 16:03 PDT
Resolved - The issue has been resolved. Notion app and API performance is back to Normal.
Oct  2, 14:28 PDT
Update - We're continuing to monitor the impact of the fix we implemented. We expect it may take a few hours to fully complete.
Oct  2, 13:54 PDT
Monitoring - We're rolling out a fix and will continue monitoring.
Oct  2, 13:02 PDT
Update - We have developed a fix and will be implementing soon.
Oct  2, 06:40 PDT
Identified - We have identified the issue and are working on a fix.
Oct  2, 06:24 PDT
Investigating - We are experiencing some performance degradation and are investigating the cause.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Threads were not loading for some users]]></title>
        <id>https://status.slack.com//2023-10/101eb09455dcaa9a</id>
        <link href="https://status.slack.com//2023-10/101eb09455dcaa9a"/>
        <updated>2023-10-02T20:16:28.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


On October 2, 2023 from 12:18PM PDT to 12:26PM PDT, some users noticed that threads were not loading correctly.


We identified a database issue on our backend that was causing the threads to not load. The issue was mitigated and users should no longer experience trouble loading threads.


We are investigating the cause of the database issue so we can ensure this does not happen again.


We apologize for any interruption to your work day!]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
</feed>