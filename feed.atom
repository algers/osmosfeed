<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2023-07-09T00:30:48.967Z</id>
    <title>osmos::feed</title>
    <updated>2023-07-09T00:30:48.967Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delay to Glo Mobile Network in Nigeria]]></title>
        <id>https://status.twilio.com/incidents/wjb1hkdj9yb0</id>
        <link href="https://status.twilio.com/incidents/wjb1hkdj9yb0"/>
        <updated>2023-07-08T09:56:25.000Z</updated>
        <summary type="html"><![CDATA[Jul  8, 02:56 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Glo Mobile network in Nigeria. This incident has been resolved.
Jul  8, 01:02 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Glo Mobile network in Nigeria. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Jul  8, 00:11 PDT
Update - We are still experiencing SMS delivery delay to Glo Mobile Network in Nigeria. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hour or as soon as more information becomes available.
Jul  7, 23:11 PDT
Investigating - We are experiencing SMS delivery delay to Glo Mobile Network in Nigeria. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Czech Republic Voice Carrier Partner Maintenance]]></title>
        <id>https://status.twilio.com/incidents/t52hlttrhcw2</id>
        <link href="https://status.twilio.com/incidents/t52hlttrhcw2"/>
        <updated>2023-07-08T04:00:34.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 21:00 PDT
Completed - The scheduled maintenance has been completed.
Jul  7, 15:01 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jul  7, 14:28 PDT
Scheduled - Our Voice carrier partner in the Czech Republic is conducting an emergency maintenance from 07 July 2023 at 15:00 PDT until 07 July 2023 at 21:00 PDT. During the maintenance window, there could be intermittent call disconnects for calls to a subset of Twilio Czech Republic phone numbers.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Carrier Partner Maintenance - Multiple Countries]]></title>
        <id>https://status.twilio.com/incidents/j6p2cwv70wh6</id>
        <link href="https://status.twilio.com/incidents/j6p2cwv70wh6"/>
        <updated>2023-07-07T23:03:00.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 16:03 PDT
Completed - The scheduled maintenance has been completed.
Jul  7, 12:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jul  3, 04:03 PDT
Scheduled - Our SMS Carrier Partner is conducting a planned maintenance from 07 July 2023 at 12:00 PDT until 07 July 2023 at 16:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to multiple countries as shown below.

Region: Asia

Afghanistan

Indonesia

Myanmar

Philippines

Taiwan

Uzbekistan 

Region: Africa

Algeria

Congo Democratic Republic

Ethiopia

Gabon

Ivory Coast

Rwanda

Sudan

Tunisia

Uganda 

Region: Europe

Belarus

Kyrgyzstan

Ukraine 

Region: South America

Argentina

Bolivia]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RESOLVED: We're investigating reports of an issue with Gmail. We will provide more information shortly. The affected users are able to access Gmail, but are seeing error messages, high latency, and/or other unexpected behavior.]]></title>
        <id>https://www.google.com/appsstatus/dashboard/incidents/aw35PEC9zRoTE8rNHWL6</id>
        <link href="https://www.google.com/appsstatus/dashboard/incidents/aw35PEC9zRoTE8rNHWL6"/>
        <updated>2023-07-07T21:29:14.000Z</updated>
        <summary type="html"><![CDATA[<p> Incident began at <strong>2023-07-06 22:15</strong> and ended at <strong>2023-07-06 23:47</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class="cBIRi14aVDP__status-update-text"><h1>Mini Incident Report</h1>
<p>We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced an impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href="https://support.google.com/a/answer/1047213">https://support.google.com/a/answer/1047213</a>.</p>
<p>(All Times US/Pacific)</p>
<p><strong>Incident Start:</strong> 6 July 2023 at 15:15</p>
<p><strong>Incident End:</strong> 6 July 2023 at 16:47</p>
<p><strong>Duration:</strong> 1 hour, 32 minutes</p>
<p><strong>Affected Services and Features:</strong></p>
<p>Gmail</p>
<p><strong>Regions/Zones:</strong> Global</p>
<p><strong>Description:</strong></p>
<p>Some Gmail users experienced elevated latency and intermittent unavailability for a period of 1 hour and 32 minutes. This was due to an unexpected resource usage increase in backend servers.</p>
<p>From the preliminary analysis, the issue was triggered during validation of a configuration change. This triggered an increase in the number of background tasks, which resulted in elevated memory pressure. Google engineers quickly started to work on containing the issue and mitigated the impact by disabling the background tasks and returning the memory usage to normal levels.</p>
<p><strong>Customer Impact:</strong></p>
<p>Impacted users may have experienced elevated latency and errors while syncing their Gmail accounts to clients. Users may have also experienced intermittent failures in the page load on the web and latency in email delivery.</p>
</div><hr><p>Affected products: Gmail</p>]]></summary>
        <author>
            <name>Google Workspace Status Dashboard Updates</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Malaysia SMS Carrier Maintenance - ONE XOX]]></title>
        <id>https://status.twilio.com/incidents/8c6dhbbyr3pc</id>
        <link href="https://status.twilio.com/incidents/8c6dhbbyr3pc"/>
        <updated>2023-07-07T20:00:28.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 13:00 PDT
Completed - The scheduled maintenance has been completed.
Jul  7, 09:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 25, 17:52 PDT
Scheduled - The ONE XOX network in Malaysia is conducting a planned  maintenance from 07 July 2023 at 09:00 PDT until 07 July 2023 at 13:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from ONE XOX Malaysia handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Malaysia SMS Carrier Maintenance - U Mobile]]></title>
        <id>https://status.twilio.com/incidents/9b6w27z6qy1q</id>
        <link href="https://status.twilio.com/incidents/9b6w27z6qy1q"/>
        <updated>2023-07-07T20:00:24.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 13:00 PDT
Completed - The scheduled maintenance has been completed.
Jul  7, 09:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 21, 23:30 PDT
Scheduled - The U Mobile network in Malaysia is conducting a planned maintenance from 07 July 2023 at 09:00 PDT until 07 July 2023 at 13:00 PDT. During the maintenance window, there could be intermittent delays delivering SMS to and from U Mobile Malaysia handsets.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMS Delivery Delays and Failure to Bakcell Network in Azerbaijan]]></title>
        <id>https://status.twilio.com/incidents/93yt3kc1dv8t</id>
        <link href="https://status.twilio.com/incidents/93yt3kc1dv8t"/>
        <updated>2023-07-07T18:10:02.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 11:10 PDT
Resolved - We are no longer experiencing SMS delivery delays when sending messages to Bakcell network in Azerbaijan. This incident has been resolved.
Jul  7, 09:10 PDT
Monitoring - We are observing recovery in SMS delivery delays when sending messages to Bakcell network in Azerbaijan. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.
Jul  7, 07:10 PDT
Update - We are still experiencing SMS delivery delays and failures when sending messages to Bakcell network in Azerbaijan. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.
Jul  7, 06:10 PDT
Investigating - We are experiencing SMS delivery delays and failures when sending messages to Bakcell network in Azerbaijan. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.]]></summary>
        <author>
            <name>Twilio Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/hz717kx2gsvq</id>
        <link href="https://www.githubstatus.com/incidents/hz717kx2gsvq"/>
        <updated>2023-07-07T17:15:00.000Z</updated>
        <summary type="html"><![CDATA[Jul  7, 17:15 UTC
Resolved - This incident has been resolved.
Jul  7, 16:36 UTC
Update - The fix has been confirmed to work in production and is in the process of being deployed to all users.
Jul  7, 16:14 UTC
Update - We have identified a bug with Pull Requests where "change base branch" dropdown menu is not responding. We are in the process of shipping a fix.
Jul  7, 15:53 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing issues with text previews]]></title>
        <id>https://status.slack.com//2023-07/b0781e1094d3cd69</id>
        <link href="https://status.slack.com//2023-07/b0781e1094d3cd69"/>
        <updated>2023-07-06T22:58:08.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

On July 6, 2023 between 06:21 AM PDT and 10:58 AM PDT, some users were experiencing issues with messages sent with an attached file and app mention. They were incorrectly displaying an error "This message contains interactive elements".


As we looked into it further, we traced the problem to a formatting issue, which resulted in missing metadata for the messages sent. Reverting a recent change fixed the issue for all affected customers.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signups affected for some new users]]></title>
        <id>https://status.notion.so/incidents/zqrrhnc8xcpb</id>
        <link href="https://status.notion.so/incidents/zqrrhnc8xcpb"/>
        <updated>2023-07-06T19:48:27.000Z</updated>
        <summary type="html"><![CDATA[Jul  6, 12:48 PDT
Resolved - A recent change affected signups for some new users. This has been rolled back and signups for new users should work correctly. This incident has been resolved.]]></summary>
        <author>
            <name>Notion Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Droplet Creation in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/m4nw5210sd5t</id>
        <link href="https://status.digitalocean.com/incidents/m4nw5210sd5t"/>
        <updated>2023-07-05T19:58:43.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 19:58 UTC
Resolved - Our Engineering team have confirmed the issue with creation of Droplets and Droplet-based products is fully resolved. As of 18:15 UTC the fix was applied to resolve the problem and impact was subsided. All the events should now be processing normally in NYC3 region as the services are completely restored. 
Thank you for your patience. If you continue to experience any problems, please open a support ticket from within your account.
Jul  5, 18:47 UTC
Monitoring - Our Engineering team identified the root cause and implemented a fix to resolve the issue with Droplet creation in our NYC3 region. Users should now be able to create Droplets and Droplet-based services in the region. 
We are monitoring the situation closely and will post an update once the matter is completely resolved.
Jul  5, 17:23 UTC
Identified - Our Engineering team has identified the cause of the issue impacting Droplet creation in the NYC3 region and is actively working on a fix.
During this time, users may still experience latency and related issues while deploying Droplets and Droplet-based services in the NYC3 region. We will post an update as soon as additional information is available.
Jul  5, 16:27 UTC
Investigating - Our Engineering team is investigating an issue with Droplet creation in our NYC3 region. 
As of 15:50 UTC, users may see latency or errors when creating Droplets and Droplet-based services like Load Balancers, Managed Databases, and Kubernetes clusters, via Cloud Control Panel. Users will also be experiencing errors or slow processing of the API calls as well.
 We apologize for inconvenience and will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FRA1 Power Maintenance]]></title>
        <id>https://status.digitalocean.com/incidents/ydd314cpt9fk</id>
        <link href="https://status.digitalocean.com/incidents/ydd314cpt9fk"/>
        <updated>2023-07-05T14:00:50.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 14:00 UTC
Completed - The scheduled maintenance has been completed.
Jul  5, 10:00 UTC
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jul  4, 16:53 UTC
Scheduled - Start: 2023-07-05  10:00 AM UTC
End: 2023-07-05   14:00 PM UTC 
Hello, 
During the above window, our Engineering team will be performing maintenance on redundant upstream electrical systems in order to improve reliability in our FRA1 region.
Expected Impact:
We don't expect to see any downtime as the power to hypervisors are redundant. In a highly unlikely situation, where we lose power to both the supplies, the Droplets in FRA1 may experience downtime for a brief period. We will endeavor to keep this to a minimum for the duration of the change.
If you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket
Thank you,
Team DigitalOcean]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/pr0ptqcw6q8d</id>
        <link href="https://www.githubstatus.com/incidents/pr0ptqcw6q8d"/>
        <updated>2023-07-05T11:37:28.000Z</updated>
        <summary type="html"><![CDATA[Jul  5, 11:37 UTC
Resolved - This incident has been resolved.
Jul  5, 11:35 UTC
Update - Pull Requests is continuing to operate normally following earlier issues.
We are synchronising impacted pull requests in the background. If there are any further issues we will status again.
Jul  5, 11:22 UTC
Update - Reprocessing of unprocessed pushes from earlier continues. We will continue to provide progress updates.
Reminder that all new pushes are working as usual.
Jul  5, 10:30 UTC
Update - We have identified impacted pushes to repositories and are reprocessing these. We will provide updates as we progress.
As previously stated, all new pushes will be processed as usual.
Jul  5, 09:55 UTC
Update - We are working to identify all impacted pushes. Once identified, we will reprocess these. We will provide updates as we progress.
As previously stated, all new pushes will be processed as usual.
Jul  5, 07:09 UTC
Update - We have identified an issue that caused pull requests to not reflect additional git pushes over the last several hours. New pushes will now be processed as normal.
Jul  5, 05:54 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in AMS and FRA]]></title>
        <id>https://status.digitalocean.com/incidents/dpfhm7hf06s3</id>
        <link href="https://status.digitalocean.com/incidents/dpfhm7hf06s3"/>
        <updated>2023-07-03T16:52:21.000Z</updated>
        <summary type="html"><![CDATA[Jul  3, 16:52 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting networking in our AMS and FRA regions. If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jul  3, 13:25 UTC
Investigating - Our Engineering team is investigating an issue impacting networking in our AMS and FRA regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in these regions.
At the moment, all Droplet-based services appear to be impacted and users can expect to see brief connectivity issues and interrupted traffic flows.  We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may experience slow-to-load threads]]></title>
        <id>https://status.slack.com//2023-06/838a7ee19a81940e</id>
        <link href="https://status.slack.com//2023-06/838a7ee19a81940e"/>
        <updated>2023-07-03T15:53:07.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 

On Wednesday June 28th, 2023 from 10:35 AM PDT to 1:55 PM PDT, some customers experienced delays in loading threads. This was caused by a strain on our databases due to some unusually data-heavy tasks running in the background.


We stopped the tasks that caused the strain — which resolved the problem for impacted users. Thank you for your patience while we fixed this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with Slack API Website]]></title>
        <id>https://status.slack.com//2023-06/469a7534a9fd990a</id>
        <link href="https://status.slack.com//2023-06/469a7534a9fd990a"/>
        <updated>2023-07-03T15:33:08.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary: 

On June 29, 2023 between 12:12 PM PDT and 1:47 PM PDT some users had trouble loading api.slack.com and API documentation was unavailable. This issue was caused due to a code change that resulted in api.slack.com to be broken for users that were not logged in. We quickly reverted the change — resolving the issue for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/mc06mgh6qv9r</id>
        <link href="https://www.githubstatus.com/incidents/mc06mgh6qv9r"/>
        <updated>2023-07-03T13:47:36.000Z</updated>
        <summary type="html"><![CDATA[Jul  3, 13:47 UTC
Resolved - This incident has been resolved.
Jul  3, 13:22 UTC
Update - We are aware of an increase in 5xx errors for Pull Requests. Our team has been working on mitigation for the past 40 minutes and expects to see full recovery in the next 20 minutes.
Jul  3, 13:18 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users may be experiencing increased loading time or delays when switching between channels]]></title>
        <id>https://status.slack.com//2023-06/a8931305c8cbcf0e</id>
        <link href="https://status.slack.com//2023-06/a8931305c8cbcf0e"/>
        <updated>2023-07-03T09:54:10.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From 09:27 PDT on Friday June 23th 2023 to 14:23 PDT the same day, some customers experienced delays in loading messages and conversations. This was caused by a slight strain on our databases due to some unusually data heavy tasks running in the background.


We stopped the tasks that caused that strain, which resolved the problem for impacted users.


Thank you for your patience while we fixed this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble logging in]]></title>
        <id>https://status.slack.com//2023-06/14c5fc2b58f7c4eb</id>
        <link href="https://status.slack.com//2023-06/14c5fc2b58f7c4eb"/>
        <updated>2023-07-03T05:30:19.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On June 30, 2023 from 8:12 AM PDT to 4:23 PM PDT, some users may have had trouble signing in to Slack with magic login codes or via the 'Sign In With Email' button. 


A code change inadvertently introduced a logic error in the sign-in flow. We identified the error and deployed an initial fix to correct the flawed logic. This resolved the problem with the 'Sign In With Email' button. After further investigation, we deployed a second correction to address the trouble with magic login codes, resolving the issue for all affected customers.


We believed the first fix had resolved the problem for all customers, and we updated the status site accordingly. However, we discovered that the issue with magic login codes had not been addressed, and we posted a separate notice on the status site while we investigated further. We apologize for any confusion and for calling the all clear too quickly. 


We have updated the titles for both entries so that they match.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing trouble logging in]]></title>
        <id>https://status.slack.com//2023-06/e4f9c32806cc233f</id>
        <link href="https://status.slack.com//2023-06/e4f9c32806cc233f"/>
        <updated>2023-07-03T05:28:56.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On June 30, 2023 from 8:12 AM PDT to 4:23 PM PDT, some users may have had trouble signing in to Slack with magic login codes or via the 'Sign In With Email' button. 


A code change inadvertently introduced a logic error in the sign-in flow. We identified the error and deployed an initial fix to correct the flawed logic. This resolved the problem with the 'Sign In With Email' button. After further investigation, we deployed a second correction to address the trouble with magic login codes, resolving the issue for all affected customers.


We believed the first fix had resolved the problem for all customers, and we updated the status site accordingly. However, we discovered that the issue with magic login codes had not been addressed, and we posted a separate notice on the status site while we investigated further. We apologize for any confusion and for calling the all clear too quickly. 


We have updated the titles for both entries so that they match.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple services down and API availability]]></title>
        <id>https://status.digitalocean.com/incidents/k8ll5g9988gt</id>
        <link href="https://status.digitalocean.com/incidents/k8ll5g9988gt"/>
        <updated>2023-07-02T10:01:03.000Z</updated>
        <summary type="html"><![CDATA[Jul  2, 10:01 UTC
Resolved - Our Engineering team has confirmed that the issue impacting Public API and multiple services has been fully resolved. 
Users should no longer experience issues with Billing services, new User registrations, and Droplet event processing. All services related to the App platform, Managed Kubernetes, Container Registry, and Block Storage Volumes should be functioning normally.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. Thank you for your patience.
Jul  2, 09:38 UTC
Update - Our Engineering team had already deployed a fix earlier and that has mitigated the issue impacting Public API and multiple services. 
As of 09:20 UTC, our Engineering team found issues related to Billing services whic…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Webhook Migration Enforcement - Phase 3]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/3f2h5b7dp3vj</id>
        <link href="https://airbnbapi.statuspage.io/incidents/3f2h5b7dp3vj"/>
        <updated>2023-07-01T07:01:10.000Z</updated>
        <summary type="html"><![CDATA[Jul  1, 00:01 PDT
Completed - The scheduled maintenance has been completed.
Jun 26, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 23, 08:32 PDT
Scheduled - The deprecation date for API Webhook Version 2021.12.31 was May 31, 2023. Any applications that have not fully migrated webhooks will be automatically migrated to a newer version, which can result in application breakage.
Automatic webhook migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 3 will occur during the week of June 26th, 2023 and will automatically migrate all Critical booking sync check webhooks:
- check_availability
- make_reservation
- update_reservation]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spaces availability in NYC3]]></title>
        <id>https://status.digitalocean.com/incidents/qj8dv6r1wdc8</id>
        <link href="https://status.digitalocean.com/incidents/qj8dv6r1wdc8"/>
        <updated>2023-06-30T05:32:48.000Z</updated>
        <summary type="html"><![CDATA[Jun 30, 05:32 UTC
Resolved - Our engineering team has resolved the issue impacting Spaces in our NYC3 region. From approximately 3:41 UTC - 5:10 UTC, users may have encountered errors with API or object requests, experienced difficulties in creating new buckets in NYC3, or faced issues with loading Spaces in the Cloud Control Panel. Spaces should now be operating normally. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.
Jun 30, 05:13 UTC
Identified - Our engineering team has identified the cause of the issue with Spaces availability in our NYC3 region and is actively working on a fix. We will post an update as soon as additional information is available.
Jun 30, 03:41 UTC
Investigating - As of 03:15 UTC, Our Engineering team is investigating a drop in the availability for Spaces in our NYC region. During this time, some users may experience errors with API or object requests, be unable to create new buckets in NYC3, and/or see issues with loading Spaces in the Cloud Control Panel. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support Center]]></title>
        <id>https://status.digitalocean.com/incidents/tgqy43vl1mhf</id>
        <link href="https://status.digitalocean.com/incidents/tgqy43vl1mhf"/>
        <updated>2023-06-29T22:31:18.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 22:31 UTC
Resolved - Our vendor has confirmed resolution of this issue and the Support Center is functioning normally as of approximately 11:00 UTC. 
Users can access the Support Center and submit tickets normally. We appreciate your patience as we worked through this issue and apologize for the disruption. 
If you continue to experience problems, please open a ticket with our support team from the contact form.
Jun 29, 13:19 UTC
Update - Our Engineering team has confirmed with our vendor that the Support Center is accessible once more. At this time, users should be able to access the Support Center normally through their DigitalOcean account. 
Our team and the vendor are continuing to monitor this issue. We will post further updates as we have new information or once we confirm th…]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[macOS agent installs will fail when Find My Mac is enabled]]></title>
        <id>https://status.rippling.com/incidents/r0c40mv4dpzs</id>
        <link href="https://status.rippling.com/incidents/r0c40mv4dpzs"/>
        <updated>2023-06-29T22:25:26.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 22:25 UTC
Resolved - Service is operating normally
Jun 29, 22:15 UTC
Monitoring - We are rolling out the fix to production and normal operation is resuming.
Jun 29, 20:33 UTC
Identified - The issue has been identified and a fix is being implemented.
Jun 29, 20:31 UTC
Investigating - Existing agents will be slow to update app.rippling.com when Find My Mac is enabled]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[App Platform Deployments]]></title>
        <id>https://status.digitalocean.com/incidents/jxchgkbpq0pg</id>
        <link href="https://status.digitalocean.com/incidents/jxchgkbpq0pg"/>
        <updated>2023-06-29T19:03:43.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 19:03 UTC
Resolved - As of 18:36 UTC, GitHub has recovered and is operating normally. App Platform users with source code hosted in GitHub should no longer experience any errors while deploying their Apps.
If you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel.
Jun 29, 18:09 UTC
Monitoring - As of 17:52 UTC, our App Platform service might experience errors due to an ongoing GitHub incident. During this time, App Platform users with source code hosted in GitHub will have their App builds failed and users will not be able to create new Apps based on GitHub sources. Additionally, users who reference GitHub source code amongst their dependencies may see those dependencies fail to fetch.
Please visit the below link for more information on the GitHub incident.
https://www.githubstatus.com/incidents/gqx5l06jjxhp
We're monitoring Github's incident and will post any relevant updates here.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident On 2023-06-29]]></title>
        <id>https://www.githubstatus.com/incidents/gqx5l06jjxhp</id>
        <link href="https://www.githubstatus.com/incidents/gqx5l06jjxhp"/>
        <updated>2023-06-29T18:36:19.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 18:36 UTC
Resolved - From 17:39-18:12 UTC GitHub was down in parts of North America, particularly the US East coast, and South America. 
GitHub takes measures to ensure that we have redundancy in our system for various disaster scenarios. We have been working on building redundancy to an earlier single point of failure in our network architecture at a second Internet edge facility. This second Internet edge facility was completed in January and has been actively routing production traffic since then. Today we were performing a live failover test to validate that we could in fact use this second Internet edge facility if the primary were to fail. Unfortunately, during this failover we inadvertently caused a production outage.
During the test we exposed that the secondary site had a …]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/xqskfvqm07w3</id>
        <link href="https://www.githubstatus.com/incidents/xqskfvqm07w3"/>
        <updated>2023-06-29T17:19:31.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 17:19 UTC
Resolved - This incident has been resolved.
Jun 29, 16:30 UTC
Update - The third party provider GitHub uses is continuing to work through the issue that is causing delays for some customers with Actions Larger Runners and has started applying mitigations.
Jun 29, 15:45 UTC
Update - The third party provider GitHub uses is continuing to work through the issue that is causing delays for some customers with Actions Larger Runners and is seeking to provide a mitigation soon.
Jun 29, 15:05 UTC
Update - A third party provider GitHub uses has identified an issue and is working on recovery over the next hour. Standard Hosted Runners and Self Hosted Runners should not be impacted.
Jun 29, 15:02 UTC
Update - GitHub Actions customers using Windows and Linux Larger Runners may be experiencing longer than normal queue times.
Jun 29, 14:51 UTC
Investigating - We are investigating reports of degraded performance for Actions.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions, API Requests and Pages]]></title>
        <id>https://www.githubstatus.com/incidents/ws2l6203cs7g</id>
        <link href="https://www.githubstatus.com/incidents/ws2l6203cs7g"/>
        <updated>2023-06-29T06:43:57.000Z</updated>
        <summary type="html"><![CDATA[Jun 29, 06:43 UTC
Resolved - This incident has been resolved.
Jun 29, 06:41 UTC
Update - With the fix deployed, we are now seeing recovery across impacted scenarios.
Jun 29, 06:05 UTC
Update - We are continuing to rollout the fix for failing Actions jobs using the deployment environments feature. We are expecting the remaining rollout to take another hour
Jun 29, 05:24 UTC
Update - We are continuing to monitor the rollout of the fix for failing Actions jobs using the deployment environments feature. We are expecting the rollout to take up to another hour
Jun 29, 04:54 UTC
Update - We are continuing to monitor the rollout of the fix for failing Actions jobs using the deployment environments feature. We are expecting the rollout to take up to another 1 to 1.5 hours.
Jun 29, 04:19 UTC
Update …]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Issues and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/yr8dbc2p1ny4</id>
        <link href="https://www.githubstatus.com/incidents/yr8dbc2p1ny4"/>
        <updated>2023-06-28T11:37:54.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 11:37 UTC
Resolved - This incident has been resolved.
Jun 28, 11:24 UTC
Update - We are investigating reports of issues with service(s): Issues, Pull Requests, Actions, Releases. We will continue to keep users updated on progress towards mitigation.
Jun 28, 11:21 UTC
Investigating - We are investigating reports of degraded performance for Issues and Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/xg8t6gl5shmp</id>
        <link href="https://status.make.com/incidents/xg8t6gl5shmp"/>
        <updated>2023-06-28T09:00:21.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 11:00 CEST
Completed - The scheduled maintenance has been completed.
Jun 28, 09:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 22, 17:01 CEST
Scheduled - Please note that we will be undergoing scheduled infrastructure maintenance on Wednesday June 28th between 09:00 AM and 11:00 AM CEST on European zones. During this maintenance you can expect few short service interruptions.
During the maintenance the us1.make.celonis.com, us1.make.com websites login might be temporarily affected.
Some webhooks and shared hooks processed at that time might be temporarily affected as well.
There will be no impact on eu1.make.com, eu2.make.com and eu1.make.celonis.com.
We do not expect any data loss.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled infrastructure maintenance]]></title>
        <id>https://status.make.com/incidents/jq61fcn60dfm</id>
        <link href="https://status.make.com/incidents/jq61fcn60dfm"/>
        <updated>2023-06-28T05:00:03.000Z</updated>
        <summary type="html"><![CDATA[Jun 28, 07:00 CEST
Completed - The scheduled maintenance has been completed.
Jun 28, 05:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 22, 16:59 CEST
Scheduled - Please note that we will be undergoing scheduled infrastructure maintenance on Wednesday June 28th between 05:00 AM and 07:00 AM CEST on European zones. During this maintenance you can expect few short service interruptions.
During the maintenance the eu1.make.celonis.com, eu1.make.com and eu2.make.com websites login might be temporarily affected.
Some webhooks and shared hooks processed at that time might be temporarily affected as well.
There will be no impact on us1.make.com and us1.make.celonis.com.
We do not expect any data loss.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may be experiencing issues loading threads or connecting to Slack.]]></title>
        <id>https://status.slack.com//2023-06/511f645055892966</id>
        <link href="https://status.slack.com//2023-06/511f645055892966"/>
        <updated>2023-06-27T22:55:04.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:


From 12:49pm PDT on Tuesday June 27th 2023 to 13:20 PDT on Tuesday, June 27th, 2023, approximately 1% of users were seeing issues loading Slack.


The issue was caused by Slack not allocating resources effectively in our backend database. Once resources were reallocated, the issue was resolved. Our engineering team is taking a look at the cause to make sure it does not occur in the future.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Users unable to create user groups]]></title>
        <id>https://status.slack.com//2023-06/e9d38ae3f06e5cfe</id>
        <link href="https://status.slack.com//2023-06/e9d38ae3f06e5cfe"/>
        <updated>2023-06-27T20:50:05.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From June 26, 2023 at 8:23 AM PDT to June 27, 2023 2:03 AM PDT, some users encountered an error message when creating user groups.


This issue was a result of a change to the user group creation flow. Once this change was reverted, the ability to create user groups was restored for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Issues sending messages with files]]></title>
        <id>https://status.slack.com//2023-06/40f7ccc7ae160447</id>
        <link href="https://status.slack.com//2023-06/40f7ccc7ae160447"/>
        <updated>2023-06-27T20:42:29.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From 00:00 AM PDT on Saturday, June 24th, 2023 to Monday, June 26th, 2023 10:18 AM PDT, some users on the desktop and browser apps encountered a 'failed to send' error message when trying to send messages with files attached. 


The issue was caused by a recent change to how file permissions are read when files are shared. This change was reverted, which resolved the problem for impacted users. 


Our team is making efforts to prevent issues like this from reoccurring in the future. Thank you for your patience while we sorted out this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Google Analytics 4 ⚡]]></title>
        <id>268097</id>
        <link href="https://changelog.bookingsync.com/introducing-google-analytics-4-268097"/>
        <updated>2023-06-27T08:11:46.000Z</updated>
        <summary type="html"><![CDATA[Improvement
  
We would like to inform you that we have recently updated our data tracking system through our Website app to adjust to Google Analytics 4.
This new version offers improved features for tracking and analyzing the performance of your website.
However, for this update to be effective and to ensure your data’s tracking, we kindly ask you to replace the Google Analytics key in the preferences of your Website application.
This key is essential for your website data to be properly collected and analyzed.
What should I do to update my key?
We invite you to follow the instructions below to make this update:
Log in to your Google Analytics account.
Go to the “Admin” section of your account.
Select the property of your website.
Click on “Data Stream” and then open the window to get your Google Analytics 4 key code.
Copy the new Google Analytics 4 key and replace the old key in the Preferences of your Website application.
If needed, you can access the video instructions here.
Thank you for your cooperation in this important update. For your information, your GA3 key is still valid until the 1st of July 2023.
If you have any questions or need any help, please do not hesitate to contact us via this form.]]></summary>
        <author>
            <name>Ella, Head of Customer Care and OnBoarding team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in AMS3]]></title>
        <id>https://status.digitalocean.com/incidents/506776wyjn35</id>
        <link href="https://status.digitalocean.com/incidents/506776wyjn35"/>
        <updated>2023-06-25T18:12:51.000Z</updated>
        <summary type="html"><![CDATA[Jun 25, 18:12 UTC
Resolved - As of 15:31 UTC our Engineering team has confirmed that the issue impacting Networking in AMS3 region has been fully resolved.
From 07:40 UTC to 15:31 UTC, Users may have experienced network timeouts, packet loss, and/or increased latency interacting with the resources in the AMS3 region. The impact has been completely subsided and the network connectivity is back to normal for all the services in AMS3 region. 
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jun 25, 15:51 UTC
Monitoring - The network issues affecting our AMS3 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in those regions, including Droplets, Managed Kubernetes, and Managed Database. 
We will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.
Jun 25, 15:18 UTC
Investigating - Our Engineering team is investigating an issue impacting the networking in our AMS3 region. During this time, a subset of users may experience intermittent packet loss or increased latency while interacting with the resources in AMS3 region.
At the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. This will also be impacting services including Spaces, Managed Kubernetes and Managed Databases as well. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions and Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/tcp7119jvq7y</id>
        <link href="https://www.githubstatus.com/incidents/tcp7119jvq7y"/>
        <updated>2023-06-25T07:01:11.000Z</updated>
        <summary type="html"><![CDATA[Jun 25, 07:01 UTC
Resolved - This incident has been resolved.
Jun 25, 06:41 UTC
Update - Actions is experiencing degraded performance. We are continuing to investigate.
Jun 25, 06:40 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Webhook Migration Enforcement - Phase 2]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/vl5ppdvchsjb</id>
        <link href="https://airbnbapi.statuspage.io/incidents/vl5ppdvchsjb"/>
        <updated>2023-06-24T07:00:22.000Z</updated>
        <summary type="html"><![CDATA[Jun 24, 00:00 PDT
Completed - The scheduled maintenance has been completed.
Jun 19, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 16, 14:55 PDT
Scheduled - The deprecation date for API Webhook Version 2021.12.31 was May 31, 2023. Any applications that have not fully migrated webhooks will be automatically migrated to a newer version, which can result in application breakage.
Automatic webhook migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 2 will occur during the week of June 19th, 2023 and will automatically migrate all Messaging, Reviews, and Reservations webhooks.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Processing in AMS3]]></title>
        <id>https://status.digitalocean.com/incidents/vyk14zgrg3kd</id>
        <link href="https://status.digitalocean.com/incidents/vyk14zgrg3kd"/>
        <updated>2023-06-24T01:36:26.000Z</updated>
        <summary type="html"><![CDATA[Jun 24, 01:36 UTC
Resolved - As of 01:30 UTC, our Engineering team has confirmed full resolution of the issue with processing events in AMS3 region. All impacted events have been marked as failed and users should now be able to process events normally for Droplets and Droplet-based services like Load Balancers, Kubernetes or Database clusters, etc.
If you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.
Thank you for your patience and we apologize for any inconvenience.
Jun 24, 01:06 UTC
Monitoring - From 23:50 UTC to 00:30 UTC, our Engineering team observed an issue with processing events in our AMS3 region. 
Users may have experienced delays or errors while creating and deleting Droplets and Droplet-based products like Load Balancers, Kubernetes or Database clusters, etc. From 00:30 UTC, Users can try performing events/tasks that were failing earlier. 
At this time, our Engineering team is monitoring this incident. We will post an update as soon as we have further information.
Jun 24, 00:28 UTC
Investigating - Beginning 23:50 UTC, our Engineering team is investigating an issue with processing events in our AMS3 region. During this time users may experience delays or errors while creating and deleting Droplets and Droplet-based products like Load Balancers, Kubernetes, or Database clusters, etc.
As soon as we have further information to share, we'll post an update.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues with sending commands to macOS devices]]></title>
        <id>https://status.rippling.com/incidents/psdc4zd8pnqd</id>
        <link href="https://status.rippling.com/incidents/psdc4zd8pnqd"/>
        <updated>2023-06-23T21:46:18.000Z</updated>
        <summary type="html"><![CDATA[Jun 23, 21:46 UTC
Resolved - This incident has been resolved.
Jun 23, 21:20 UTC
Monitoring - A fix has been implemented and we are monitoring the results.
Jun 23, 18:57 UTC
Identified - The issue has been identified and a fix is being implemented.
Jun 23, 18:53 UTC
Investigating - When installing new Rippling MDM Enrollment profiles there maybe a delay in establishing a connection to the MDM server.  This may also cause issues for new installations of the Rippling agent.]]></summary>
        <author>
            <name>Rippling Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/l1v69r4fm1jx</id>
        <link href="https://airbnbapi.statuspage.io/incidents/l1v69r4fm1jx"/>
        <updated>2023-06-23T20:08:53.000Z</updated>
        <summary type="html"><![CDATA[Jun 23, 13:08 PDT
Resolved - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started today (June 23, 2023) around 01:00 PM PDT. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.
The issue has been mitigated at 1:25PM PDT. Please apply any failed requests during the incident.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/tx6z0zm2rfmg</id>
        <link href="https://airbnbapi.statuspage.io/incidents/tx6z0zm2rfmg"/>
        <updated>2023-06-22T22:09:15.000Z</updated>
        <summary type="html"><![CDATA[Jun 22, 15:09 PDT
Resolved - This incident has been mitigated. Please apply any failed requests that were made during the incident time.
Jun 22, 13:26 PDT
Investigating - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started today (June 22, 2023) around 12:30 PM PDT. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Degraded Functions Service]]></title>
        <id>https://status.digitalocean.com/incidents/9hwth0p75c6y</id>
        <link href="https://status.digitalocean.com/incidents/9hwth0p75c6y"/>
        <updated>2023-06-21T18:05:27.000Z</updated>
        <summary type="html"><![CDATA[Jun 21, 18:05 UTC
Resolved - Our Engineering team has confirmed that the issue impacting our Functions service has been fully resolved. If you continue to experience any issues in relation to this incident please open a ticket with our support team. Thank you for your patience.
Jun 21, 17:11 UTC
Monitoring - Functions in BLR1 region are now restored to normal service. As of 17:00 UTC, all impact has been mitigated and users should no longer experience any issues with the Functions. We are monitoring the situation and will post an update once the incident is completely resolved.
Thank you for your patience and we apologize for the inconvenience.
Jun 21, 16:50 UTC
Update - Functions in FRA1 and LON1 are now restored to normal service. We continue work to remediate BLR1.
Our Engineering team has discovered that some Function invocations were duplicated, so a subset of users may have experienced that during the course of this incident.
We will post an update once the last region has returned to normal service.
Jun 21, 16:01 UTC
Identified - Our Engineering team has been working in order to restore the Functions service. At this time, normal service has been restored in most of the regions including AMS3, SGP1, TOR1, and SYD1.
We’re now working to fully remediate the remaining regions: BLR1, FRA1, and LON1. We will post an update as soon as that has completed. Thank you for your patience!
Jun 21, 15:43 UTC
Investigating - Beginning 14:37 UTC, our Engineering team has identified an issue impacting Functions in AMS3, BLR1, FRA1, LON1, SGP1, SYD1, and TOR1. During this time, users with standalone Functions or Functions deployed through Apps may experience performance degradation, leading to delays in Functions invocations and/or Functions working more slowly than normal.
Our team is working to mitigate the issue and we will provide an update as soon as possible.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Connectivity in European Regions]]></title>
        <id>https://status.digitalocean.com/incidents/bclmy32d12p0</id>
        <link href="https://status.digitalocean.com/incidents/bclmy32d12p0"/>
        <updated>2023-06-21T14:31:05.000Z</updated>
        <summary type="html"><![CDATA[Jun 21, 14:31 UTC
Resolved - From 12:00 - 13:40 UTC our Engineering team observed an issue with network connectivity to US-EAST-1 and US-EAST-2 (AWS) from European datacenter regions including DigitalOcean's AMS, FRA & LON datatancers. During this time, a subset of users might have experienced high latency or errors while connecting to  services, including Droplets and Droplet-based products. We also received reports of issues in pushing/pulling images to and from DigitalOcean Container Registries. 
As of 13:40 UTC the impact has been subsided and users should no longer be facing network connectivity issues. We apologize for the inconcenience and if you are still experiencing issues or have any additional questions then please open a support ticket from within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issue affecting listings bedrooms, bathrooms and beds]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/620bns7c4y3p</id>
        <link href="https://airbnbapi.statuspage.io/incidents/620bns7c4y3p"/>
        <updated>2023-06-20T21:10:48.000Z</updated>
        <summary type="html"><![CDATA[Jun 20, 14:10 PDT
Resolved - This incident has been resolved. We have restored the correct bedroom, bathroom and bed count on the Listings API.
Jun 15, 20:37 PDT
Update - We have implemented a fix to restore the correct bedrooms, bathrooms and bed values on the Listings API and will continue to monitor this incident.
We apologize for any inconvenience this may cause and appreciate your understanding as we resolve this issue.
Jun 15, 14:38 PDT
Monitoring - We are currently addressing a bug that affects the bedroom, bathroom, and bed count on our Listings API. Due to a change in behavior of how bedrooms, bathrooms and beds interact with the Listing Rooms API, you might come across the following issues:
* On the Listings API, some listings might have the fields `bedrooms`,`bathrooms` and/or `beds` set to 0.
* On the Listings Rooms API, some rooms might have been created/deleted.
As of today, we have reverted the problematic behavior and are working diligently to restore the correct bedrooms, bathrooms, and beds values on the Listings API. To ensure that your Listing Rooms are accurately set, we kindly ask you to re-sync your data.
We apologize for any inconvenience this may cause and appreciate your understanding as we resolve this issue.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions]]></title>
        <id>https://www.githubstatus.com/incidents/22qm4j1kvn0h</id>
        <link href="https://www.githubstatus.com/incidents/22qm4j1kvn0h"/>
        <updated>2023-06-19T23:09:51.000Z</updated>
        <summary type="html"><![CDATA[Jun 19, 23:09 UTC
Resolved - This incident has been resolved.
Jun 19, 22:47 UTC
Investigating - We are investigating reports of degraded performance for Actions.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Issues Affecting Guest Name Parameters in Webhooks]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/qn4kgtck381x</id>
        <link href="https://airbnbapi.statuspage.io/incidents/qn4kgtck381x"/>
        <updated>2023-06-19T14:46:01.000Z</updated>
        <summary type="html"><![CDATA[Jun 19, 07:46 PDT
Resolved - This incident has been resolved. However, there remains a minor possibility that reservation acceptance confirmation webhooks may be sent without the guest's last name due to an Airbnb race condition. In case you encounter this issue, please  you can retrieve the missing information with the GET Reservations API.
We apologize for the inconvenience this may have caused and don't hesitate to reach out if you have any questions, or require further assistance.
Jun 14, 16:46 PDT
Identified - Unfortunately, the mitigation did not resolve the issue and we are still seeing reservation acceptance confirmation webhooks missing the guest last name. Please know our engineering teams are working to get this issue resolved, and we will update you with the latest information as soon as possible. Meanwhile, please use the GET reservations API for any reservations that did not include the guest last name within the webhooks. We apologize for the inconvenience.
Jun 12, 08:41 PDT
Monitoring - We have implemented a fix for this issue and are monitoring the results. Apologies for the inconvenience caused.
Jun  9, 00:12 PDT
Identified - The issue has been identified and we are working on a fix.
Meanwhile, please attempt GET reservations requests for any reservations that did not include the guest names in full within the webhooks.
Jun  8, 19:36 PDT
Investigating - We are currently investigating an issue where guest names are not returned in full within webhooks - namely the reservation acceptance confirmation webhooks. This is not a widespread issue and is only affecting a small percentage of webhooks.
Alternatively, you may be able to retrieve the details with GET reservations requests.
We will post updates as soon as we know more.
Apologies for the inconvenience caused and thank you for your understanding.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[v2021.12.31 Webhook Migration Enforcement - Phase 1]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/bf02v30139sd</id>
        <link href="https://airbnbapi.statuspage.io/incidents/bf02v30139sd"/>
        <updated>2023-06-17T07:00:20.000Z</updated>
        <summary type="html"><![CDATA[Jun 17, 00:00 PDT
Completed - The scheduled maintenance has been completed.
Jun 13, 00:00 PDT
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 12, 11:00 PDT
Scheduled - The deprecation date for API Webhook Version 2021.12.31 was May 31, 2023. Any applications that have not fully migrated webhooks will be automatically migrated to a newer version, which can result in application breakage.
Automatic webhook migration will occur in phases as detailed in https://developer.airbnb.com/docs/versioning-enforcement
Phase 1 will occur during the week of June 12th, 2023 and will automatically migrate all async webhooks (except Messaging, Reviews and Reservations).]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[500 Errors Across Multiple Endpoints]]></title>
        <id>https://airbnbapi.statuspage.io/incidents/l4t3crp7lr61</id>
        <link href="https://airbnbapi.statuspage.io/incidents/l4t3crp7lr61"/>
        <updated>2023-06-17T01:52:37.000Z</updated>
        <summary type="html"><![CDATA[Jun 16, 18:52 PDT
Resolved - This incident has now been resolved.
Jun 16, 16:38 PDT
Monitoring - An issue has been deployed and we are monitoring the results - the 500 error rate has significantly dropped.
Jun 16, 15:44 PDT
Identified - We are actively investigating an increased number of 500 errors and an increase in latency across multiple endpoints. These errors started today (June 16, 2023) around 14:20 PDT. If your API requests fail with a 500, please retry them. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.]]></summary>
        <author>
            <name>Airbnb API Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating reports of degraded performance.]]></title>
        <id>https://www.githubstatus.com/incidents/psdymfgt6p7q</id>
        <link href="https://www.githubstatus.com/incidents/psdymfgt6p7q"/>
        <updated>2023-06-16T23:59:23.000Z</updated>
        <summary type="html"><![CDATA[Jun 16, 23:59 UTC
Resolved - This incident has been resolved.
Jun 16, 23:46 UTC
Update - Users may be unable to access org-owned resources in GitHub-owned client apps such as VS Code, GitHub Desktop, GitHub Mobile, and the CLI. We are working on reverting a change that is causing this
Jun 16, 23:46 UTC
Investigating - We are currently investigating this issue.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elevated Error Rate for Volumes and Snapshots]]></title>
        <id>https://status.digitalocean.com/incidents/7dgq9yz9ch1n</id>
        <link href="https://status.digitalocean.com/incidents/7dgq9yz9ch1n"/>
        <updated>2023-06-16T19:00:00.000Z</updated>
        <summary type="html"><![CDATA[Jun 16, 19:00 UTC
Resolved - From 19:12 - 19:40 UTC, we experienced an issue with Volumes and Snapshots endpoints. During that time, we saw elevated 500 errors and a subset of users experienced latency or errors while loading the Volumes or Images tab in the Cloud Control Panel. A subset of users also experienced errors in both the Cloud Control Panel and the API for CRUD (create, read, update, and delete) operations on Volumes and Snapshots. 
Our Engineering team was able to identify the root cause and quickly deployed a configuration fix to resolve the situation.  
We apologize for the inconvenience. If you are still experiencing any problems or have additional questions, please open a support ticket within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with Google Calendar and Outlook Calendar App]]></title>
        <id>https://status.slack.com//2023-06/4edd40b2f4290679</id>
        <link href="https://status.slack.com//2023-06/4edd40b2f4290679"/>
        <updated>2023-06-16T12:00:18.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:

From June 13, 2023 at 12:30 PM PDT until June 15, 2023 at 2:30 AM PDT, some users may have experienced issues with statuses, reminders and agendas failing to sync from their Google Calendar and Outlook Calendar apps.


These issues were due to an outage from an upstream service provider which affected many services. The provider actively worked to remediate those issues, which then fully resolved the reported failures for impacted Slack users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Some users may notice slow performance throughout Slack]]></title>
        <id>https://status.slack.com//2023-06/7363f55dbcde89c1</id>
        <link href="https://status.slack.com//2023-06/7363f55dbcde89c1"/>
        <updated>2023-06-15T23:28:00.000Z</updated>
        <summary type="html"><![CDATA[Issue summary:


On June 15, 2023 from 8:10 AM PDT to around 8:20 AM PDT, some users may have experienced slow performance and errors while loading conversations, sending messages and taking other actions in Slack.


We traced the issue to a recent code change which caused connection failures. We reverted this change and all affected functionality should be restored for users. Thank you for your patience while we resolved this issue.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Copilot]]></title>
        <id>https://www.githubstatus.com/incidents/6tfr2b2s2073</id>
        <link href="https://www.githubstatus.com/incidents/6tfr2b2s2073"/>
        <updated>2023-06-15T19:36:17.000Z</updated>
        <summary type="html"><![CDATA[Jun 15, 19:36 UTC
Resolved - This incident has been resolved.
Jun 15, 19:22 UTC
Investigating - We are investigating reports of degraded performance for Copilot.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with workflows, loading channels and accessing Slack]]></title>
        <id>https://status.slack.com//2023-06/20df18536cc9524b</id>
        <link href="https://status.slack.com//2023-06/20df18536cc9524b"/>
        <updated>2023-06-15T18:31:06.000Z</updated>
        <summary type="html"><![CDATA[Issue Summary:

From June 14, 2023 at 3:44 PM PDT until June 15, 2023 at 4:30 AM PDT, some customers may have been unable to send messages, join Huddles, run workflows, and access Slack.


A recent code change caused some resources on our backend web hosting service to be temporarily exhausted. Due to this, users experienced errors when joining Huddles, sending messages, and may have had their connection to Slack reset. We identified the issue and reverted the change restoring full functionality to these features.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pages]]></title>
        <id>https://www.githubstatus.com/incidents/h3vqt1fvp9tc</id>
        <link href="https://www.githubstatus.com/incidents/h3vqt1fvp9tc"/>
        <updated>2023-06-15T16:57:15.000Z</updated>
        <summary type="html"><![CDATA[Jun 15, 16:57 UTC
Resolved - This incident has been resolved.
Jun 15, 16:47 UTC
Update - Due to an issue with Let's Encrypt, obtaining a new HTTPS certificate for Pages sites is delayed.
Jun 15, 16:34 UTC
Investigating - We are investigating reports of degraded performance for Pages.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Networking in Europe Datacenters (AMS, FRA, LON)]]></title>
        <id>https://status.digitalocean.com/incidents/1d5qv6pmsv6b</id>
        <link href="https://status.digitalocean.com/incidents/1d5qv6pmsv6b"/>
        <updated>2023-06-15T03:37:51.000Z</updated>
        <summary type="html"><![CDATA[Jun 15, 03:37 UTC
Resolved - Our Engineering team has confirmed the full resolution of the issue impacting networking in our European regions.
From 16:40 UTC to 02:10 UTC, Users may have experienced network timeouts, packet loss, and/or increased latency interacting with the resources in these regions (AMS2/3, FRA1 & LON1).
If you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for the inconvenience.
Jun 15, 02:40 UTC
Monitoring - The network issues affecting our European regions have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in those regions, including Droplets, Managed Kubernetes, and Managed Database.
We will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.
Jun 15, 01:26 UTC
Update - Our engineering team continues to investigate the ongoing issues impacting the networking in our European data centres including AMS2/3, FRA1 & LON1 regions. During this time, you may experience intermittent packet loss or increased latency while interacting with the resources in these regions. We apologize for the inconvenience and will share an update once we have more information.
Jun 14, 20:06 UTC
Investigating - From 16:40 UTC our Engineering team is investigating an issue impacting the networking in our European data centers including AMS2/3, FRA1 & LON1 regions. During this time, you may experience intermittent packet loss or increased latency while interacting with the resources in these regions.
At the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. This will also be impacting services including Spaces and Managed Databases. 
We apologize for the inconvenience and will share an update once we have more information.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scheduled Maintenance]]></title>
        <id>https://status.make.com/incidents/413nrl6rvmsx</id>
        <link href="https://status.make.com/incidents/413nrl6rvmsx"/>
        <updated>2023-06-14T11:00:40.000Z</updated>
        <summary type="html"><![CDATA[Jun 14, 13:00 CEST
Completed - The scheduled maintenance has been completed.
Jun 14, 11:00 CEST
In progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.
Jun 13, 15:02 CEST
Scheduled - We would like to inform you about a scheduled maintenance window that will be taking place on 14th June between 11:00 CEST - 13:00 CEST.
During this maintenance period, there will be a short interruption resulting in the unavailability of the login page and organisations page. 
If you experience a blank page , clearing your browser cache might resolve the issue . If not please reach out to our customer care team.
However, we assure you that there will be no data loss and all scenarios will continue running properly.
We apologize for any inconvenience caused and appreciate your understanding.]]></summary>
        <author>
            <name>Make Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Trouble with workflows and clips transcripts]]></title>
        <id>https://status.slack.com//2023-06/a0c185e5ad5eac55</id>
        <link href="https://status.slack.com//2023-06/a0c185e5ad5eac55"/>
        <updated>2023-06-14T03:06:22.000Z</updated>
        <summary type="html"><![CDATA[Issue summary: 

On June 13, 2023 from 11:55 AM PDT to around 3:47 PM PDT, some users may have experienced failure with clips transcribing and transcoding, execution of workflows with custom functions, new deployments of apps with hosted functions, and some delay when using canvases in Slack. 


These issues were due to an outage from an upstream service provider which affected many services. The provider actively worked to remediate those issues, which then fully resolved the reported failures for impacted Slack users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Actions and Pages]]></title>
        <id>https://www.githubstatus.com/incidents/csnbwc85chp2</id>
        <link href="https://www.githubstatus.com/incidents/csnbwc85chp2"/>
        <updated>2023-06-13T00:17:05.000Z</updated>
        <summary type="html"><![CDATA[Jun 13, 00:17 UTC
Resolved - This incident has been resolved.
Jun 13, 00:09 UTC
Update - We have applied a mitigation and are seeing signs of recovery. We will post another update shortly.
Jun 12, 23:39 UTC
Update - We are investigating reports of degraded performance for Actions. We have identified the root cause and have begun applying a mitigation.
Jun 12, 22:48 UTC
Update - Pages is experiencing degraded performance. We are still investigating and will provide an update when we have one.
Jun 12, 22:43 UTC
Investigating - We are investigating reports of degraded performance for Actions.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container Registry in FRA1 and SGP1]]></title>
        <id>https://status.digitalocean.com/incidents/rptq6jh8ljd7</id>
        <link href="https://status.digitalocean.com/incidents/rptq6jh8ljd7"/>
        <updated>2023-06-12T17:44:00.000Z</updated>
        <summary type="html"><![CDATA[Jun 12, 17:44 UTC
Resolved - From 04:30 to 09:30 UTC, our Engineering team observed issues with DigitalOcean Container Registry in SGP1 and FRA1 regions. During this time, users might have experienced errors while interacting with their registries in SGP1 and FRA1 regions. Users might also have experienced latency in pushing/pulling images to/from registries.
Our Engineering team has observed that the impact has been mitigated. As of 17:20 UTC, users should no longer experience any issues with pushing/pulling images to/from their registries in SGP1 and FRA1 regions. We apologize for the inconvenience. If you are still experiencing any problems or have additional questions, please open a support ticket within your account.]]></summary>
        <author>
            <name>DigitalOcean Status - Incident History</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident: Small number of users might be experiencing connectivity issues]]></title>
        <id>https://status.slack.com//2023-06/1bd97a73cb7874f1</id>
        <link href="https://status.slack.com//2023-06/1bd97a73cb7874f1"/>
        <updated>2023-06-12T14:56:48.000Z</updated>
        <summary type="html"><![CDATA[On June 11, 2023 from 10:57 AM PDT to 12:10 PM PDT, some users may have experienced difficulties when trying to access Slack due to a database issue.


The underlying cause has since been determined and remediation steps were applied. This resolved the above for all impacted users.]]></summary>
        <author>
            <name>Slack System Status</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incident with Pull Requests]]></title>
        <id>https://www.githubstatus.com/incidents/0csnqhwzxp1m</id>
        <link href="https://www.githubstatus.com/incidents/0csnqhwzxp1m"/>
        <updated>2023-06-09T23:17:57.000Z</updated>
        <summary type="html"><![CDATA[Jun  9, 23:17 UTC
Resolved - This incident has been resolved.
Jun  9, 22:46 UTC
Update - We are investigating elevated error rates on Pull Requests. We will continue to keep users updated on progress towards mitigation.
Jun  9, 22:18 UTC
Investigating - We are investigating reports of degraded performance for Pull Requests.]]></summary>
        <author>
            <name>GitHub Status - Incident History</name>
        </author>
    </entry>
</feed>