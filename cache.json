{
  "sources": [
    {
      "title": "BookingSync.com news",
      "feedUrl": "https://changelog.bookingsync.com/rss",
      "siteUrl": "https://changelog.bookingsync.com",
      "articles": [
        {
          "id": "283940",
          "author": "Basile, Product Manager",
          "description": "Improvement\n  \nIntroducing Smily’s enhanced deposit management: seamless synchronization with booking.com\nSay goodbye to data discrepancies and hello to effortless property management!\nKey Features:\nEasier deposit handling: Our system now allows you to manage properly damage deposits directly through Smily, ensuring perfect synchronization with Booking.com.\nNo more data mismatch: We’ve revolutionized our sync logic to eliminate discrepancies between Smily and Booking.com. This means the information you see on one platform is exactly what you’ll find on the other.\nEnhanced user experience: Managing your properties has never been smoother. Our updates are tailored to make your workflow more intuitive and efficient.\nRevamped collection & return methods: Say farewell to the hassle of cash deposits and wire returns. With our update, deposit collection and returns are streamlined through credit card transactions, simplifying the process for you and your guests.\nBenefits for you:\nPeace of mind: With matched data across platforms, reduce the risk of errors and enjoy a more streamlined management experience.\nTime savings: Minimize manual work, allowing you to focus on growing your business and enhancing guest experiences.\nConsistent information: Rest assured knowing that what you set in Smily is precisely reflected on Booking.com.\n-> Update your knowledge with our revised manual. Get all the latest information and tips for maximizing the benefits of this new feature at Smily's updated manual.\nYour Smily team :)",
          "link": "https://changelog.bookingsync.com/smily-s-enhanced-deposit-management-seamless-synchronization-with-booking-com-283940",
          "publishedOn": "2024-01-22T17:40:37.000Z",
          "wordCount": 456,
          "title": "Smily’s enhanced deposit management: seamless synchronization with booking.com",
          "imageUrl": null
        },
        {
          "id": "283898",
          "author": "Yannick, Customer Care Team Leader - Pro Team",
          "description": "Action required\n  \nWe have an important update regarding your email communications with your guests via the notification app.\nTo maintain uninterrupted service, some adjustments are needed on your domain provider to keep sending notifications from your own domain name.\nIn case you are confident to do the changes yourself, please follow the instructions on our manual and let us know once it is done so we can verify, otherwise please contact our Customer Support team (Yannick or Pauline) to receive proper instructions and/or schedule a video meeting and we will be happy to help you.\nNote: Please make sure you have access to your domain provider before the call.\nYour prompt attention to this matter is appreciated to prevent any disruptions. Please note that the changes need to be done before February 13 2024.\nThank you for your cooperation.",
          "link": "https://changelog.bookingsync.com/action-required-for-continued-email-communication-from-your-own-domain-283898",
          "publishedOn": "2024-01-22T13:29:03.000Z",
          "wordCount": 380,
          "title": "Action required for continued Email communication from your own domain",
          "imageUrl": null
        },
        {
          "id": "283413",
          "author": "Maud , Partnership Manager",
          "description": "New!\n  \n\n\nWe are thrilled to announce a new partnership that will revolutionise key management for vacation rental hosts and property managers like you. 🔑\n  \n\n💡What is KeyNest Points?\nKeyNest Points is a global network of over 5,500 locations where you can securely store and exchange keys with ease. No more hassles of on-site visits or high installation costs.\nMost KeyNest Points are open 24/7, ensuring flexibility for key exchanges at your convenience. Discover your nearest Point and its opening hours on the interactive map provided.\n\n \n\n\n🏠 KeyNest Points - Your Trusted Partners\nThese points are typically local businesses such as convenience stores, cafes, hotels, or petrol stations, conveniently situated near your properties.\nEach KeyNest Point is managed by trained staff, ensuring instant and secure key exchanges. Your peace of mind is paramount.\n\n🔤 How Does it Work?\n\nHere's a quick overview of how KeyNest Points operate:\nDrop your keys at a local KeyNest Point.\nYour keys are tagged and logged into the system for real-time tracking.\nStaff at the Point securely stores your keys.\nSend the location of the key and the code to your guests within your automated Smily messages.\nGuests or cleaners visit the Point, show their safety code, and receive the key.\nUpon return, they drop the key back at the same Point, and you are notified.\n \n👉 Install the Keynest app here\n \n💌 Learn more on our dedicated manual page.\n  \nIf you have any questions, please don't hesitate to contact support@keynest.com.",
          "link": "https://changelog.bookingsync.com/new-partnership-announcement-with-keynest!-283413",
          "publishedOn": "2024-01-16T06:58:43.000Z",
          "wordCount": 474,
          "title": "🆕 New partnership announcement with KeyNest!",
          "imageUrl": "https://cloud.headwayapp.co/changelogs_images/images/big/000/120/909-b0c60a35f9afa4ac910a5afc1d805a93b692e947.png"
        },
        {
          "id": "283019",
          "author": "Basile, Product Manager",
          "description": "New!\n \nImprovement\n  \nWe're thrilled to share a new feature we’ve just released for booking.com properties.\nWhat's New?\nThe rate rule Booking at least 'x' day ahead is now synchronised on booking.com channel;\nWhy Does This Matter?\nYou were facing the obstacle where Guests were making a booking for the same day or 'x' days (based on your settings) even though the setting to prevent that was activated;\n-> It won’t happen anymore.\nWhat's Next?\nIf you face any obstacle with this setting, reach out to us; \nWe are and will be working on more improvements to make your day-to-day easier :)\n--\n💡 For more detailed information, simply click here;\nLet's embark on this journey together to make your vacation rental dreams a reality 🚀\nHave a great day,\nYour Smily team :)",
          "link": "https://changelog.bookingsync.com/new-fix-for-your-booking-com-reservations-283019",
          "publishedOn": "2024-01-10T17:36:04.000Z",
          "wordCount": 362,
          "title": "New fix for your booking.com reservations",
          "imageUrl": null
        },
        {
          "id": "282774",
          "author": "Ella, Chief Customer Officer (CCO) & Cofounder",
          "description": "Communications\n  \n\nAs we step into this new year, we want to take a moment to express our heartfelt gratitude for your trust and partnership. Your journey is at the heart of everything we do, and it's been an incredible experience growing alongside each of you.\nThis past year has been filled with challenges but also triumphs, and through it all, our Property Managers and Owners have always been our main inspiration. We're honored to be a part of your story, helping you manage and grow your vacation rental business.\nLooking ahead, we're excited to continue our journey together. With every challenge comes an opportunity, and we're here to support you every step of the way. Let's make this year one of progress, success, and shared smiles!\nHereby some of Smily's milestones and snapshot of our 2023 key achievements:\nIncrease efficient communication: enhanced Inbox filters for efficient guest communication, enabling quick searches by conversation status, assignee, guest, booking stage, and more.\nPowerful partnerships: Collaborations with Maeva and PlumGuide have opened new horizons and more OTA distribution channels!\nGuest experience & security: From synchronized check-in types to robust website https updates, we're elevating the guest journey.\nSustainable visibility: Synchronization of our eco-amenities to Holidu to boost visibility and conversion rates!\nReviewed payment processes: With easier Stripe account onboarding and better integration but also by streamlining our billing system, managing finances should be smoother than ever.\nReview management made easy: Our AI-powered Review Response and Automated Guest Reviews are setting new standards in reputation management.\nTech Marvels: Our focus on modernization and security has supercharged system stability and performance. Don't miss the insights on our new Tech Blog.\nOnce again, here's to a year filled with happiness, health, and prosperity. Let's keep building dreams and sharing smiles with every soul.\nHappy New Year! 🌟\nWith love and warmth,\nYour Smily Team",
          "link": "https://changelog.bookingsync.com/2024-here-we-are!-happy-new-year-from-all-of-us-at-smily!-282774",
          "publishedOn": "2024-01-08T09:34:22.000Z",
          "wordCount": 556,
          "title": "2024, here we are! Happy New Year from all of us at Smily! 🎉",
          "imageUrl": "https://cloud.headwayapp.co/changelogs_images/images/big/000/120/521-ab80fc2159e2f0d70a2ae0f84b447500ce6e3763.jpg"
        }
      ]
    },
    {
      "title": "Security Bulletins on Tailscale",
      "feedUrl": "https://tailscale.com/security-bulletins/index.xml",
      "siteUrl": "https://tailscale.com/security-bulletins/",
      "articles": [
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2024-001",
          "author": null,
          "description": "Description: On Windows before Tailscale version 1.52 and on Linux before\nTailscale 1.54, the tailscale serve and tailscale funnel features allowed\nusers to serve the contents of directories that their user account could not\naccess, but which the tailscaled service process could.\nWhat happened?\nA user could escalate their own file read access by running, for example,\ntailscale.exe serve http / C:\\, and then browsing to the local HTTP endpoint.\nThe issue can also occur on Linux if the local administrator enabled an operator\nuser ID with tailscale up --operator=$USER, as the $USER account could\nserve itself files that it could not normally read.\nWho is affected?\nOwners of Windows deployments for which the users of Tailscale nodes do not also\nhave OS-level administrative access, and owners of Linux deployments where the\nadministrator enabled non-root --operator access.\nThis issue can only be triggered by a local user and cannot be triggered\nremotely.\nWhat is the impact?\nThis issue enables local privilege escalation (file read access). Access to\ncertain system files (such as /etc/shadow on Linux) can then be used to obtain\nfull administrative control over the host.\nWhat do I need to do?\nOn Windows 10 and later, upgrade to Tailscale 1.52 (released 30 October\n2023) or later, which resolves the issue.\nOn Windows 7 and 8, upgrade to Tailscale 1.44.3 (released 8 Jan\n2024), which resolves the issue.\nOn Linux, upgrade to 1.54 (released 15 November 2023) or later,\nwhich resolves the issue.\nThe best practice is to run the latest stable version, which as of this writing\nis 1.56.1. Consider turning on automatic updates.\nUse Tailscale ACLs to control the availability of Funnel.",
          "link": "https://tailscale.com/security-bulletins/#ts-2024-001",
          "publishedOn": "2024-01-08T00:00:00.000Z",
          "wordCount": 10150,
          "title": "TS-2024-001",
          "imageUrl": "https://cdn.sanity.io/images/w77i7m8x/production/8e0455b2d9b33c6151016afdf2ea81d7623c2f04-1200x628.png"
        }
      ]
    },
    {
      "title": "Airbnb API Status - Incident History",
      "feedUrl": "https://airbnbapi.statuspage.io/history.rss",
      "siteUrl": "https://airbnbapi.statuspage.io",
      "articles": [
        {
          "id": "https://airbnbapi.statuspage.io/incidents/g0r1ty50zxsz",
          "author": null,
          "description": "Jan  5, 19:33 PST\nResolved - This incident has been resolved. Please contact us if you still can't see API-connected listings on the Partner Portal.\nJan  4, 22:32 PST\nUpdate - We are still working on this issue.\nYou may still view listing-related events by filtering by Listing ID on the Partner Portal Events Dashboard.\nYou may also utilise GET requests to view the current snapshots of a listing if required.\nWe apologise for the inconvenience caused.\nJan  4, 08:53 PST\nUpdate - We are still working on this issue, apologies for the inconvenience.\nJan  3, 16:00 PST\nIdentified - We are investigating an issue affecting our Diagnostic tools in the Partner Portal that started today Jan 3, 2024 around 12:00PM PST. Currently, you might not see any API-connected listings on the Listings dashboard, and in a listing's overview page, you might see the following message \"The listing is not API connected\". Please note that no listings were disconnected due to this incident; this issue affects the Partner Portal view only.\nWe are actively working to resolve this issue, and will post more updates as soon as possible.",
          "link": "https://airbnbapi.statuspage.io/incidents/g0r1ty50zxsz",
          "publishedOn": "2024-01-06T03:33:46.000Z",
          "wordCount": 3998,
          "title": "Issues Affecting the Partner Portal",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Twilio Status - Incident History",
      "feedUrl": "https://status.twilio.com/history.rss",
      "siteUrl": "https://status.twilio.com",
      "articles": []
    },
    {
      "title": "DigitalOcean Status - Incident History",
      "feedUrl": "https://status.digitalocean.com/history.rss",
      "siteUrl": "http://status.digitalocean.com",
      "articles": [
        {
          "id": "https://status.digitalocean.com/incidents/2yxmsbx1r89b",
          "author": null,
          "description": "Jan 25, 02:56 UTC\nResolved - Our Engineering team has resolved the issue with snapshots taken by customers in the NYC3 and SFO3 regions. If you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.\nJan 25, 01:50 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue with snapshots taken by customers in the NYC3 and SFO3 regions and are monitoring the situation closely. \nWe will post another update once we're confident that the issue is fully resolved.\nJan 25, 00:26 UTC\nIdentified - Our Engineering team has identified an issue with snapshots taken by customers in the NYC3 and SFO3 regions and is actively working on a fix. We will post an update as soon as additional information is available.",
          "link": "https://status.digitalocean.com/incidents/2yxmsbx1r89b",
          "publishedOn": "2024-01-25T02:56:12.000Z",
          "wordCount": 6332,
          "title": "Snapshots are failing in SFO3 and NYC3",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/s4kt15q6xq19",
          "author": null,
          "description": "Jan 23, 22:50 UTC\nResolved - Our Engineering team has completed mitigation efforts for the issue impacting Managed Kubernetes in the FRA1 region and we are marking this incident as Resolved. \nAt this time, functionality to impacted clusters has been restored but customers may need to reconfigure some Kubernetes resources. Customer Support is contacting impacted customers directly with further instructions. \nIf you have any questions or concerns regarding this incident, please open a ticket with our support team.\nJan 23, 18:44 UTC\nUpdate - Our Engineering team continues to work on mitigation efforts. An additional small bug has been discovered and remediated. About 10% of clusters have had accessibility restored and restoration efforts are ongoing. \nWe will post another update as soon as we have new developments.\nThank you for your patience and we apologize for any inconvenience.\nJan 23, 15:12 UTC\nIdentified - Our Engineering team has identified the cause of the issue with Managed Kubernetes clusters in the FRA1 region. 200 clusters are impacted by the issue and remain inaccessible to users at this time. \nOur Engineering team is engaged in remediating these clusters to restore accessibility. As soon as we are able to provide an estimated time to restore, we will provide an update.\nJan 23, 13:02 UTC\nInvestigating - As of 12:18 UTC, our Engineering team is investigating an issue with Kubernetes clusters in the FRA1 region. During this time, users may experience errors while communicating with their clusters in the FRA1 region. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/s4kt15q6xq19",
          "publishedOn": "2024-01-23T22:50:40.000Z",
          "wordCount": 6444,
          "title": "Managed Kubernetes Cluster in FRA1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/9y133zrfqngf",
          "author": null,
          "description": "Jan 22, 18:38 UTC\nResolved - As of 18:37 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.\nIf you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. \nThank you for your patience and we apologize for any inconvenience.\nJan 22, 13:32 UTC\nMonitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaas, Managed Kubernetes, and Managed Database. \nWe are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.\nJan 22, 12:25 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting networking in the LON1 region and is actively working on a fix.\nDuring this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, LBaaS, Managed Kubernetes, and Managed Databases.\nWe will post an update as soon as additional information is available\nJan 22, 11:36 UTC\nInvestigating - Our Engineering team is investigating a networking issue in our LON1 region. At this time, you may experience packet loss or dropped connections.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/9y133zrfqngf",
          "publishedOn": "2024-01-22T18:38:44.000Z",
          "wordCount": 6418,
          "title": "Network connectivity in LON1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/299xh1cfs8s5",
          "author": null,
          "description": "Jan 22, 10:32 UTC\nResolved - Our Engineering team has resolved the issue impacting Spaces API availability in our SFO2 region. From approximately 09:00 UTC - 10:00 UTC, users may have experienced latency or timeouts when trying to access or manage their Spaces buckets. Spaces should now be operating normally.\nIf you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.\nJan 22, 10:10 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue impacting SFO2 Spaces API availability and monitoring the situation. We will post an update as soon as the issue is fully resolved.\nJan 22, 09:59 UTC\nIdentified - As of 09:50 UTC, our Engineering team has identified the cause of the issue impacting Spaces API availability in our SFO2 region and is actively working on a fix.\nWe will post an update as soon as additional information is available\nJan 22, 09:13 UTC\nInvestigating - As of 09:00 UTC, Our Engineering team is investigating an issue impacting Spaces API availability in our SFO2 region.\nDuring this time, users may experience slowness or timeouts when trying to access or manage their Spaces resources in SFO2.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/299xh1cfs8s5",
          "publishedOn": "2024-01-22T10:32:23.000Z",
          "wordCount": 6397,
          "title": "Spaces availability in SFO2",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/x0f8bvr688rs",
          "author": null,
          "description": "Jan 17, 09:34 UTC\nResolved - As of 09:17 UTC, our Engineering team has confirmed the full resolution of the issue impacting the Droplet rebuild via the Cloud Control Panel.\nWe appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.\nJan 17, 09:22 UTC\nMonitoring - Our Engineering team has taken actions to mitigate the issue impacting the Droplet rebuild via Cloud Control Panel and is monitoring the situation.\nThe impact has been subsided and the users should no longer experience issues when rebuilding Droplets from the Cloud Control Panel. We apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.\nJan 17, 08:26 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting the Droplet rebuild via the Cloud Control Panel and is actively working on a fix. During this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel. We will post an update as soon as additional information is available.\nJan 17, 07:15 UTC\nInvestigating - As of 06:50 UTC, our Engineering team is investigating an issue impacting the Droplet rebuild via the Cloud Control Panel.\nDuring this time, users may get an error response when trying to rebuild the Droplet via the Cloud Control Panel.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/x0f8bvr688rs",
          "publishedOn": "2024-01-17T09:34:03.000Z",
          "wordCount": 6428,
          "title": "Droplet rebuild",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/j4jwmwl3szxz",
          "author": null,
          "description": "Jan 16, 13:34 UTC\nResolved - Our Engineering team has resolved the issue impacting multiple spaces-related functionalities. From approximately 10:30 UTC - 13:30 UTC, users may have experienced issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API. Spaces-related functionalities should now be operating normally.\nIf you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.\nJan 16, 12:51 UTC\nMonitoring - Our Engineering team has taken actions to mitigate the issue affecting multiple Spaces-related functionalities and is monitoring the situation.\nThe impact has been subsided and the users should no longer experience issues with Spaces-related functionalities. \nWe apologize for the inconvenience and we will post an update once we confirm this incident is fully resolved.\nJan 16, 12:03 UTC\nIdentified - Our Engineering team has identified the issue affecting multiple Spaces-related functionalities and is actively working on a fix.\nDuring this time, users may experience issues while trying to perform multiple actions on Spaces via the Cloud Control Panel and API.\nAdditionally, this may also impact Container Registry creation and issues with transferring images between regions. \nWe apologize for the inconvenience and will share an update once we have more information.\nJan 16, 10:54 UTC\nInvestigating - As of 10:30 UTC, our Engineering team is investigating an issue with multiple Spaces functionalities via the Cloud Control Panel. \nDuring this time, users may experience errors when attempting to delete objects via the Cloud Control Panel. At this moment we are investigating the exact impact and will share more information as soon as we have it.\nWe apologize for the inconvenience.",
          "link": "https://status.digitalocean.com/incidents/j4jwmwl3szxz",
          "publishedOn": "2024-01-16T13:34:24.000Z",
          "wordCount": 6473,
          "title": "Spaces Functionality",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/47dqh6pzqybt",
          "author": null,
          "description": "Jan 16, 09:30 UTC\nResolved - As of 08:50 UTC, our Engineering team has confirmed the full resolution of the issue impacting the ability to access and manage Functions through the Cloud Control Panel.\nWe appreciate your patience throughout the process. If you continue to experience problems, please open a ticket with our support team.\nJan 16, 08:59 UTC\nMonitoring - Our Engineering team has been able to mitigate the issue related to the access and operations with Functions through the Cloud Control Panel.\nUsers should no longer face any problems in accessing or operating Functions using the Cloud Control Panel. \nWe apologize for the inconvenience. We are monitoring the situation and will post an update once we confirm this incident is fully resolved.\nJan 16, 08:17 UTC\nInvestigating - As of 03:00 AM UTC, our Engineering team is investigating an issue impacting Functions. \nDuring this time, users may experience issues with accessing Functions via the Cloud Control Panel. At this time, API operations shouldn't be impacted and should continue to function as intended. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/47dqh6pzqybt",
          "publishedOn": "2024-01-16T09:30:15.000Z",
          "wordCount": 6364,
          "title": "Functions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/33vqf05m8396",
          "author": null,
          "description": "Jan 11, 00:18 UTC\nResolved - Our Engineering team has confirmed full resolution of this incident. \nFrom approximately 20:15 - 21:45 UTC, DigitalOcean experienced a global networking issue that impacted multiple services and products. Users saw increased error rates and latency for event processing, accessing our Cloud Control Panel/API, applying Cloud Firewall policies, accessing www.digitalocean.com and our Community site, and DNS resolution. Additionally, users saw timeouts/increased latency for networking requests to Droplets and Droplet-based services, as well as connections to existing App Platform Apps. \nWe sincerely apologize for the disruption. If you continue to experience issues or have questions, please reach out to our Support team by opening a ticket from within your account. …",
          "link": "https://status.digitalocean.com/incidents/33vqf05m8396",
          "publishedOn": "2024-01-11T00:18:25.000Z",
          "wordCount": 6738,
          "title": "Global Networking",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/8fm83lgfsdhm",
          "author": null,
          "description": "Jan  9, 12:55 UTC\nResolved - As of 12:30  UTC, our Engineering team has resolved the issue impacting CRUD (create, read, update, delete) operations for Managed Database Clusters in all the regions.\nEverything should now be functioning normally. We appreciate your patience throughout the process.\nIf you continue to experience problems, please open a ticket with our support team.\nJan  9, 10:17 UTC\nMonitoring - Our Engineering team has confirmed that the action taken to mitigate the recurrence of the issue is successful. Users should no longer experience errors in CRUD (create, read, update, delete) operations for Managed Database Clusters in all regions, via both the Cloud Control Panel and API requests.\nWe are actively monitoring the situation to ensure stability and will provide an update …",
          "link": "https://status.digitalocean.com/incidents/8fm83lgfsdhm",
          "publishedOn": "2024-01-09T12:55:23.000Z",
          "wordCount": 6547,
          "title": "Managed Database Operations",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/88sl0dldq6dh",
          "author": null,
          "description": "Jan  8, 15:57 UTC\nResolved - As of 15:00 UTC, Our engineering team has resolved the issue impacting Container Registry  and App platform builds in various regions including AMS3, LON1 and FRA1 regions. \nEverything should now be functioning normally. We appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.\nJan  8, 15:13 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue impacting Container Registry App platform builds in AMS3, LON1 and FRA1 regions. User should not be facing any issues while interacting with their Container registries and also while building their Apps. \nWe are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. \nThank you for your patience and we apologize for the inconvenience.\nJan  8, 14:15 UTC\nInvestigating - Our Engineering team is investigating an issue with DigitalOcean Container Registry service in our AMS3 and LON1 regions. \nDuring this time a subset of customers may experience latency while interacting with the Container Registries. This is also impacting App Platform builds and users may encounter delays in while building their Apps and could potentially experience timeout errors in builds as a result. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/88sl0dldq6dh",
          "publishedOn": "2024-01-08T15:57:06.000Z",
          "wordCount": 6424,
          "title": "Container Registry and App Platform in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/x0l7xgm3bc91",
          "author": null,
          "description": "Jan  8, 00:32 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue with authentication failures on Managed Kubernetes Clusters.\nFrom 19:26 - 23:29 UTC, users saw errors accessing and performing actions on Managed Kubernetes Clusters, due to authentication failures. All services are now operating normally.  \nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!\nJan  8, 00:01 UTC\nMonitoring - Our Engineering team has confirmed that the action taken to mitigate the impact of this incident is successful and customers are no longer seeing issues accessing or performing actions on Managed Kubernetes Clusters. \nWe'll monitor the situation for a short while and will post a final update once we confirm full resolution.\nJan  7, 23:32 UTC\nIdentified - Our Engineering team has taken action to mitigate the impact of this incident and internal tests on impacted clusters are now passing. At this time, users should start to see recovery and be able to access clusters normally. \nWe are still investigating root cause and ensuring this mitigation will continue to be successful. We'll provide another update as soon as possible.\nJan  7, 22:02 UTC\nInvestigating - Our Engineering team is investigating customer reports of errors when trying to perform actions on Managed Kubernetes Clusters using kubectl. At this time, customers may see errors indicating that the API token is unauthorized or missing credentials. We will provide an update as soon as we have more information.",
          "link": "https://status.digitalocean.com/incidents/x0l7xgm3bc91",
          "publishedOn": "2024-01-08T00:32:13.000Z",
          "wordCount": 6444,
          "title": "Kubernetes Clusters - Authentication Errors",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/1pbm98zxkh8w",
          "author": null,
          "description": "Jan  4, 15:59 UTC\nResolved - From approximately 15:00 - 15:35 UTC, users saw a 404 error when navigating to the Settings page in our Cloud Control Panel. \nOur Engineering team has confirmed full resolution of the issue. \nIf you continue to experience issues, please open a Support ticket from within your account.\nJan  4, 15:39 UTC\nMonitoring - Our Engineering team has identified the root cause of the issue with the Settings portion of our Cloud Control Panel throwing a 404 and has implemented a fix. At this time, users should be able to navigate to the Settings tab or directly to https://cloud.digitalocean.com/account/team normally. \nWe are monitoring the fix and will post a final update once we confirm full resolution.\nJan  4, 15:24 UTC\nInvestigating - Our Engineering team is investigating an issue with the Settings portion of our Cloud Control Panel. At this time, users navigating to the Settings tab or directly to https://cloud.digitalocean.com/account/team will see a 404 error. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/1pbm98zxkh8w",
          "publishedOn": "2024-01-04T15:59:14.000Z",
          "wordCount": 6361,
          "title": "Settings in Cloud Control Panel Unavailable",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/p6j21rldnxtl",
          "author": null,
          "description": "Dec 30, 12:23 UTC\nResolved - Our Engineering team has resolved the issue impacting Spaces Object Storage in our SGP1 region. From approximately 09:20 UTC - 11:31 UTC, users may have experienced an increased error rate and low performance while using the Spaces system. Spaces should now be operating normally. \nIf you continue to experience problems, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.\nDec 30, 11:53 UTC\nMonitoring - From 09:20 UTC, our Engineering team observed an issue with Spaces Object Storage availability in the SGP1 region. \nDuring this time you may have experienced an increased error rate and low performance while using the Spaces system. \nThe impact has now subsided and users should no longer be experiencing issues with accessing the Spaces Object Storage.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/p6j21rldnxtl",
          "publishedOn": "2023-12-30T12:23:06.000Z",
          "wordCount": 6327,
          "title": "Spaces Availability in SGP1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Notion Status - Incident History",
      "feedUrl": "https://status.notion.so/history.rss",
      "siteUrl": "https://status.notion.so",
      "articles": [
        {
          "id": "https://status.notion.so/incidents/wn8zmpphxwr9",
          "author": null,
          "description": "Jan 15, 11:19 PST\nResolved - We've pushed a fix for this issue now and users can move pages in bulk again without errors. We apologize for the earlier disruption and thank you for bearing with us through this. \nPlease open Notion and press Cmd/Ctrl + Shift + R to reload the latest changes before trying to move pages again.\nJan 15, 10:24 PST\nIdentified - Our team has identified the cause of problems when moving pages in bulk across Notion databases and is working on a fix. We will share further updates as soon as the problem is resolved.\nJan 15, 07:53 PST\nInvestigating - Users may be experiencing degraded behavior when moving pages in bulk across Notion databases. We are investigating this issue and will share an update as soon as possible.",
          "link": "https://status.notion.so/incidents/wn8zmpphxwr9",
          "publishedOn": "2024-01-15T19:19:20.000Z",
          "wordCount": 3452,
          "title": "Degraded behavior when moving pages in bulk",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/qbkb93dwrm9r",
          "author": null,
          "description": "Jan 11, 22:56 PST\nResolved - This incident has been resolved.\nJan 11, 20:52 PST\nIdentified - We are experiencing issues with MFA  login method. The team has identified the cause and is working on the fix.",
          "link": "https://status.notion.so/incidents/qbkb93dwrm9r",
          "publishedOn": "2024-01-12T06:56:42.000Z",
          "wordCount": 3344,
          "title": "Issue with MFA Login",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/4kbzwnzxc4df",
          "author": null,
          "description": "Jan  4, 18:06 PST\nResolved - This incident has been resolved. \nTime of incident was approximately from 5pm PDT - 6pm PDT.\nJan  4, 17:32 PST\nInvestigating - We are experiencing latency issues with our collections service, so users may experience issues loading databases. \nWe're actively looking into this, and we'll provide updates when we get more information.",
          "link": "https://status.notion.so/incidents/4kbzwnzxc4df",
          "publishedOn": "2024-01-05T02:06:40.000Z",
          "wordCount": 3362,
          "title": "Database Performance Issues",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/mdxnttqpv9z2",
          "author": null,
          "description": "Dec 27, 08:41 PST\nResolved - The issue was resolved.\nDec 27, 07:20 PST\nIdentified - We have identified the issue & are implementing a fix.\nDec 27, 05:30 PST\nInvestigating - Some users are receiving an error when trying to re-enter the Notion App on an iOS mobile device after having opened a different App in the meantime. We are actively looking into this.",
          "link": "https://status.notion.so/incidents/mdxnttqpv9z2",
          "publishedOn": "2023-12-27T16:41:14.000Z",
          "wordCount": 3377,
          "title": "Degenerated iOS mobile App Performance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Rippling Status - Incident History",
      "feedUrl": "https://status.rippling.com/history.rss",
      "siteUrl": "https://status.rippling.com",
      "articles": [
        {
          "id": "https://status.rippling.com/incidents/crg2qtnbmb1t",
          "author": null,
          "description": "Jan  8, 18:26 UTC\nResolved - The Rippling application was inaccessible from 9:39am to 10:06am PST; it has recovered and is now performing normally. The root cause is an incident with Cloudflare: https://www.cloudflarestatus.com/\nJan  8, 18:11 UTC\nMonitoring - The issue accessing Rippling has been mitigated, and we are monitoring the results.\nJan  8, 17:56 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/crg2qtnbmb1t",
          "publishedOn": "2024-01-08T18:26:01.000Z",
          "wordCount": 5296,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/5qwcw06j2w1b",
          "author": null,
          "description": "Jan  4, 22:53 UTC\nResolved - This incident has been resolved.\nJan  4, 22:45 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nJan  4, 19:22 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/5qwcw06j2w1b",
          "publishedOn": "2024-01-04T22:53:03.000Z",
          "wordCount": 5309,
          "title": "Erroneous emails being sent to IT admins about user account privileges on Windows devices",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        }
      ]
    },
    {
      "title": "Google Workspace Status Dashboard Updates",
      "feedUrl": "https://www.google.com/appsstatus/dashboard/en/feed.atom",
      "siteUrl": "https://www.google.com/appsstatus/dashboard/",
      "articles": []
    },
    {
      "title": "GitHub Status - Incident History",
      "feedUrl": "https://www.githubstatus.com/history.rss",
      "siteUrl": "https://www.githubstatus.com",
      "articles": [
        {
          "id": "https://www.githubstatus.com/incidents/bsk6hmj0d7nv",
          "author": null,
          "description": "Jan 23, 18:53 UTC\nResolved - On January 23, 2024 at 14:36 UTC, our internal metrics began showing an increase in exceptions originating from our live update service. Live updates to Issues, PRs, Actions, and Projects were failing, but refreshing the page successfully updated page content. We resolved the issue by rolling back a problematic dependency update and reenabled live updates at 18:53 UTC. \nWe are working to improve alerting and monitoring of our live update service to reduce our time to detection and mitigation.\nJan 23, 18:53 UTC\nUpdate - Live updates have been restored and the system is operating normally.\nJan 23, 18:14 UTC\nUpdate - We have identified and are beginning to roll out a potential fix for issues with live updates to our Web UI that power automatic page updates such as…",
          "link": "https://www.githubstatus.com/incidents/bsk6hmj0d7nv",
          "publishedOn": "2024-01-23T18:53:29.000Z",
          "wordCount": 5687,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/7ck5966p1073",
          "author": null,
          "description": "Jan 21, 09:34 UTC\nResolved - On 2024-01-21 at 3:38 UTC, we experienced an incident that affected customers using Codespaces. Customers encountered issues creating and resuming Codespaces in multiple regions due to operational issues with compute and storage resources.\nAround 25% of customers were impacted, primarily in East US and West Europe. We re-routed traffic for Codespace creations to less impacted regions, but existing Codespaces in these regions may have been unable to resume during the incident.\nBy 7:30 UTC, we had recovered connectivity to all regions except West Europe, which had an extended recovery time due to increased load in that particular region. The incident was resolved on 2024-01-21 at 9:34 UTC once Codespace creations and resumes were working normally in all regions.\n…",
          "link": "https://www.githubstatus.com/incidents/7ck5966p1073",
          "publishedOn": "2024-01-21T09:34:34.000Z",
          "wordCount": 5683,
          "title": "Incident with Codespaces",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/hmvr5kpgzc45",
          "author": null,
          "description": "Jan 21, 06:19 UTC\nResolved - On 2024-01-21 from 02:05 UTC to 06:19 UTC, GitHub Hosted Runners experienced increased error rates from our main cloud service provider. The errors were initially limited to a single region and we were able to route around the issue by transparently failing over to other regions. However, errors gradually expanded across all regions we deploy to and led to our available compute capacity being exhausted.\nDuring the incident, up to 35% of Actions jobs using Larger Runners and 2% of Actions jobs using GitHub Hosted Runners overall may have experienced intermittent delays in starting. Once the issue was resolved by our cloud service provider, our systems made a full recovery without intervention.\nWe’re working closely with our service provider to understand the cause of the outage and mitigations we can put in place. We’re also working to increase our resilience to outages of this nature by expanding the regions we deploy to beyond the existing set, especially for Larger Runners.\nJan 21, 05:54 UTC\nUpdate - We've applied a mitigation to fix the issues with queuing and running Actions jobs. We are seeing improvements in telemetry and are monitoring for full recovery.\nJan 21, 05:26 UTC\nUpdate - We have mitigated the issues impacting Actions Larger Runners. We are still experiencing delays starting normal jobs, and are continuing to investigate.\nJan 21, 04:53 UTC\nUpdate - The team has identified the cause of the issues with Actions Larger Runners and has begun mitigation.\nJan 21, 04:16 UTC\nUpdate - The team continues to investigate issues with some Actions jobs being queued for a long time and a percentage of jobs failing. We will continue providing updates on the progress towards mitigation.\nJan 21, 03:45 UTC\nInvestigating - We are investigating reports of degraded performance for Actions",
          "link": "https://www.githubstatus.com/incidents/hmvr5kpgzc45",
          "publishedOn": "2024-01-21T06:19:52.000Z",
          "wordCount": 5544,
          "title": "Incident with Actions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/pxg3dz4yg7lp",
          "author": null,
          "description": "Jan  9, 14:40 UTC\nResolved - On January 9 between 12:45 and 13:56 UTC, services in one of our three sites experienced elevated latency for connections.  This led to a sustained period of timed out requests across a number of services, including but not limited to our git backend.  An average of 5% and max of 10% of requests failed with a 5xx response or timed out during this period.  This was caused by a combination of events that led to connection limits being hit in load balancer proxies in that site.  An upgrade of hosts was in flight, which meant a subset of proxy hosts were draining and coming offline as the upgrade rolled through the fleet.  A config change event also triggered a connection reset across all services in that site.  These events are commonplace, but led to a spike in c…",
          "link": "https://www.githubstatus.com/incidents/pxg3dz4yg7lp",
          "publishedOn": "2024-01-09T14:40:45.000Z",
          "wordCount": 5933,
          "title": "Incident with Issues, API Requests, Pull Requests, Actions, Pages, Git Operations, Webhooks, Packages and Codespaces",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/dxthh9653wkk",
          "author": null,
          "description": "Jan  9, 05:44 UTC\nResolved - On January 9 between 1:06 and 5:43 UTC, no audit log events were streamed for customers that have that enabled. All events that happened during this time were delivered after the issue was mitigated. The event delivery was failing due to a data shape issue, with a streaming configuration linked to a soft-deleted Enterprise causing a runtime error for a backend service.  This was mitigated by removing that configuration.  Since the incident, we have improved our detection of these errors and ensured support for similar scenarios.\nJan  9, 05:40 UTC\nUpdate - Audit logs streaming is currently unavailable due to a misconfiguration issue. The root cause is understood and our engineers will be updating the broken configuration shortly.\nJan  9, 04:59 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/dxthh9653wkk",
          "publishedOn": "2024-01-09T05:44:37.000Z",
          "wordCount": 5382,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/35pgg0gc75nv",
          "author": null,
          "description": "Jan  8, 23:41 UTC\nResolved - From January 5th to January 8th, Codespaces experienced issues with port forwarding when connecting from a web browser. During this incident 100% of operations to forward ports and connect to a forwarded port failed for customers located in US West and Australia. The cause was due to a cross origin API change in our port forwarding service. After detecting the issue, we mitigated the impact by rolling back the change that caused the discrepancy in the cross origin rules. We have improved the way we are detecting issues like this one including implementing automated alerts during and after the rollout process so that we get signals as early as possible.\nJan  8, 22:37 UTC\nUpdate - The initial mitigation we attempted did not fully resolve this issue.  We are continuing to investigate and will provide an update as soon as possible.\nJan  8, 21:55 UTC\nUpdate - We are engaged on the issue and continuing to work toward a mitigation. Please continue to fallback to VS Code desktop for port forwarding workflows.\nJan  8, 21:03 UTC\nUpdate - Mitigation of degraded Codespaces port forwarding in is progress.  In the meantime, please continue to use VS Code Desktop for port forwarding.\nJan  8, 20:22 UTC\nUpdate - We are actively mitigating degraded performance of Codespaces port forwarding in the web and GitHub CLI.  Codespaces port forwarding on VS Code Desktop is unaffected.\nJan  8, 20:22 UTC\nInvestigating - We are investigating reports of degraded performance for Codespaces",
          "link": "https://www.githubstatus.com/incidents/35pgg0gc75nv",
          "publishedOn": "2024-01-08T23:41:03.000Z",
          "wordCount": 5496,
          "title": "Incident with Codespaces",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/zzy7pdshgqk0",
          "author": null,
          "description": "Jan  3, 17:05 UTC\nResolved - From 10:30 to 16:15 UTC on January 3rd, our `/settings/emails` page experienced downtime due to an outage in our contact permissions system. During this time customers were unable to update their email subscription preferences.\nThis system plays a crucial role in managing email subscriptions and the contactability of GitHub customers by our marketing and sales teams. Unfortunately, the outage prevented the generation of links for managing email subscriptions, resulting in the `/settings/emails` page timing out. Consequently, customers were unable to update their email subscription preferences during this period.\nDuring the incident we implemented graceful degradation on the `/settings/email` page to enable certain functionality, albeit with slower response time…",
          "link": "https://www.githubstatus.com/incidents/zzy7pdshgqk0",
          "publishedOn": "2024-01-03T17:05:05.000Z",
          "wordCount": 5624,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/1ksntt4bk3bq",
          "author": null,
          "description": "Dec 29, 21:21 UTC\nResolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/\nDec 29, 21:09 UTC\nUpdate - We are in the process of reverting a change that introduced these failures.\nDec 29, 20:05 UTC\nUpdate - We’re investigating reports of increased failure rates for migrations with GitHub Enterprise Importer and exports using the Organization Migrations REST API.\nDec 29, 20:05 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/1ksntt4bk3bq",
          "publishedOn": "2023-12-29T21:21:38.000Z",
          "wordCount": 5386,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/464b7g25n77j",
          "author": null,
          "description": "Dec 29, 18:33 UTC\nResolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/\nDec 29, 18:31 UTC\nUpdate - With a mitigation deploying, we see recovery in most API requests and are continuing to monitor full rollout and mitigation.\nDec 29, 18:21 UTC\nUpdate - Secret Scanning and potentially other APIs are returning 500 error responses.  We're working on a mitigation.\nDec 29, 18:17 UTC\nInvestigating - We are investigating reports of degraded performance for API Requests",
          "link": "https://www.githubstatus.com/incidents/464b7g25n77j",
          "publishedOn": "2023-12-29T18:33:52.000Z",
          "wordCount": 5388,
          "title": "Incident with API Requests",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/1h37j1mb9xnx",
          "author": null,
          "description": "Dec 29, 02:04 UTC\nResolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/\nDec 29, 01:41 UTC\nUpdate - Users without an existing valid session are unable to login and will see an error page.  We are working on a mitigation.\nDec 29, 01:41 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/1h37j1mb9xnx",
          "publishedOn": "2023-12-29T02:04:44.000Z",
          "wordCount": 5366,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/lt4kyp4tgx45",
          "author": null,
          "description": "Dec 28, 06:57 UTC\nResolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/\nDec 28, 06:49 UTC\nUpdate - We have deployed a fix and email service should be restored shortly.\nDec 28, 06:48 UTC\nUpdate - We are experiencing issues sending some pull request, actions and other notification emails. Some emails may not be received as the result of activity on GitHub. Web and mobile push notifications are not affected.\nDec 28, 06:43 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/lt4kyp4tgx45",
          "publishedOn": "2023-12-28T06:57:42.000Z",
          "wordCount": 5398,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/hzd6d6xhjpl6",
          "author": null,
          "description": "Dec 27, 19:25 UTC\nResolved - On December 26, 2023, GitHub received a report through our Bug Bounty Program demonstrating a vulnerability which, if exploited, allowed access to credentials within a production container. We fixed this vulnerability on GitHub.com the same day and began rotating all potentially exposed credentials. Through this process we found some flaws in how we rotate certain credentials and are working on improving our credential rotation process. More detail can be found on our blog: https://github.blog/2024-01-16-rotating-credentials-for-github-com-and-new-ghes-patches/\nDec 27, 19:25 UTC\nUpdate - A recent update to an Action that GitHub Pages deployer service relies on impacted that service, and was corrected and redeployed.\nDec 27, 19:12 UTC\nUpdate - We've identified the cause of some Pages errors and are deploying a mitigating fix now.\nDec 27, 19:01 UTC\nUpdate - We continue to investigate issues with Pages, and will continue to keep users updated on progress towards mitigation.\nDec 27, 18:29 UTC\nUpdate - Pages workflow builds which use the actions actions/upload-pages-artifact@v3 and actions/deploy-pages@v4 are currently failing.\nDec 27, 18:29 UTC\nInvestigating - We are investigating reports of degraded performance for Pages",
          "link": "https://www.githubstatus.com/incidents/hzd6d6xhjpl6",
          "publishedOn": "2023-12-27T19:25:23.000Z",
          "wordCount": 5430,
          "title": "Incident with Pages",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        }
      ]
    },
    {
      "title": "Slack System Status",
      "feedUrl": "https://status.slack.com/feed/rss",
      "siteUrl": "https://status.slack.com/",
      "articles": [
        {
          "id": "https://status.slack.com//2024-01/0a000b0ad09623a0",
          "author": null,
          "description": "The issue that was causing some users to experience latency with connecting to Slack and loading channels has been resolved.\n\r\nWe apologize for any disruptions to your day.",
          "link": "https://status.slack.com//2024-01/0a000b0ad09623a0",
          "publishedOn": "2024-01-25T01:27:35.000Z",
          "wordCount": 143,
          "title": "Incident: Some latency with Slack",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2024-01/8b623c0b5640c28f",
          "author": null,
          "description": "We’ve implemented a fix and all impacted features are up and running once again. We appreciate your patience while we sorted this out. If you’re still encountering any trouble, please reload Slack using Command + Shift + R (Mac) or Ctrl + Shift + R (Windows/Linux). Thank you for your patience while we sorted out this issue.",
          "link": "https://status.slack.com//2024-01/8b623c0b5640c28f",
          "publishedOn": "2024-01-24T17:43:25.000Z",
          "wordCount": 172,
          "title": "Incident: Service wide connection issues",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2024-01/f39851209d6c471a",
          "author": null,
          "description": "The issue that some users experienced when uploading, downloading, and viewing files in Slack should no longer be affecting users. We will share a summary of the issue that will include additional details and findings once available. We apologize for any disruptions to your day.",
          "link": "https://status.slack.com//2024-01/f39851209d6c471a",
          "publishedOn": "2024-01-24T01:28:00.000Z",
          "wordCount": 189,
          "title": "Incident: Some users are unable to upload, download, and view files in Slack.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2024-01/166eb312bd134031",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 12:20pm PST until 3:00pm PST on January 22, 2024, some Enterprise Grid users noticed multiple Slackbot responses being triggered unexpectedly.\n\r\n\r\nWe determined that the rollout of a fix for an older bug report pertaining to Slackbot responses not working in org-wide or multi-workspace channels, was the root cause. \n\r\n\r\nWhilst the fix for the bug was intended to improve this features behaviour, ensuring that Slackbot custom responses would work in these channel types, our wider team concluded that the fixed behaviour might not function well for organizations with potentially thousands of custom Slackbot responses.\n\r\n\r\nWe rolled back the deployment which caused this behaviour. Customers will no longer see these Slackbot custom responses being triggered unexpectedly.\n\r\n\r\nA discussion is underway about the long-term future of Slackbot custom response behaviour within large organizations.\n\r\n\r\nThank you for your patience whilst we resolved this.",
          "link": "https://status.slack.com//2024-01/166eb312bd134031",
          "publishedOn": "2024-01-23T03:06:24.000Z",
          "wordCount": 327,
          "title": "Incident: Some Enterprise Grid users may be seeing unexpected Slackbot responses",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2024-01/9301a4fb7cc577ea",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 11:26 AM PST on January 12, 2024 to 10:04 AM PST on January 15, 2024, some users were unable to configure 2FA on their accounts. We were made aware of this after a spike in reports early in the morning of Monday, January 15.\n\r\n\r\nUpon investigation, this issue was traced back to a recent code change which we discovered was preventing users from being redirected back to the 2FA configuration page after entering their password during the 2FA setup process. We immediately reverted this change which fully resolved the issue.",
          "link": "https://status.slack.com//2024-01/9301a4fb7cc577ea",
          "publishedOn": "2024-01-15T23:51:07.000Z",
          "wordCount": 242,
          "title": "Incident: Some users may not be able to configure 2FA",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        }
      ]
    },
    {
      "title": "Make Status - Incident History",
      "feedUrl": "https://status.make.com/history.rss",
      "siteUrl": "https://status.make.com",
      "articles": [
        {
          "id": "https://status.make.com/incidents/h3gvmlbldj1q",
          "author": null,
          "description": "Jan 15, 16:18 CET\nResolved - We have reactivated the affected scenarios. Please note that this reactivation will not be visible in the scenario logs. Currently, the Monday app is fully operational.\nJan 15, 11:42 CET\nUpdate - A fix has been rolled and we are investigating options to re-enable affected scenarios automatically.\nJan 15, 10:27 CET\nMonitoring - A fix has been implemented and we are monitoring the results.\nJan 15, 09:47 CET\nIdentified - The issue has been identified and a fix is being implemented.",
          "link": "https://status.make.com/incidents/h3gvmlbldj1q",
          "publishedOn": "2024-01-15T15:18:40.000Z",
          "wordCount": 3892,
          "title": "Monday app not working",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}