{
  "sources": [
    {
      "title": "BookingSync.com news",
      "feedUrl": "https://changelog.bookingsync.com/rss",
      "siteUrl": "https://changelog.bookingsync.com",
      "articles": []
    },
    {
      "title": "Security Bulletins on Tailscale",
      "feedUrl": "https://tailscale.com/security-bulletins/index.xml",
      "siteUrl": "https://tailscale.com/security-bulletins/",
      "articles": [
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "author": null,
          "description": "Description: Privilege escalation bugs in the Tailscale\nKubernetes operator’s API proxy allowed authenticated tailnet clients\nto send Kubernetes API requests as the operator’s service account.\nTailscale Kubernetes operator version v1.53.37 fixes the issue and\nusers of the operator who enable the API proxy functionality should\nupdate as described below.\nWhat happened?\nThe Tailscale Kubernetes operator can optionally act as an API server\nproxy\nfor the cluster’s Kubernetes API. This proxy allows authenticated\ntailnet users to use their tailnet identity in Kubernetes\nauthentication and RBAC rules. The API server proxy uses\nimpersonation\nheaders\nto translate tailnet identities to Kubernetes identities.\nThe operator prior to v1.53.37 has two bugs in the forwarding logic,\nwhich affects different …",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "publishedOn": "2023-11-01T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-008",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        },
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "author": null,
          "description": "Description: Microsoft Defender is flagging Tailscale 1.46.1 as malware.\nThese classifications are false positives, and we are working with Microsoft to\nresolve the situation.\nAs of 2023-10-27 1:05 AM UTC, we have confirmed that Microsoft have addressed\nthe false positive, meaning Defender no longer flags Tailscale 1.46.1 as\nmalware. A rescan of tailscaled.exe 1.46.1 on VirusTotal confirms this.\nWhat happened?\nMicrosoft Defender was flagging Tailscale 1.46.1 as malware. This caused\nDefender to quarantine the binaries, meaning they could not run.\nWe submitted Tailscale 1.46.1 to Microsoft to investigate the false positive,\nwho then updated Defender to avoid flagging this release as malware at\n2023-10-27 1:05 AM UTC.\nWho is affected?\nPeople using Defender and Tailscale 1.46.1.\nWhat is the impact?\nTailscale will not run on affected machines.\nWhat do I need to do?\nTo resolve this issue on your own tailnet, you can take either or both of 2\napproaches:\nUpdate to a newer version of Tailscale. Newer versions are not affected by this problem.\nCreate an exception in Microsoft Defender. Microsoft has published instructions explaining how to do this.\nUpdate Microsoft Defender.",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "publishedOn": "2023-10-26T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-007",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        }
      ]
    },
    {
      "title": "Airbnb API Status - Incident History",
      "feedUrl": "https://airbnbapi.statuspage.io/history.rss",
      "siteUrl": "https://airbnbapi.statuspage.io",
      "articles": [
        {
          "id": "https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt",
          "author": null,
          "description": "Nov  9, 14:49 PST\nResolved - This incident has been resolved.\nNov  8, 10:38 PST\nIdentified - We have observed a higher number of intermittent 500 errors on the GET Resolutions API. We are actively working on resolving this issue and plan to deploy a fix tomorrow (Nov 9, 2023). Thank you for your understanding and patience as we work to rectify this situation.",
          "link": "https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt",
          "publishedOn": "2023-11-09T22:49:38.000Z",
          "wordCount": 3883,
          "title": "Intermittent 500 errors on the GET Resolutions API",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "author": null,
          "description": "Oct 31, 10:00 PDT\nResolved - This incident has been resolved.\nOct 30, 18:14 PDT\nMonitoring - Today (Oct 30, 2023) between 1:00 PM and 3:00 PM PDT we encountered an incident that impacted our webhooks functionality. As a result, some reservations may have been missing webhooks during this time period. The issue has been resolved, and our team is actively monitoring the results to ensure the stability of our systems. If you have noticed any reservations missing webhooks, we recommend using the GET Reservations API to retrieve the details of those reservations. Alternatively, you can contact us with a list of affected reservations, and we will assist you in backfilling the missing information.\nWe apologize for any inconvenience this may have caused and appreciate your understanding.",
          "link": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "publishedOn": "2023-10-31T17:00:19.000Z",
          "wordCount": 3930,
          "title": "Issue Affecting Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Twilio Status - Incident History",
      "feedUrl": "https://status.twilio.com/history.rss",
      "siteUrl": "https://status.twilio.com",
      "articles": [
        {
          "id": "https://status.twilio.com/incidents/8w5rf6y4n0w4",
          "author": null,
          "description": "THIS IS A SCHEDULED EVENT Nov 20, 20:00 - 23:00 PST\nNov 17, 02:52 PST\nScheduled - Our SMS carrier partner in Israel is conducting a planned maintenance from 20 November 2023 at 20:00 PST until 20 November 2023 at 23:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from a subset of Israel handsets.",
          "link": "https://status.twilio.com/incidents/8w5rf6y4n0w4",
          "publishedOn": "2023-11-21T04:00:00.000Z",
          "wordCount": 7245,
          "title": "Israel SMS Carrier Partner Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/ps2ststnr7sm",
          "author": null,
          "description": "THIS IS A SCHEDULED EVENT Nov 20, 20:00 - 22:00 PST\nNov 16, 23:10 PST\nScheduled - Our SMS carrier partner in Australia is conducting an emergency maintenance from 20 November 2023 at 02:00 PST until 20 November 2023 at 04:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to a subset of Twilio Australia phone numbers.",
          "link": "https://status.twilio.com/incidents/ps2ststnr7sm",
          "publishedOn": "2023-11-21T04:00:00.000Z",
          "wordCount": 7242,
          "title": "Australia SMS Carrier Partner Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/cbvr70lpwhyq",
          "author": null,
          "description": "Nov 20, 16:11 PST\nMonitoring - We are seeing an improvement in increased latency and errors with Dialogflow agents, and will continue to monitor for system stability. We expect to provide another update in 2 hours or as soon as more information becomes available\nNov 20, 14:50 PST\nUpdate - We are investigating intermittent increased latency and errors which may result in dropped virtual agent sessions or slow responses with Dialogflow agents. We expect to provide another update in 2 hours or as soon as more information becomes available\nNov 20, 13:51 PST\nUpdate - We are investigating intermittent increased latency and errors which may result in dropped virtual agent sessions or slow responses with Dialogflow agents. We expect to provide another update in 1 hour or as soon as more information becomes available\nNov 20, 13:51 PST\nInvestigating - We are investigating intermittent increased latency and errors which may result in dropped virtual agent sessions or slow responses with Dialogflow agents. We expect to provide another update in 1 hour or as soon as more information becomes available",
          "link": "https://status.twilio.com/incidents/cbvr70lpwhyq",
          "publishedOn": "2023-11-21T00:11:36.000Z",
          "wordCount": 7380,
          "title": "Dropped Sessions or Slow Responses with Dialogflow Agents",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/71fwtb426s8g",
          "author": null,
          "description": "Nov 20, 16:01 PST\nCompleted - The scheduled maintenance has been completed.\nNov 20, 12:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 16, 07:27 PST\nScheduled - The Megafon and Yota networks in Russia are conducting a planned maintenance from 20 November 2023 at 12:00 PST until 20 November 2023 at 16:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to Megafon and Yota Russia handsets.",
          "link": "https://status.twilio.com/incidents/71fwtb426s8g",
          "publishedOn": "2023-11-21T00:01:11.000Z",
          "wordCount": 7278,
          "title": "Russia SMS Carrier Maintenance - Megafon and Yota",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/lrh5kjzx80x0",
          "author": null,
          "description": "Nov 20, 15:48 PST\nResolved - We have fully investigated the potential issue with NetNumber for A2P triggered by our automated alert, and it was determined that there is no noticeable customer impact. All systems are operational.\nNov 20, 15:39 PST\nInvestigating - We've become aware of a potential issue with NetNumber for A2P. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/lrh5kjzx80x0",
          "publishedOn": "2023-11-20T23:48:42.000Z",
          "wordCount": 7259,
          "title": "On Call Engineers are Investigating",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/nd9ksfqfyp8l",
          "author": null,
          "description": "Nov 20, 14:30 PST\nResolved - Sender ID was degraded for 3 hours and 8 minutes between 16:19 and 19:27  Pacific Time on 11/13/23. During this period of time customers may have experienced high latency in errored responses for Sender ID. The issue has now been resolved.",
          "link": "https://status.twilio.com/incidents/nd9ksfqfyp8l",
          "publishedOn": "2023-11-20T22:30:32.000Z",
          "wordCount": 7232,
          "title": "Sender ID Service Returning High Latency Errored Responses",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/scthjqt9qrcm",
          "author": null,
          "description": "Nov 20, 14:01 PST\nCompleted - The scheduled maintenance has been completed.\nNov 20, 10:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 20, 05:16 PST\nScheduled - The Tele2 and Altel network in Kazakhstan are conducting an emergency maintenance from 20 November 2023 at 10:00 PST until 20 November 2023 at 14:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to Tele2 and Altel Kazakhstan handsets.",
          "link": "https://status.twilio.com/incidents/scthjqt9qrcm",
          "publishedOn": "2023-11-20T22:01:27.000Z",
          "wordCount": 7278,
          "title": "Kazakhstan SMS Carrier Maintenance - Tele2 and Altel",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/8pgz0bjg4vjm",
          "author": null,
          "description": "Nov 20, 14:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 13, 12:56 PST\nScheduled - The K-Telecom networks in Russia is conducting a planned maintenance from 20 November 2023 at 14:00 PST until 20 November 2023 at 17:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to K-Telecom Russia handsets.",
          "link": "https://status.twilio.com/incidents/8pgz0bjg4vjm",
          "publishedOn": "2023-11-20T22:00:41.000Z",
          "wordCount": 7264,
          "title": "Russia SMS Carrier Maintenance - K-Telecom (WIN Mobile)",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/s153y240450s",
          "author": null,
          "description": "Nov 20, 14:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 13, 12:57 PST\nScheduled - The KTK Telecom networks in Russia is conducting a planned maintenance from 20 November 2023 at 14:00 PST until 20 November 2023 at 17:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to KTK Telecom Russia handsets.",
          "link": "https://status.twilio.com/incidents/s153y240450s",
          "publishedOn": "2023-11-20T22:00:20.000Z",
          "wordCount": 7269,
          "title": "Russia SMS Carrier Maintenance - KTK Telecom (Volna Mobile)",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/kxjn9hxvn8rm",
          "author": null,
          "description": "Nov 20, 14:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 13, 07:38 PST\nScheduled - Our SMS carrier partner in Russia is conducting a planned maintenance from 20 November 2023 at 14:00 PST until 20 November 2023 at 17:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to Russia handsets.",
          "link": "https://status.twilio.com/incidents/kxjn9hxvn8rm",
          "publishedOn": "2023-11-20T22:00:19.000Z",
          "wordCount": 7258,
          "title": "SMS Carrier Partner Maintenance - Russia",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/2d7gzt24gks6",
          "author": null,
          "description": "Nov 20, 13:00 PST\nCompleted - The scheduled maintenance has been completed.\nNov 20, 12:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nNov 17, 03:20 PST\nScheduled - The Beeline network in Russia is conducting a planned maintenance from 20 November 2023 at 12:00 PST until 20 November 2023 at 13:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to Beeline Russia handsets.",
          "link": "https://status.twilio.com/incidents/2d7gzt24gks6",
          "publishedOn": "2023-11-20T21:00:08.000Z",
          "wordCount": 7268,
          "title": "Russia SMS Carrier Maintenance - Beeline",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "DigitalOcean Status - Incident History",
      "feedUrl": "https://status.digitalocean.com/history.rss",
      "siteUrl": "http://status.digitalocean.com",
      "articles": [
        {
          "id": "https://status.digitalocean.com/incidents/wpz7gdly2p32",
          "author": null,
          "description": "Nov 15, 01:59 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting DOKS across all regions. \nAfter the rollout, we are no longer reliant on that affected upstream provider for our DOKS product.\nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nNov 15, 00:42 UTC\nMonitoring - Our Engineering team has completed the rollout to pivot away from the affected upstream provider and we are no longer reliant on that provider for our DOKS product. \nAt this time, Users should no longer see any issues with nodes going into not ready states, creating new clusters, or scaling up additional nodes. \nWe're monitoring the fix and will post another update once we confirm this issue is fully resolved.\nNov 14, 23:24 UTC\nUpdate - Our Engineering team is currently working to pivot away from the affected upstream provider to mitigate impact from this incident. That fix is rolling out across our fleet and users should start to see conditions on affected clusters improve. \nAs soon as the fix is rolled out completely, we'll post another update.\nNov 14, 22:20 UTC\nUpdate - Our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS across all regions which was initially reported for a few regions. \nDuring this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state across all regions.\nWe will share an update as soon as we have any information from our upstream provider.\nNov 14, 21:59 UTC\nIdentified - Beginning around 20:00 UTC, our Engineering team has confirmed an issue on our upstream provider's end impacting DOKS in our multiple regions. \nDuring this time, Users will not be able to create new clusters, scale up additional nodes or may see nodes in an unready state. \nWe will share an update as soon as we have any information from our upstream provider.",
          "link": "https://status.digitalocean.com/incidents/wpz7gdly2p32",
          "publishedOn": "2023-11-15T01:59:25.000Z",
          "wordCount": 6266,
          "title": "DOKS in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/hxcw6mrw1ywv",
          "author": null,
          "description": "Nov  8, 18:07 UTC\nResolved - Our Engineering team has seen no recurrences and performance has remained stable since 16:40 UTC. This incident is fully resolved.\nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience!\nNov  8, 17:23 UTC\nUpdate - Our Engineering team identified a configuration that was responsible for the recurrence we saw. From 16:20 - 16:40 UTC, connectivity between SFO2 and the rest of DigitalOcean's network was impacted.\nAs of 16:40 UTC, all impact has subsided and users should no longer face any issues.\nWe are monitoring the situation closely and will share an update once the issue is completely resolved.\nNov  8, 16:32 UTC\nUpdate - Our Engineering team is seeing a recurrence of network alerts that indicate we're exp…",
          "link": "https://status.digitalocean.com/incidents/hxcw6mrw1ywv",
          "publishedOn": "2023-11-08T18:07:20.000Z",
          "wordCount": 6263,
          "title": "Networking Connectivity Between NYC and SFO",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "author": null,
          "description": "Nov  4, 08:18 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting App Platform Deployments. \nFrom 11:54 on Nov 2nd to 06:42 on Nov 4th UTC, App Platform users may have experienced delays when deploying new apps or when deploying updates to existing Apps. Our Upstream provider and the Engineering team closely worked together to resolve the issue. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel. Thank you for your patience and we apologize for any inconvenience.\nNov  4, 06:53 UTC\nUpdate - As per the recent update from our Upstream provider, they fully recovered the services used…",
          "link": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "publishedOn": "2023-11-04T08:18:05.000Z",
          "wordCount": 6442,
          "title": "App Platform Deployments",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "author": null,
          "description": "Nov  3, 23:14 UTC\nResolved - Our Engineering team has confirmed the full resolution of issue impacting network connectivity in our SFO regions. \nUsers should no longer experience any latency or timeout issue with any of the Droplet based services. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nNov  3, 20:58 UTC\nMonitoring - As of 19:55 UTC, our Engineering team has confirmed that a fix has been implemented by our upstream carrier to mitigate the cause of the issue impacting network connectivity in our SFO region. \nWe are closely monitoring the situation and will update as soon as we have more information from the provider.\nNov  3, 19:43 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting network connectivity in our SFO region. Upstream congestion with a network provider between Los Angeles and Dallas is impacting traffic traversing out of our SFO datacentres.\nA case has been opened by our team with the provider. Our team has attempted to shift traffic to improve the situation, but unfortunately, we continue to see approximately 10% of customer traffic impacted by this issue.\nOur team is working on an option to shift to an alternate provider if this issue is not able to be resolved by the provider in a timely manner. We will share another update once we have further information from the provider or we have an update from our Engineering team.\nNov  3, 19:27 UTC\nInvestigating - As of 17:40 UTC, our Engineering team is investigating an issue impacting networking in the SFO regions. During this time, a subset of users may experience packet loss/latency and timeouts with Droplet based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "publishedOn": "2023-11-03T23:14:12.000Z",
          "wordCount": 6235,
          "title": "Networking in SFO Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "author": null,
          "description": "Nov  2, 11:50 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting the Cloud Control Panel, API, and multiple services.\nFrom 05:05 - 08:40 UTC, users may have encountered errors with the Cloud Control Panel and public API while attempting to create new user registrations, or while making payments. Users also may have experienced issues with processing Droplet and Managed Kubernetes cluster creations along with Droplet-based events and experienced latencies while accessing our Cloud Control Panel along with the DigitalOcean Container Registry. Our Upstream provider and the Engineering team closely worked together to resolve the issues. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf you…",
          "link": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "publishedOn": "2023-11-02T11:50:30.000Z",
          "wordCount": 6571,
          "title": "Multiple services down and API availability",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "author": null,
          "description": "Oct 27, 14:47 UTC\nResolved - As of 14:30 UTC, our Engineering team has resolved the issue impacting Spaces in our SGP1 and SFO3 regions. Users should no longer experience slowness or timeouts when trying to create or access their Spaces resources in the SGP1 and SFO3 regions. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nOct 27, 14:03 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue impacting Spaces in our SGP1 and SFO3 regions and is monitoring the situation closely. We will post an update as soon as the issue is fully resolved.\nOct 27, 12:04 UTC\nUpdate - Our Engineering team is continuing to investigate an issue impacting Object Storage in our SGP1 region. Additionally, we have become aware that this issue has also impacted Object Storage in the SFO3 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 27, 11:25 UTC\nInvestigating - As of 10:22 UTC, our Engineering team is investigating an issue with Object Storage in our SGP1 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "publishedOn": "2023-10-27T14:47:58.000Z",
          "wordCount": 6209,
          "title": "Object Storage - SGP1 and SFO3",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "author": null,
          "description": "Oct 26, 15:52 UTC\nResolved - As of 15:45 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.\nIf you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. \nThank you for your patience and we apologize for any inconvenience.\nOct 26, 15:18 UTC\nMonitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.\nOct 26, 14:25 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting the networking in the LON1 region and is actively working on a fix. During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. \nWe will post an update as soon as additional information is available.\nOct 26, 12:50 UTC\nInvestigating - As of 11:40 UTC, our Engineering team is investigating an issue impacting the networking in the LON1 region. During this time, a subset of users may experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will share an update once we have further information.",
          "link": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "publishedOn": "2023-10-26T15:52:41.000Z",
          "wordCount": 6179,
          "title": "Network Connectivity in LON1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "author": null,
          "description": "Oct 24, 23:00 UTC\nCompleted - The scheduled maintenance has been completed.\nOct 24, 21:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nOct 20, 19:40 UTC\nUpdate - After a thorough review by the team performing this maintenance, we have determined that our initial messaging does not convey the complete scope and potential impact of this event. Existing infrastructure will continue running without issue during this maintenance window. However, users may experience increased latency with some platform operations, including: \nCloud Control Panel and API operations\nEvent processing\nDroplet creates, resizes, rebuilds, and power events\nManaged Kubernetes reconciliation and scaling\nLoad Balancer operations\nContainer Registry operations\nApp …",
          "link": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "publishedOn": "2023-10-24T23:00:07.000Z",
          "wordCount": 6222,
          "title": "Core Infrastructure Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "author": null,
          "description": "Oct 24, 09:15 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.\nThank you for your patience and we apologize for any inconvenience.\nOct 24, 09:03 UTC\nMonitoring - Our Engineering team has received communication from the upstream provider that a fix to resolve the networking issue has been implemented. We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.\nOct 24, 07:41 UTC\nIdentified - Our Engineering team has identified the cause of issues impacting networking in multiple regions. The issues are a direct result of traffic congestion from our upstream providers, which is in the process of being repaired.\nAt this time, a subset of users will continue to experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nWe apologize for the inconvenience and will share an update once we have more information.\nOct 24, 06:11 UTC\nInvestigating - As of 05:30 UTC, our Engineering team is investigating an issue impacting the networking in multiple regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nAt the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "publishedOn": "2023-10-24T09:15:10.000Z",
          "wordCount": 6208,
          "title": "Networking in multiple regions.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Notion Status - Incident History",
      "feedUrl": "https://status.notion.so/history.rss",
      "siteUrl": "https://status.notion.so",
      "articles": [
        {
          "id": "https://status.notion.so/incidents/tk0hmlbd3lg9",
          "author": null,
          "description": "Nov 17, 16:04 PST\nResolved - Our team has now resolved the issue preventing template duplication, and this is working as normal again. We appreciate your patience while we worked through this issue.\nNov 17, 14:05 PST\nIdentified - We are experiencing an issue with duplicating published templates and our team is actively working on a fix.",
          "link": "https://status.notion.so/incidents/tk0hmlbd3lg9",
          "publishedOn": "2023-11-18T00:04:17.000Z",
          "wordCount": 3364,
          "title": "Issue affecting template duplication",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/vvx0wz4pgd9k",
          "author": null,
          "description": "Nov 16, 19:11 PST\nResolved - The incident has been resolved. Time of resolution Nov 16 2023 6:46PM PST\nNov 16, 19:09 PST\nIdentified - Notion AI is down. We are working with them on a fix. Time - Nov 16 2023 6:11PM PST",
          "link": "https://status.notion.so/incidents/vvx0wz4pgd9k",
          "publishedOn": "2023-11-17T03:11:59.000Z",
          "wordCount": 3351,
          "title": "Notion AI is down",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/n1sp5244k09v",
          "author": null,
          "description": "Nov  8, 07:28 PST\nResolved - The incident has been resolved. Time of resolution Nov 8 2023 7:28AM PST\nNov  8, 07:28 PST\nMonitoring - Our AI provider has implemented a fix and we are seeing Notion AI recover gradually since 7:28AM PST. We are currently monitoring the situation.\nNov  8, 06:57 PST\nUpdate - The issue has been identified now and a fix is being worked on for this issue.\nNov  8, 06:29 PST\nIdentified - One of our AI providers is down. We are working with them on a fix.",
          "link": "https://status.notion.so/incidents/n1sp5244k09v",
          "publishedOn": "2023-11-08T15:28:28.000Z",
          "wordCount": 3403,
          "title": "Notion AI is down",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Rippling Status - Incident History",
      "feedUrl": "https://status.rippling.com/history.rss",
      "siteUrl": "https://status.rippling.com",
      "articles": [
        {
          "id": "https://status.rippling.com/incidents/54s1f3rs3n56",
          "author": null,
          "description": "Nov 10, 23:59 UTC\nResolved - This incident has been resolved.\nNov  9, 19:56 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nNov  9, 18:41 UTC\nUpdate - We are continuing to investigate this issue.\nNov  9, 18:39 UTC\nUpdate - We are continuing to investigate this issue.\nNov  9, 18:34 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/54s1f3rs3n56",
          "publishedOn": "2023-11-10T23:59:07.000Z",
          "wordCount": 5044,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/w8r2ldkc0rcj",
          "author": null,
          "description": "Nov  7, 07:51 UTC\nResolved - This incident has been fully resolved.\nNov  6, 19:08 UTC\nMonitoring - The Rippling app has mostly recovered but there are still a few lags in performance that we're further monitoring and investigating.\nNov  6, 18:33 UTC\nInvestigating - We are continuing to see degraded performance in the Rippling app, so we are continuing to investigate.\nNov  6, 17:28 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nNov  6, 17:25 UTC\nInvestigating - There are issues loading Rippling. We are working on a fix.",
          "link": "https://status.rippling.com/incidents/w8r2ldkc0rcj",
          "publishedOn": "2023-11-07T07:51:01.000Z",
          "wordCount": 5074,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "author": null,
          "description": "Oct 27, 15:51 UTC\nResolved - The issue has now been resolved. All admins are able to access their Admin Account view as expected.\nOct 27, 14:50 UTC\nIdentified - We are aware of an issue where 'Admin Account' is not visible in the account dropdown for Super Admins. After they navigate to their 'Employee Account' view they can't navigate back to their admin view. \nThe root cause has been identified and the issue should be resolved in the next hour.",
          "link": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "publishedOn": "2023-10-27T15:51:44.000Z",
          "wordCount": 5077,
          "title": "Admin account not visible in the account dropdown for Super Admins",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "author": null,
          "description": "Oct 25, 21:10 UTC\nResolved - This incident has been resolved.\nOct 25, 21:02 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:52 UTC\nIdentified - An issue has been identified and we are working on a fix.\nOct 25, 20:46 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 25, 20:22 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:17 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 25, 20:14 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "publishedOn": "2023-10-25T21:10:01.000Z",
          "wordCount": 5089,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "author": null,
          "description": "Oct 24, 22:51 UTC\nResolved - This incident has been resolved.\nOct 24, 22:08 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 24, 22:07 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 24, 21:58 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 24, 21:57 UTC\nInvestigating - Customers are experiencing intermittent issues using single sign-on from Rippling (IdP-initiated SAML) to third-party applications. We are investigating this issue.",
          "link": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "publishedOn": "2023-10-24T22:51:30.000Z",
          "wordCount": 5090,
          "title": "Issues with single sign-on from Rippling to third-party applications",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        }
      ]
    },
    {
      "title": "Google Workspace Status Dashboard Updates",
      "feedUrl": "https://www.google.com/appsstatus/dashboard/en/feed.atom",
      "siteUrl": "https://www.google.com/appsstatus/dashboard/",
      "articles": []
    },
    {
      "title": "GitHub Status - Incident History",
      "feedUrl": "https://www.githubstatus.com/history.rss",
      "siteUrl": "https://www.githubstatus.com",
      "articles": [
        {
          "id": "https://www.githubstatus.com/incidents/mnv0g944fncw",
          "author": null,
          "description": "Nov 15, 11:34 UTC\nResolved - On 2023-11-15, from 09:44 to 10:42 UTC, some GitHub customers experienced increased latency or errors accessing repo data.\nHigh concurrent access to a specific git object exposed a bug that forced a backend service to perform excessive calculations, overloading the service. Access to this repo was paused while load was re-rerouted, mitigating the problem.\nThe conditions that triggered the expensive operations have been identified and refactored.\nNov 15, 11:33 UTC\nUpdate - Error rates and performance have returned to normal.\nNov 15, 11:21 UTC\nUpdate - We have identified the source of the issue and have removed the additional load from the service. Sporadic delays in pull request experiences and intermittent 500s are still occurring and impacting a very small percentage of traffic. Next update is expected within 30 minutes.\nNov 15, 11:04 UTC\nUpdate - We are seeing connectivity issues between some of our systems and git backend services. This is causing intermittent error responses and delays in pull request experiences for a very small percentage of traffic. We are investigating mitigations and expect to provide another update within 30 minutes.\nNov 15, 09:50 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/mnv0g944fncw",
          "publishedOn": "2023-11-15T11:34:14.000Z",
          "wordCount": 5190,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/knl5pxnx0byt",
          "author": null,
          "description": "Nov 13, 21:38 UTC\nResolved - Between 20:35 and 21:38 we experienced up to a 20 minute delay delivering around 30,000 notifications due to side effects of some planned maintenance on supporting systems. We have noted the unexpected user impact of this type of maintenance and will address it in future maintenance planning.\nNov 13, 21:38 UTC\nUpdate - An issue related to notifications has been resolved. Users should again be seeing their notifications.\nNov 13, 21:15 UTC\nUpdate - We're seeing issues related to notifications.\nNov 13, 21:13 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/knl5pxnx0byt",
          "publishedOn": "2023-11-13T21:38:40.000Z",
          "wordCount": 5088,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/wyk0ns67krlz",
          "author": null,
          "description": "Nov 11, 02:14 UTC\nResolved - On November 11, 2023, at 1:00 UTC, GitHub background jobs encountered delays lasting up to 50 minutes. This delay affected various services utilizing background jobs, including Actions, Webhooks, Pull Requests, and Pages. The impact persisted for approximately one hour until 2:10 UTC.\nDuring the incident, some customers experienced delays in starting Github Actions workflow runs and Pages builds. We estimate that about 10% of Actions workflow runs were delayed during the impact window and 99% of Pages builds failed from 1:00 UTC to 1:20 UTC. Users may have experienced a delay in seeing recent pushes reflected in pull request views. This delay averaged between 5 and 10 minutes and affected up to 30% of pull request page views during the incident. 1% of pull requ…",
          "link": "https://www.githubstatus.com/incidents/wyk0ns67krlz",
          "publishedOn": "2023-11-11T02:14:08.000Z",
          "wordCount": 5354,
          "title": "Incident with Pages, Webhooks and Actions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/m61vxgn4kvh2",
          "author": null,
          "description": "Nov  7, 14:25 UTC\nResolved - Our internal search infrastructure experienced increased latency and timeouts between 13:15 and 14:05 UTC leading to some end user timeouts and slow responses for any requests that made use of that subsystem. This included but was not limited to: user search, repository search, releases, and audit logs.\nWe mitigated the issue by migrating traffic to an older version of our search clusters and are investigating what caused the performance issues in our new clusters.\nNov  7, 14:25 UTC\nUpdate - API Requests is operating normally.\nNov  7, 14:15 UTC\nUpdate - Response times stabilized back to normal at 13:58 UTC.  We are continuing to monitor the slow dependency to ensure it's stable before resolving this incident.\nNov  7, 14:03 UTC\nUpdate - We're seeing intermittent spikes in latency of API requests and page loads.  We are investigating but do not have an ETA at this time.\nNov  7, 13:50 UTC\nUpdate - We are investigating reports of issues with service(s): Issues, API Requests. We will continue to keep users updated on progress towards mitigation.\nNov  7, 13:46 UTC\nUpdate - Issues is experiencing degraded performance. We are continuing to investigate.\nNov  7, 13:44 UTC\nInvestigating - We are investigating reports of degraded performance for API Requests",
          "link": "https://www.githubstatus.com/incidents/m61vxgn4kvh2",
          "publishedOn": "2023-11-07T14:25:40.000Z",
          "wordCount": 5211,
          "title": "Incident with API Requests and Issues",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "author": null,
          "description": "Nov  3, 19:21 UTC\nResolved - A performance and resilience optimization to the authorization microservice contained a memory leak that was exposed under high traffic. This resulted in a number of pages returning 404’s that should not have. Testing the build in our canary ring did not expose the service to sufficient traffic to discover the leak, allowing it to graduate to production at 6:37 PM UTC.  The memory leak under high load caused pods to crash repeatedly starting at 6:42 PM UTC, failing authorization checks. These failures triggered alerts at 6:44 PM UTC. Rolling back the authorization service change was delayed as parts of the deployment infrastructure relied on the authorization service and required manual intervention to complete. Rollback completed at 7:08 PM UTC and all impacte…",
          "link": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "publishedOn": "2023-11-03T19:21:48.000Z",
          "wordCount": 5723,
          "title": "Incident with Git Operations, Issues, Pull Requests, Actions, API Requests, Codespaces, Packages, Pages and Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "author": null,
          "description": "Oct 25, 22:15 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 21:37 UTC\nUpdate - The observed Copilot API error rate is around 5% of the requests. As a result, some of the Copilot code suggestions are skipped or not delivered on time.\nOct 25, 21:19 UTC\nUpdate - We are seeing an impact in the US region as well. We continue the investigation.\nOct 25, 20:53 UTC\nUpdate - Copilot is experiencing intermittent issues in our Japan region. Engineers are currently investigating.\nOct 25, 20:50 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "publishedOn": "2023-10-25T22:15:54.000Z",
          "wordCount": 5213,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "author": null,
          "description": "Oct 25, 13:02 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 12:56 UTC\nUpdate - We have applied a fix to help with Copilot performance. Initial signals show good recovery. We will continue to monitor for the time being and resolve when confident the issue has been resolved.\nOct 25, 12:29 UTC\nUpdate - We are investigating degraded performance in Europe for Copilot. We will continue to keep users updated on progress towards mitigation.\nOct 25, 12:10 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "publishedOn": "2023-10-25T13:02:51.000Z",
          "wordCount": 5202,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "author": null,
          "description": "Oct 22, 16:07 UTC\nResolved - This incident has been resolved.\nFrom 11:21 to 16:07 UTC some GitHub customers experienced errors cloning via workflows or via the command line.\nA third-party configuration change resulted in an unexpected behavior to our systems that resulted in Git clone failures. Once we detected the change we were able to disable it, and our systems started operating normally.\nWith the incident mitigated, we are working with our third-party provider to improve subsequent configuration change rollouts.\nOct 22, 15:58 UTC\nUpdate - We have mitigated the cause of the issue and are awaiting positive confirmation from impacted customers that the issue is fully resolved.\nOct 22, 15:34 UTC\nUpdate - We are currently investigating reports from some customers encountering errors when cloning repositories via workflows or via the command line. We do not currently have an ETA for resolution. Next update in 30 minutes.\nOct 22, 15:16 UTC\nInvestigating - We are investigating reports of degraded performance for Git Operations",
          "link": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "publishedOn": "2023-10-22T16:07:58.000Z",
          "wordCount": 5148,
          "title": "Incident with Git Operations",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        }
      ]
    },
    {
      "title": "Slack System Status",
      "feedUrl": "https://status.slack.com/feed/rss",
      "siteUrl": "https://status.slack.com/",
      "articles": [
        {
          "id": "https://status.slack.com//2023-11/2b97ec921a81988a",
          "author": null,
          "description": "Issue Summary:\n\r\nOn November 8, 2023 from 7:55 AM PT until 8:27 PM PT, a small number of users experienced errors when attempting to join or start huddles.\n\r\n\r\nWe traced this back to an unexpected spike in a database load for which we quickly saw recovery.\n\r\n\r\nWe monitored the situation closely and in an effort to prevent further unexpected error spikes, we implemented a strategic configuration adjustment to optimize the load on our database.\n\r\n\r\nAffected users should no longer experience issues with huddles.",
          "link": "https://status.slack.com//2023-11/2b97ec921a81988a",
          "publishedOn": "2023-11-17T21:42:44.000Z",
          "wordCount": 195,
          "title": "Incident: Some users may be experiencing issues with huddles",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/0e7bc0e7a7e7cd87",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 3:15 PM PST on November 14, 2023 to around 4:13 PM PST, a small number of customers using the Slack desktop app were unable to connect to Slack. This may have manifested as a \"Something's gone awry\" error page.\n\r\n\r\nA recent code change inadvertently introduced a logic error that prevented the desktop app from connecting as expected. We reverted this code change as an immediate mitigation step, then rolled out a fix to correct the logic error. \n\r\n\r\nThe fix resolved the issue for all affected customers, restoring full access to Slack.",
          "link": "https://status.slack.com//2023-11/0e7bc0e7a7e7cd87",
          "publishedOn": "2023-11-15T03:01:39.000Z",
          "wordCount": 264,
          "title": "Incident: Slack not loading for some users",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/16fed44d7948cf49",
          "author": null,
          "description": "Issue Summary:\n\r\n\r\nFrom 4:00 PM PDT on October 31, 2023 to 2:00 PM PDT on November 9, 2023, user presence would unexpectedly change to away or inactive.\n\r\n\r\nWe determined that an error during a routine system optimization on connectivity state caused this issue. \n\r\n\r\nWe reverted the change which fixed the issue for all affected customers.\n\r\n\r\nThank you for your patience while we resolved this.",
          "link": "https://status.slack.com//2023-11/16fed44d7948cf49",
          "publishedOn": "2023-11-13T15:57:36.000Z",
          "wordCount": 253,
          "title": "Incident: User presence is unexpectedly changing",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/ea3a2a3e32e79902",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 10:00 PM PST on November 7, 2023 to 1:15 PM PST on November 8, 2023, some users experienced issues with their user status not updating, removing previews, and being unable to mark channels as read. Some keyboard shortcuts were also affected and were unable to be used.\n\r\n\r\nWe determined that a recent code change was the root cause for the unexpected behaviour with these features.\n\r\n\r\nTo restore functionality, we reverted the related code. We then did some additional testing and monitoring to confirm all issues were fully resolved and Slack was operating as expected.",
          "link": "https://status.slack.com//2023-11/ea3a2a3e32e79902",
          "publishedOn": "2023-11-09T04:29:31.000Z",
          "wordCount": 413,
          "title": "Incident: Issues with user status, read state, and file previews",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "author": null,
          "description": "Issue summary:\n\r\nOn November 4, 2023, between 5:09 PM PDT and 5:20 PM PDT, many customers were unable send messages or to connect to Slack.\n\r\n\r\nA routine code change introduced a database error that prevented cached data from being cleared correctly, resulting in severe performance issues.\n\r\n\r\nWe rolled back the code change and refreshed all affected servers, resolving the issue for all users.",
          "link": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "publishedOn": "2023-11-08T01:59:22.000Z",
          "wordCount": 176,
          "title": "Outage: Users unable to connect to Slack or send messages",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/8c886b57284762b6",
          "author": null,
          "description": "Issue summary:\n\r\n\r\nFrom 5:01 PM PDT on October 31, 2023 to around 5:54 PM PDT, customers with international data residency in the Paris, France region, experienced issues connecting to Slack and sending messages.\n\r\n\r\nA routine credential rotation caused database sync issues for the Paris, France data residency region. We reverted this code change and restarted the affected databases, resolving the issue for all impacted customers. \n\r\n\r\nOnce we had mitigated the immediate impact and restored connectivity for customers in the Paris, France data residency region, we reviewed the credential rotation for all other data residency regions to ensure the same issue would not occur anywhere else.\n\r\n\r\nPlease note that the start and end time of the incident have been edited for accuracy.",
          "link": "https://status.slack.com//2023-10/8c886b57284762b6",
          "publishedOn": "2023-11-02T03:45:50.000Z",
          "wordCount": 409,
          "title": "Outage: Issues for customers enrolled in the French data region",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "author": null,
          "description": "Customers should no longer be experiencing any connection issues with Slack. Apologies for the trouble today and thank you for your patience.",
          "link": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "publishedOn": "2023-10-24T21:08:02.000Z",
          "wordCount": 195,
          "title": "Incident: A small number of users are having problems loading Slack.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        }
      ]
    },
    {
      "title": "Make Status - Incident History",
      "feedUrl": "https://status.make.com/history.rss",
      "siteUrl": "https://status.make.com",
      "articles": [
        {
          "id": "https://status.make.com/incidents/d82gd0vnvgz1",
          "author": null,
          "description": "Nov 16, 09:45 CET\nResolved - Due to a configuration error, some scenarios in the eu1.make.com, eu2.make.com and us1.make.com zones may have intermittently failed to execute and in case of multiple consecutive errors get disabled by the system. According to our telemetry, the number of impacted scenarios was very small. Customers may have experienced this behavior between 9:45am and 8:00pm CET. We have addressed the configuration problem and all executions are now stable. We will continue monitoring the situation.",
          "link": "https://status.make.com/incidents/d82gd0vnvgz1",
          "publishedOn": "2023-11-16T08:45:00.000Z",
          "wordCount": 3896,
          "title": "Intermittent scenario execution errors in eu1, eu2 and us1 zones",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.make.com/incidents/dzjb57y129qn",
          "author": null,
          "description": "Nov 14, 18:04 CET\nResolved - This incident has been resolved.\nNov 14, 13:53 CET\nUpdate - The current situation is stable, and after performing a series of checks, no issues were observed. We will maintain ongoing monitoring for the next 4 hours, and if everything remains stable during this period, we will proceed to resolve the incident.\nNov 13, 21:16 CET\nMonitoring - The issue is currently stable. We will continue careful monitoring of the situation and provide updates regularly.\nNov 13, 16:58 CET\nUpdate - Our team is continuing to investigate the technical difficulties affecting webhooks and mailhooks on eu2.make.com. At this time, we have not yet identified the root cause, and users may still experience sporadic delays in the processing of these services.\nNov 13, 14:54 CET\nInvestigating - We are currently experiencing technical difficulties with webhooks and mailhooks on eu2.make.com. Users may encounter delays in the processing of webhooks and mailhooks.\nWe will provide another update on this Statuspage within the next 2 hours or as soon as more information becomes available.",
          "link": "https://status.make.com/incidents/dzjb57y129qn",
          "publishedOn": "2023-11-14T17:04:00.000Z",
          "wordCount": 3991,
          "title": "EU2 hooks delayed processing",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.make.com/incidents/g6gklsv4shx6",
          "author": null,
          "description": "Nov  7, 12:43 CET\nResolved - This incident has been resolved.\nNov  6, 16:30 CET\nMonitoring - We have noticed a degradation of performance on eu1.make.celonis.com. A fix has been applied and the system is fully operational again.",
          "link": "https://status.make.com/incidents/g6gklsv4shx6",
          "publishedOn": "2023-11-07T11:43:11.000Z",
          "wordCount": 3844,
          "title": "Failing to execute scenarios",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}