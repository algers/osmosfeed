{
  "sources": [
    {
      "title": "BookingSync.com news",
      "feedUrl": "https://changelog.bookingsync.com/rss",
      "siteUrl": "https://changelog.bookingsync.com",
      "articles": [
        {
          "id": "276591",
          "author": "Maud , Partnership Manager",
          "description": "New!\n  \nWe are delighted to announce our new partnership with Plum Guide. 🥳\nPlum Guide is a curated online travel agency that showcases the world’s best homes - they're like the Michelin Guide, but for homes.\nThey do this by independently vetting every home and property manager in each market and selecting only the best to feature in our collection.\nThey were recently named ‘Best OTA’ as the 2023 Shortyz awards. 🏆\n\n\n\n\n\n💡 How can Plum Guide help you?\nIncremental revenue thanks to unique guests\nEditorial style listings built by their Merchandising team\nDedicated functions for individual Hosts & Property Management Companies\n\n\n\n🤩 Benefits:\n2.1x longer stays than on Airbnb\nOver 50% of bookings occur outside of high season\nDedicated AM, Sales & Onboarding teams for both individual Hosts and Property Management Companies\nMajority of Plum Guide Guests have never booked on other OTAs\nHigh Quality Guests (avg. age is 45)\nGuest Cancellation Rate is <0.5%\n\n\n\nInterested in advertising your properties on Plum Guide?\n👉 Install the app here\n\ndedicated manual page or book a call with your Account Manager here.  \nWe hope that this new channel will help you get more bookings from travelers around the world!\nDon’t hesitate to contact the Plum Guide team supply@plumguide.com or our support team for any questions.\n\n\nBest regards,",
          "link": "https://changelog.bookingsync.com/new-plum-guide-is-now-available-on-smily!-276591",
          "publishedOn": "2023-10-12T16:07:05.000Z",
          "wordCount": 464,
          "title": "NEW - Plum Guide is now available on Smily! 🎉",
          "imageUrl": "https://cloud.headwayapp.co/changelogs_images/images/big/000/116/376-84bdc4de7ba351ee521a89528433b6b6d5876cfd.png"
        }
      ]
    },
    {
      "title": "Security Bulletins on Tailscale",
      "feedUrl": "https://tailscale.com/security-bulletins/index.xml",
      "siteUrl": "https://tailscale.com/security-bulletins/",
      "articles": [
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "author": null,
          "description": "Description: Privilege escalation bugs in the Tailscale\nKubernetes operator’s API proxy allowed authenticated tailnet clients\nto send Kubernetes API requests as the operator’s service account.\nTailscale Kubernetes operator version v1.53.37 fixes the issue and\nusers of the operator who enable the API proxy functionality should\nupdate as described below.\nWhat happened?\nThe Tailscale Kubernetes operator can optionally act as an API server\nproxy\nfor the cluster’s Kubernetes API. This proxy allows authenticated\ntailnet users to use their tailnet identity in Kubernetes\nauthentication and RBAC rules. The API server proxy uses\nimpersonation\nheaders\nto translate tailnet identities to Kubernetes identities.\nThe operator prior to v1.53.37 has two bugs in the forwarding logic,\nwhich affects different …",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "publishedOn": "2023-11-01T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-008",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        },
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "author": null,
          "description": "Description: Microsoft Defender is flagging Tailscale 1.46.1 as malware.\nThese classifications are false positives, and we are working with Microsoft to\nresolve the situation.\nAs of 2023-10-27 1:05 AM UTC, we have confirmed that Microsoft have addressed\nthe false positive, meaning Defender no longer flags Tailscale 1.46.1 as\nmalware. A rescan of tailscaled.exe 1.46.1 on VirusTotal confirms this.\nWhat happened?\nMicrosoft Defender was flagging Tailscale 1.46.1 as malware. This caused\nDefender to quarantine the binaries, meaning they could not run.\nWe submitted Tailscale 1.46.1 to Microsoft to investigate the false positive,\nwho then updated Defender to avoid flagging this release as malware at\n2023-10-27 1:05 AM UTC.\nWho is affected?\nPeople using Defender and Tailscale 1.46.1.\nWhat is the impact?\nTailscale will not run on affected machines.\nWhat do I need to do?\nTo resolve this issue on your own tailnet, you can take either or both of 2\napproaches:\nUpdate to a newer version of Tailscale. Newer versions are not affected by this problem.\nCreate an exception in Microsoft Defender. Microsoft has published instructions explaining how to do this.\nUpdate Microsoft Defender.",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "publishedOn": "2023-10-26T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-007",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        }
      ]
    },
    {
      "title": "Airbnb API Status - Incident History",
      "feedUrl": "https://airbnbapi.statuspage.io/history.rss",
      "siteUrl": "https://airbnbapi.statuspage.io",
      "articles": [
        {
          "id": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "author": null,
          "description": "Oct 31, 10:00 PDT\nResolved - This incident has been resolved.\nOct 30, 18:14 PDT\nMonitoring - Today (Oct 30, 2023) between 1:00 PM and 3:00 PM PDT we encountered an incident that impacted our webhooks functionality. As a result, some reservations may have been missing webhooks during this time period. The issue has been resolved, and our team is actively monitoring the results to ensure the stability of our systems. If you have noticed any reservations missing webhooks, we recommend using the GET Reservations API to retrieve the details of those reservations. Alternatively, you can contact us with a list of affected reservations, and we will assist you in backfilling the missing information.\nWe apologize for any inconvenience this may have caused and appreciate your understanding.",
          "link": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "publishedOn": "2023-10-31T17:00:19.000Z",
          "wordCount": 3930,
          "title": "Issue Affecting Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Twilio Status - Incident History",
      "feedUrl": "https://status.twilio.com/history.rss",
      "siteUrl": "https://status.twilio.com",
      "articles": [
        {
          "id": "https://status.twilio.com/incidents/m63pb097ncv0",
          "author": null,
          "description": "Nov  4, 15:57 PDT\nMonitoring - We are observing recovery in receiving message delivery reports when sending SMS to AT&T network in Mexico from short codes 334-050. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  4, 14:11 PDT\nUpdate - We are continuing to experience delays in receiving message delivery reports when sending SMS to AT&T network in Mexico from short codes 334-050. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 4 hours or as soon as more information becomes available.\nNov  4, 12:10 PDT\nUpdate - We are continuing to experience delays in receiving message delivery reports when sending SMS to AT&T network in Mexico from short codes  334-050. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available.\nNov  4, 11:12 PDT\nInvestigating - We are experiencing delays in receiving message delivery reports when sending to to AT&T network in Mexico from short codes  334-050. Our engineers are working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.",
          "link": "https://status.twilio.com/incidents/m63pb097ncv0",
          "publishedOn": "2023-11-04T22:57:38.000Z",
          "wordCount": 7455,
          "title": "SMS Delivery Report Delays When Sending SMS to AT&T Network in Mexico From Short Codes 334-050",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/6f80rpsmph3w",
          "author": null,
          "description": "Nov  4, 14:01 PDT\nResolved - We are no longer experiencing MMS delivery delays when sending messages to Verizon Network in the US from a subset of short codes. This incident has been resolved.\nNov  4, 12:10 PDT\nMonitoring - We are observing recovery in MMS delivery delays when sending messages to Verizon Network in the US from a subset of short codes. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  4, 10:15 PDT\nUpdate - We continue to experience MMS delivery delays when sending messages to Verizon Network in the US from a subset of short codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  4, 09:16 PDT\nInvestigating - We are experiencing MMS delivery delays when sending messages to Verizon Network in the US from a subset of short codes. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.",
          "link": "https://status.twilio.com/incidents/6f80rpsmph3w",
          "publishedOn": "2023-11-04T21:01:46.000Z",
          "wordCount": 7432,
          "title": "MMS Delivery Delays When Sending Messages to Verizon Network in the US From a Subset of Short Codes",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/6wlnhfppv57x",
          "author": null,
          "description": "Nov  4, 13:00 PDT\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nOct 30, 06:42 PDT\nScheduled - Our carrier partner Vodafone UK is conducting a planned maintenance from 04 November 2023 at 13:00 PDT until 04 November 2023 at 23:00 PDT. During the maintenance window, there could be intermittent API request failures for Vodafone UK customers.\nImpacted Products: Lookup Identity Match, Legacy Identity MatchAndAttributes",
          "link": "https://status.twilio.com/incidents/6wlnhfppv57x",
          "publishedOn": "2023-11-04T20:00:13.000Z",
          "wordCount": 7278,
          "title": "United Kingdom Account Security Carrier Partner Maintenance - Vodafone",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/424q5vn77m0k",
          "author": null,
          "description": "Nov  4, 04:45 PDT\nUpdate - We continue to experience SMS delivery delays when sending messages to the CTBC/Algar Telecom network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 16 hours or as soon as more information becomes available.\nNov  3, 20:54 PDT\nUpdate - We continue to experience SMS delivery delays when sending messages to the CTBC/Algar Telecom network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 8 hours or as soon as more information becomes available.\nNov  3, 16:55 PDT\nUpdate - We continue to experience SMS delivery delays when sending messages to the CTBC/Algar Telecom network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 4 hours or as soon as more information becomes available.\nNov  3, 14:55 PDT\nUpdate - We are still experiencing SMS delivery delays when sending messages to the CTBC/Algar Telecom network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  3, 13:55 PDT\nUpdate - We are experiencing SMS delivery delays when sending messages to the CTBC/Algar Telecom network in Brazil. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.\nNov  3, 13:42 PDT\nInvestigating - Our monitoring systems have detected a potential issue with SMS delivery delays to Brazil. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/424q5vn77m0k",
          "publishedOn": "2023-11-04T11:45:02.000Z",
          "wordCount": 7504,
          "title": "SMS Delivery Delays to CTBC/Algar Telecom Network in Brazil",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/4xlv6n9xw8w0",
          "author": null,
          "description": "Nov  4, 04:27 PDT\nResolved - The issue with call failures in the US1 region has been resolved and the service is operating normally at this time.\nNov  4, 04:05 PDT\nMonitoring - Voice calls in the US1 region are now operating normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.\nNov  4, 03:35 PDT\nUpdate - We are investigating an issue with calls failing in the US1 region. We expect to provide another update in 1 hour or as soon as more information becomes available.\nNov  4, 03:12 PDT\nInvestigating - Our monitoring systems have detected a potential issue with calls failing in the US1 region. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/4xlv6n9xw8w0",
          "publishedOn": "2023-11-04T11:27:40.000Z",
          "wordCount": 7336,
          "title": "Voice Call Failures in the US1 Region",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/770m6tpctxv4",
          "author": null,
          "description": "Nov  3, 20:55 PDT\nResolved - The intermittent connectivity issue for JS client for Sydney edge has been resolved and is now operating normally at this time.\nNov  3, 20:16 PDT\nIdentified - Our engineers have identified an intermittent connectivity issue for JS client for Sydney edge and are working to deploy a fix. We expect to provide another update in 1 hour or as soon as more information becomes available.",
          "link": "https://status.twilio.com/incidents/770m6tpctxv4",
          "publishedOn": "2023-11-04T03:55:06.000Z",
          "wordCount": 7268,
          "title": "Intermittent connectivity issue for JS client for Sydney edge",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/x85wb46bp9wl",
          "author": null,
          "description": "Nov  3, 20:28 PDT\nResolved - We are no longer experiencing SMS delivery delays when sending messages to multiple networks in Morocco. This incident has been resolved.\nNov  3, 18:31 PDT\nMonitoring - We are observing recovery in SMS delivery delays when sending messages to multiple networks in Morocco. We will continue monitoring the service to ensure a full recovery. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  3, 16:36 PDT\nUpdate - We are still experiencing SMS delivery delays when sending messages to multiple networks in Morocco. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 2 hours or as soon as more information becomes available.\nNov  3, 15:39 PDT\nUpdate - We are experiencing SMS delivery delays when sending messages to multiple networks in Morocco. Our engineers are working with our carrier partner to resolve the issue. We will provide another update in 1 hour or as soon as more information becomes available.\nNov  3, 15:30 PDT\nInvestigating - Our monitoring systems have detected a potential issue with SMS delivery delays to multiple networks in Morocco. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/x85wb46bp9wl",
          "publishedOn": "2023-11-04T03:28:46.000Z",
          "wordCount": 7419,
          "title": "SMS Delivery Delays to Multiple Networks in Morocco",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/l8l5jbqdb4pv",
          "author": null,
          "description": "Nov  3, 15:42 PDT\nResolved - We experienced elevated 5XX count for media retrieval for 4 minutes between 2:38 AM and 2:42 AM Pacific Time on 11/03/2023. A subset of customers who attempted to fetch media from the /Messages/.../Media API received 5XX HTTP responses. The issue has now been resolved.",
          "link": "https://status.twilio.com/incidents/l8l5jbqdb4pv",
          "publishedOn": "2023-11-03T22:42:07.000Z",
          "wordCount": 7244,
          "title": "Elevated 5XX HTTP Errors for Media Retrieval via the Media API",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/s4rzzkhct58t",
          "author": null,
          "description": "Nov  3, 13:39 PDT\nResolved - We have fully investigated the issue causing delays in the Billing platform triggered by our automated alert, and it was determined that there is no noticeable customer impact. All systems are operational.\nNov  3, 13:27 PDT\nInvestigating - Our monitoring systems have detected a potential issue with delays in the Billing platform. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/s4rzzkhct58t",
          "publishedOn": "2023-11-03T20:39:15.000Z",
          "wordCount": 7262,
          "title": "On Call Engineers are Investigating an issue",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/488k16dmsdqc",
          "author": null,
          "description": "Nov  3, 13:01 PDT\nResolved - The issue causing failures of inbound calls from a subset of AT&T subscribers in the Texas area to Twilio has been resolved, and the service is functioning normally at this time.\nNov  2, 16:31 PDT\nMonitoring - We have started seeing recovery from the issue causing failures of inbound calls from a subset of AT&T subscribers in the Texas area to Twilio. We will continue to monitor to ensure a full recovery. We expect to provide another update in 24 hours or as soon as more information becomes available.\nNov  2, 15:00 PDT\nUpdate - The issue causing call failures from a subset of AT&T subscribers in the Texas area to Twilio has been identified. We continue to work with our carrier partner to resolve the issue. We expect to provide another update in 2 hours or as soon as more information becomes available.\nNov  2, 14:00 PDT\nIdentified - The issue causing call failures from a subset of AT&T subscribers in the Texas area to Twilio has been identified. We are actively working with our carrier partner to resolve the issue. We expect to provide another update in 1 hour or as soon as more information becomes available.\nNov  2, 13:41 PDT\nUpdate - We are observing an issue with AT&T calls failing to Twilio, where callers are receiving a message reporting \"all circuits are busy\". We are currently working with our carrier partner to resolve the issue. We expect to provide another update in 30 minutes or as soon as more information becomes available.\nNov  2, 13:22 PDT\nInvestigating - We've become aware of a potential issue with inbound calls from AT&T subscribers failing. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/488k16dmsdqc",
          "publishedOn": "2023-11-03T20:01:56.000Z",
          "wordCount": 7511,
          "title": "Call Failures From a Subset of AT&T Numbers to Twilio",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.twilio.com/incidents/gnfdp9wb57l4",
          "author": null,
          "description": "Nov  3, 09:46 PDT\nResolved - TrustHub incident in the console has been resolved and service is operating normally at this time.\nNov  3, 08:58 PDT\nMonitoring - TrustHub in the console is now operating normally. We will continue to monitor for system stability. We'll provide another update in 30 minutes or as soon as more information becomes available.\nNov  3, 08:54 PDT\nUpdate - We are investigating issues with TrustHub in the console. Customers are unable to use the Console to create secondary business profiles when within the context of a subaccount. We expect to provide another update in 1 hour or as soon as more information becomes available.\nNov  3, 08:24 PDT\nInvestigating - Our monitoring systems have detected a potential issue with the TrustHub Console. Our engineering team has been alerted and is actively investigating. We will update as soon as we have more information.",
          "link": "https://status.twilio.com/incidents/gnfdp9wb57l4",
          "publishedOn": "2023-11-03T16:46:20.000Z",
          "wordCount": 7349,
          "title": "Console Issues When creating Secondary Business Profiles in TrustHub",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "DigitalOcean Status - Incident History",
      "feedUrl": "https://status.digitalocean.com/history.rss",
      "siteUrl": "http://status.digitalocean.com",
      "articles": [
        {
          "id": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "author": null,
          "description": "Nov  4, 08:18 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting App Platform Deployments. \nFrom 11:54 on Nov 2nd to 06:42 on Nov 4th UTC, App Platform users may have experienced delays when deploying new apps or when deploying updates to existing Apps. Our Upstream provider and the Engineering team closely worked together to resolve the issue. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel. Thank you for your patience and we apologize for any inconvenience.\nNov  4, 06:53 UTC\nUpdate - As per the recent update from our Upstream provider, they fully recovered the services used…",
          "link": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "publishedOn": "2023-11-04T08:18:05.000Z",
          "wordCount": 6442,
          "title": "App Platform Deployments",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "author": null,
          "description": "Nov  3, 23:14 UTC\nResolved - Our Engineering team has confirmed the full resolution of issue impacting network connectivity in our SFO regions. \nUsers should no longer experience any latency or timeout issue with any of the Droplet based services. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nNov  3, 20:58 UTC\nMonitoring - As of 19:55 UTC, our Engineering team has confirmed that a fix has been implemented by our upstream carrier to mitigate the cause of the issue impacting network connectivity in our SFO region. \nWe are closely monitoring the situation and will update as soon as we have more information from the provider.\nNov  3, 19:43 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting network connectivity in our SFO region. Upstream congestion with a network provider between Los Angeles and Dallas is impacting traffic traversing out of our SFO datacentres.\nA case has been opened by our team with the provider. Our team has attempted to shift traffic to improve the situation, but unfortunately, we continue to see approximately 10% of customer traffic impacted by this issue.\nOur team is working on an option to shift to an alternate provider if this issue is not able to be resolved by the provider in a timely manner. We will share another update once we have further information from the provider or we have an update from our Engineering team.\nNov  3, 19:27 UTC\nInvestigating - As of 17:40 UTC, our Engineering team is investigating an issue impacting networking in the SFO regions. During this time, a subset of users may experience packet loss/latency and timeouts with Droplet based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "publishedOn": "2023-11-03T23:14:12.000Z",
          "wordCount": 6235,
          "title": "Networking in SFO Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "author": null,
          "description": "Nov  2, 11:50 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting the Cloud Control Panel, API, and multiple services.\nFrom 05:05 - 08:40 UTC, users may have encountered errors with the Cloud Control Panel and public API while attempting to create new user registrations, or while making payments. Users also may have experienced issues with processing Droplet and Managed Kubernetes cluster creations along with Droplet-based events and experienced latencies while accessing our Cloud Control Panel along with the DigitalOcean Container Registry. Our Upstream provider and the Engineering team closely worked together to resolve the issues. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf you…",
          "link": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "publishedOn": "2023-11-02T11:50:30.000Z",
          "wordCount": 6571,
          "title": "Multiple services down and API availability",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "author": null,
          "description": "Oct 27, 14:47 UTC\nResolved - As of 14:30 UTC, our Engineering team has resolved the issue impacting Spaces in our SGP1 and SFO3 regions. Users should no longer experience slowness or timeouts when trying to create or access their Spaces resources in the SGP1 and SFO3 regions. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nOct 27, 14:03 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue impacting Spaces in our SGP1 and SFO3 regions and is monitoring the situation closely. We will post an update as soon as the issue is fully resolved.\nOct 27, 12:04 UTC\nUpdate - Our Engineering team is continuing to investigate an issue impacting Object Storage in our SGP1 region. Additionally, we have become aware that this issue has also impacted Object Storage in the SFO3 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 27, 11:25 UTC\nInvestigating - As of 10:22 UTC, our Engineering team is investigating an issue with Object Storage in our SGP1 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "publishedOn": "2023-10-27T14:47:58.000Z",
          "wordCount": 6209,
          "title": "Object Storage - SGP1 and SFO3",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "author": null,
          "description": "Oct 26, 15:52 UTC\nResolved - As of 15:45 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.\nIf you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. \nThank you for your patience and we apologize for any inconvenience.\nOct 26, 15:18 UTC\nMonitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.\nOct 26, 14:25 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting the networking in the LON1 region and is actively working on a fix. During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. \nWe will post an update as soon as additional information is available.\nOct 26, 12:50 UTC\nInvestigating - As of 11:40 UTC, our Engineering team is investigating an issue impacting the networking in the LON1 region. During this time, a subset of users may experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will share an update once we have further information.",
          "link": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "publishedOn": "2023-10-26T15:52:41.000Z",
          "wordCount": 6179,
          "title": "Network Connectivity in LON1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "author": null,
          "description": "Oct 24, 23:00 UTC\nCompleted - The scheduled maintenance has been completed.\nOct 24, 21:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nOct 20, 19:40 UTC\nUpdate - After a thorough review by the team performing this maintenance, we have determined that our initial messaging does not convey the complete scope and potential impact of this event. Existing infrastructure will continue running without issue during this maintenance window. However, users may experience increased latency with some platform operations, including: \nCloud Control Panel and API operations\nEvent processing\nDroplet creates, resizes, rebuilds, and power events\nManaged Kubernetes reconciliation and scaling\nLoad Balancer operations\nContainer Registry operations\nApp …",
          "link": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "publishedOn": "2023-10-24T23:00:07.000Z",
          "wordCount": 6222,
          "title": "Core Infrastructure Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "author": null,
          "description": "Oct 24, 09:15 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.\nThank you for your patience and we apologize for any inconvenience.\nOct 24, 09:03 UTC\nMonitoring - Our Engineering team has received communication from the upstream provider that a fix to resolve the networking issue has been implemented. We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.\nOct 24, 07:41 UTC\nIdentified - Our Engineering team has identified the cause of issues impacting networking in multiple regions. The issues are a direct result of traffic congestion from our upstream providers, which is in the process of being repaired.\nAt this time, a subset of users will continue to experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nWe apologize for the inconvenience and will share an update once we have more information.\nOct 24, 06:11 UTC\nInvestigating - As of 05:30 UTC, our Engineering team is investigating an issue impacting the networking in multiple regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nAt the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "publishedOn": "2023-10-24T09:15:10.000Z",
          "wordCount": 6208,
          "title": "Networking in multiple regions.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/f38zfs58jkl8",
          "author": null,
          "description": "Oct 17, 17:33 UTC\nResolved - As of 16:10 UTC, our Engineering team has confirmed the full resolution of the issue impacting our App platform service where users were not able to access their Apps via Cloud Panel. Apps should be loading fine https://cloud.digitalocean.com/apps without any errors. \nWe appreciate your patience during the process and if you continue to experience any issues  please open a ticket with our support team.\nOct 17, 16:51 UTC\nMonitoring - Our Engineering team has deployed a fix to resolve the ongoing issue with accessing Apps list via Cloud panel for our App platform service. As of 16:10 UTC the situation started to improve and the users should be able to access their apps via Cloud panel UI without any issues. \nWe are monitoring the situation closely and will post an update once the issue is completely resolved.\nOct 17, 16:19 UTC\nIdentified - Our Engineering team has identified an issue impacting App platform service in all regions. During this time users may experience issues while loading apps via https://cloud.digitalocean.com/apps and the requests appear to be timing out. New App deployments via API or doctl and the existing deployed apps are not impacted by this incident.  \nOur team is working to mitigate the issue and we will provide an update as soon as possible.",
          "link": "https://status.digitalocean.com/incidents/f38zfs58jkl8",
          "publishedOn": "2023-10-17T17:33:44.000Z",
          "wordCount": 6143,
          "title": "App Platform Accessibility via Cloud",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/7548dwrnthz8",
          "author": null,
          "description": "Oct 17, 06:48 UTC\nResolved - As of 06:25 UTC, our Engineering team has confirmed that the issue impacting the newly created Managed Database Clusters has been fully resolved. \nUsers will no longer experience issues with the newly created Managed Database Cluster.\nIf you continue to experience any issues with Managed Database Clusters please open a ticket with our support team. Thank you for your patience.\nOct 17, 06:24 UTC\nMonitoring - As of 06:15 UTC, our Engineering team has identified the issue impacting our Managed Database product and a fix has been implemented to mitigate the issue. Currently, new users may no longer experience issues with the newly created Managed Database Cluster due to hostname resolution.\nUsers who already have a Managed Database cluster are not impacted by this incident.\nThank you for your patience and we apologize for the inconvenience caused. We are monitoring the situation and will post an update once the incident is completely resolved.\nOct 17, 04:22 UTC\nInvestigating - Our Engineering team is currently investigating reports of issues impacting a subset of users using our Managed Database Clusters. \nDuring this time, hostnames for the newly created Managed Database Cluster are not resolving. However, previously created Clusters are not impacted due to the same.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/7548dwrnthz8",
          "publishedOn": "2023-10-17T06:48:06.000Z",
          "wordCount": 6149,
          "title": "Managed Database Cluster",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/yxfmyjl0kmpr",
          "author": null,
          "description": "Oct 16, 23:14 UTC\nResolved - From 22:45 to 23:00 UTC, our Engineering team has reported an issue with DigitalOcean Control Panel and API.\n During that time, Customers may have experienced intermittent timeout errors while using DigitalOcean Control Panel and API. \nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.",
          "link": "https://status.digitalocean.com/incidents/yxfmyjl0kmpr",
          "publishedOn": "2023-10-16T23:14:32.000Z",
          "wordCount": 5980,
          "title": "DigitalOcean Control Panel and API",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/k58m2k4qcchf",
          "author": null,
          "description": "Oct 15, 01:04 UTC\nResolved - Our Engineering team has confirmed the full resolution of the networking issue in the SFO regions. If you continue to see issues with latency or packet loss in the SFO region please reach out directly to our support team for assistance.\nOct 14, 20:30 UTC\nUpdate - Our Engineering team is continuing to monitor the networking issue in the SFO regions. So far, we haven't observed any major spike in latency or packet loss with network connections going in or out of the SFO regions. However, we will post an update as soon as the issue is fully resolved.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 14, 15:25 UTC\nMonitoring - Our Engineering team has detected a recurrence of the networking issue identified in a previous incident today: \nhttps://status.digitalocean.com/incidents/bhpslzd37517\nOur team is actively monitoring the situation and has implemented traffic routing changes where applicable to alleviate the latency. Some users may still experience packet loss or increased latency accessing the resources in the SFO region from certain ISPs.\nWe apologize for any inconvenience caused and will provide updates as the situation progresses.",
          "link": "https://status.digitalocean.com/incidents/k58m2k4qcchf",
          "publishedOn": "2023-10-15T01:04:23.000Z",
          "wordCount": 6118,
          "title": "Network Latency in SFO Region",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/bhpslzd37517",
          "author": null,
          "description": "Oct 14, 11:46 UTC\nResolved - Our Engineering team has confirmed full resolution of the issue with networking in affected regions. Users should no longer experience timeouts or delays when connecting to or from these regions. \nIf you continue to experience problems, please open a ticket with our support team.\nThank you for your patience and we apologize for any inconvenience.\nOct 14, 11:10 UTC\nMonitoring - As of 10:55 UTC, our Engineering team has implemented a fix to address the networking problem in the affected regions and is currently monitoring the situation. Users should no longer face timeouts or encounter delays when connecting to or from these regions. We will post an update as soon as the issue is fully resolved.\nOct 14, 10:30 UTC\nUpdate - As of 10:27 UTC, our Engineering team is continuing to investigate an issue with networking in our SFO regions. Additionally, it has come to our attention that this issue has affected other regions, specifically SGP1, SYD1, and NYC3. Users may encounter timeouts or experience delays in network connections going in and out of these regions. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 14, 09:47 UTC\nUpdate - We are continuing to investigate this issue.\nOct 14, 09:12 UTC\nInvestigating - As of 8:30 UTC, our Engineering team is investigating an issue with networking in our SFO regions. During this time, users may experience timeouts or latency with network connections going in or out of the SFO regions. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/bhpslzd37517",
          "publishedOn": "2023-10-14T11:46:50.000Z",
          "wordCount": 6231,
          "title": "Network Latency in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/lh3j71zp57hx",
          "author": null,
          "description": "Oct 12, 19:17 UTC\nResolved - Our Engineering team has confirmed the resolution of the issue that impacted the Spaces CDN. Objects should be accessible over the CDN endpoint without any issues. However, HTTP/2 is temporarily unavailable due to upstream issues, and due to this HTTP/2 requests should automatically be re-negotiated to 1.1, but in case you experience failures please open a ticket with our support team.\nThank you for your patience and we apologize for any inconvenience.\nOct 12, 16:10 UTC\nUpdate - After our upstream provider implemented a remediation step to resolve the issue with the Spaces CDN, the Spaces CDN is serving the objects stored in the Spaces bucket without any errors or performance issues. However, we are still monitoring the situation closely and we'll share more in…",
          "link": "https://status.digitalocean.com/incidents/lh3j71zp57hx",
          "publishedOn": "2023-10-12T19:17:29.000Z",
          "wordCount": 6454,
          "title": "Spaces CDN in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/fsfsv9fj43w7",
          "author": null,
          "description": "Oct 11, 07:26 UTC\nResolved - Our Engineering team has resolved the issue with the Managed Kubernetes Service. A daemonset has been released to all existing clusters eliminating the auto-update process and it will be removed again going forward. If you find worker nodes that could still be affected by a prior occurrence of this incident, please replace them for permanent mitigation.\nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nOct 11, 04:06 UTC\nMonitoring - Our Engineering team has completed the work for both items. New images have been released that eliminate the auto-update process and the daemonset has been applied to all existing clusters. \nGiven this, the team is not expecting to see a recurrence of this inciden…",
          "link": "https://status.digitalocean.com/incidents/fsfsv9fj43w7",
          "publishedOn": "2023-10-11T07:26:53.000Z",
          "wordCount": 7715,
          "title": "Managed Kubernetes Service",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/08q0d3g8dtp3",
          "author": null,
          "description": "Oct  7, 20:01 UTC\nResolved - As of 19:52 UTC, our Engineering team has confirmed the full resolution of the issue impacting the listing of Spaces buckets in the FRA1 region via the Cloud Control Panel. \nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.\nOct  7, 19:38 UTC\nMonitoring - Our engineering team has implemented a fix to resolve the issue with listing Spaces in our FRA1 region via the Cloud Panel and is monitoring the situation. We will post an update as soon as the issue is fully resolved.\nOct  7, 19:15 UTC\nInvestigating - Our Engineering team is investigating an issue with Spaces in our FRA1 region. As of 13:40 UTC, users may experience issues listing the Spaces created in the FRA1 region via the Cloud Panel. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/08q0d3g8dtp3",
          "publishedOn": "2023-10-07T20:01:58.000Z",
          "wordCount": 6081,
          "title": "Spaces listing in FRA1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Notion Status - Incident History",
      "feedUrl": "https://status.notion.so/history.rss",
      "siteUrl": "https://status.notion.so",
      "articles": [
        {
          "id": "https://status.notion.so/incidents/d38fkw0dxb29",
          "author": null,
          "description": "Oct 17, 11:59 PDT\nResolved - Notification health should be back to normal. Engineering will continue to monitor system health.\nOct 17, 11:40 PDT\nInvestigating - We are experiencing service degradation related to notifications. Notifications may be delayed, and notification badge counts may not be accurate.",
          "link": "https://status.notion.so/incidents/d38fkw0dxb29",
          "publishedOn": "2023-10-17T18:59:19.000Z",
          "wordCount": 3365,
          "title": "Notion is experiencing service degradation related to notifications",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/55frj2c42yvq",
          "author": null,
          "description": "Oct 12, 11:14 PDT\nResolved - Users were unable to log in using the Notion mobile and desktop apps to enterprise workspaces requiring SAML SSO login. This issue is resolved now.\nOct 12, 11:10 PDT\nUpdate - We are experiencing an issue with SAML SSO login in the Notion mobile and desktop apps, and we are investigating the cause.\nOct 12, 11:05 PDT\nInvestigating - We are experiencing an issue with SAML SSO and we are investigating the cause.",
          "link": "https://status.notion.so/incidents/55frj2c42yvq",
          "publishedOn": "2023-10-12T18:14:45.000Z",
          "wordCount": 3403,
          "title": "Notion is experiencing an issue with SAML SSO login",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Rippling Status - Incident History",
      "feedUrl": "https://status.rippling.com/history.rss",
      "siteUrl": "https://status.rippling.com",
      "articles": [
        {
          "id": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "author": null,
          "description": "Oct 27, 15:51 UTC\nResolved - The issue has now been resolved. All admins are able to access their Admin Account view as expected.\nOct 27, 14:50 UTC\nIdentified - We are aware of an issue where 'Admin Account' is not visible in the account dropdown for Super Admins. After they navigate to their 'Employee Account' view they can't navigate back to their admin view. \nThe root cause has been identified and the issue should be resolved in the next hour.",
          "link": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "publishedOn": "2023-10-27T15:51:44.000Z",
          "wordCount": 5077,
          "title": "Admin account not visible in the account dropdown for Super Admins",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "author": null,
          "description": "Oct 25, 21:10 UTC\nResolved - This incident has been resolved.\nOct 25, 21:02 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:52 UTC\nIdentified - An issue has been identified and we are working on a fix.\nOct 25, 20:46 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 25, 20:22 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:17 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 25, 20:14 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "publishedOn": "2023-10-25T21:10:01.000Z",
          "wordCount": 5089,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "author": null,
          "description": "Oct 24, 22:51 UTC\nResolved - This incident has been resolved.\nOct 24, 22:08 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 24, 22:07 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 24, 21:58 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 24, 21:57 UTC\nInvestigating - Customers are experiencing intermittent issues using single sign-on from Rippling (IdP-initiated SAML) to third-party applications. We are investigating this issue.",
          "link": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "publishedOn": "2023-10-24T22:51:30.000Z",
          "wordCount": 5090,
          "title": "Issues with single sign-on from Rippling to third-party applications",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        }
      ]
    },
    {
      "title": "Google Workspace Status Dashboard Updates",
      "feedUrl": "https://www.google.com/appsstatus/dashboard/en/feed.atom",
      "siteUrl": "https://www.google.com/appsstatus/dashboard/",
      "articles": [
        {
          "id": "https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM",
          "author": null,
          "description": "<p> Incident began at <strong>2023-09-28 07:00</strong> and ended at <strong>2023-10-04 18:22</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class=\"cBIRi14aVDP__status-update-text\"><h1>Mini Incident Report</h1>\n<p>We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced an impact outside of what is listed below, please reach out to Google Workspace Support using the help article <a href=\"https://support.google.com/a/answer/1047213\">https://support.google.com/a/answer/1047213</a>.</p>\n<p>(All Times US/Pacific)</p>\n<p><strong>Incident Start:</strong> 28 Sept 2023 00:00</p>\n<p><strong>Incident End:</strong> 4 Oct 2023 11:22</p>\n<p><strong>Duration:</strong>  6 days, 11 hours, 22 minutes</p>\n<p><strong>Affected Services and Features:</strong></p>\n<p>Google Keep - Email notifications</p>\n<p><strong>Regions/Zones:</strong> Global</p>\n<p><strong>Description:</strong></p>\n<p>Google Keep experienced an issue where notification emails about note sharing ended up in the receiver&#39;s Spam folder for a duration of 6 days, 11 hours and 22 minutes. From preliminary analysis, the root cause is an unexpected increase in notifications.</p>\n<p><strong>Customer Impact:</strong></p>\n<p>Google Keep users incorrectly received email notifications in their spam folder.</p>\n</div><hr><p>Affected products: Google Keep</p>",
          "link": "https://www.google.com/appsstatus/dashboard/incidents/BhdnzdJaE1T5JstM2CjM",
          "publishedOn": "2023-10-09T11:49:19.000Z",
          "wordCount": 656,
          "title": "RESOLVED: **Summary**\nGoogle Keep users are experiencing issues with notification emails when a note is shared.\n**Description:**\nWe are experiencing an issue with Google Keep beginning on Wednesday, 2023-09-27. Mitigation work is underway by our engineering team. We do not have an ETA for mitigation at this point. We will provide more information by Wednesday, 2023-10-04 10:00 US/Pacific.\n**Diagnosis**\nGoogle Keep users are experiencing an issue where notification emails about note sharing are ending up in receiver's Spam folder\n**Workaround**\nNone at this time",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "GitHub Status - Incident History",
      "feedUrl": "https://www.githubstatus.com/history.rss",
      "siteUrl": "https://www.githubstatus.com",
      "articles": [
        {
          "id": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "author": null,
          "description": "Nov  3, 19:21 UTC\nResolved - A performance and resilience optimization to the authorization microservice contained a memory leak that was exposed under high traffic. This resulted in a number of pages returning 404’s that should not have. Testing the build in our canary ring did not expose the service to sufficient traffic to discover the leak, allowing it to graduate to production at 6:37 PM UTC.  The memory leak under high load caused pods to crash repeatedly starting at 6:42 PM UTC, failing authorization checks. These failures triggered alerts at 6:44 PM UTC. Rolling back the authorization service change was delayed as parts of the deployment infrastructure relied on the authorization service and required manual intervention to complete. Rollback completed at 7:08 PM UTC and all impacte…",
          "link": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "publishedOn": "2023-11-03T19:21:48.000Z",
          "wordCount": 5723,
          "title": "Incident with Git Operations, Issues, Pull Requests, Actions, API Requests, Codespaces, Packages, Pages and Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "author": null,
          "description": "Oct 25, 22:15 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 21:37 UTC\nUpdate - The observed Copilot API error rate is around 5% of the requests. As a result, some of the Copilot code suggestions are skipped or not delivered on time.\nOct 25, 21:19 UTC\nUpdate - We are seeing an impact in the US region as well. We continue the investigation.\nOct 25, 20:53 UTC\nUpdate - Copilot is experiencing intermittent issues in our Japan region. Engineers are currently investigating.\nOct 25, 20:50 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "publishedOn": "2023-10-25T22:15:54.000Z",
          "wordCount": 5213,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "author": null,
          "description": "Oct 25, 13:02 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 12:56 UTC\nUpdate - We have applied a fix to help with Copilot performance. Initial signals show good recovery. We will continue to monitor for the time being and resolve when confident the issue has been resolved.\nOct 25, 12:29 UTC\nUpdate - We are investigating degraded performance in Europe for Copilot. We will continue to keep users updated on progress towards mitigation.\nOct 25, 12:10 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "publishedOn": "2023-10-25T13:02:51.000Z",
          "wordCount": 5202,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "author": null,
          "description": "Oct 22, 16:07 UTC\nResolved - This incident has been resolved.\nFrom 11:21 to 16:07 UTC some GitHub customers experienced errors cloning via workflows or via the command line.\nA third-party configuration change resulted in an unexpected behavior to our systems that resulted in Git clone failures. Once we detected the change we were able to disable it, and our systems started operating normally.\nWith the incident mitigated, we are working with our third-party provider to improve subsequent configuration change rollouts.\nOct 22, 15:58 UTC\nUpdate - We have mitigated the cause of the issue and are awaiting positive confirmation from impacted customers that the issue is fully resolved.\nOct 22, 15:34 UTC\nUpdate - We are currently investigating reports from some customers encountering errors when cloning repositories via workflows or via the command line. We do not currently have an ETA for resolution. Next update in 30 minutes.\nOct 22, 15:16 UTC\nInvestigating - We are investigating reports of degraded performance for Git Operations",
          "link": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "publishedOn": "2023-10-22T16:07:58.000Z",
          "wordCount": 5148,
          "title": "Incident with Git Operations",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/gkrfrz6r7flc",
          "author": null,
          "description": "Oct 17, 13:49 UTC\nResolved - From 10:59 UTC to 13:48 UTC, GitHub Codespaces service was down or degraded due to an outage in our authentication service.This issue impacted 67% of users over this time period.\nOur service auth layer experienced throttling with our third party dependencies due to higher load. This impacted all user facing scenarios for Codespaces service. Our automated regional failover kicked in, but it failed to mitigate as this issue impacted the service globally. We mitigated manually by reducing load on external dependency.\nWith the incident mitigated, we are working to assess and implement scaling improvements to make our service more resilient with increasing load.\nOct 17, 13:46 UTC\nUpdate - Codespaces is experiencing degraded performance. We are continuing to investigate.\nOct 17, 13:24 UTC\nUpdate - We are continuing with efforts to mitigate Codespaces issues   and are beginning to see some Codespace creations succeed.\nOct 17, 12:18 UTC\nUpdate - We have identified an issue impacting most Codespaces operations and are working on a mitigation.\nOct 17, 11:18 UTC\nUpdate - Codespaces is experiencing degraded availability. We are continuing to investigate.\nOct 17, 11:14 UTC\nInvestigating - We are investigating reports of degraded performance for Codespaces",
          "link": "https://www.githubstatus.com/incidents/gkrfrz6r7flc",
          "publishedOn": "2023-10-17T13:49:00.000Z",
          "wordCount": 5185,
          "title": "Incident with Codespaces",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/gtsz1l2jc96n",
          "author": null,
          "description": "Oct  9, 15:18 UTC\nResolved - On October 9, 2023 at 14:24 UTC, a noticeable delay in commits appearing in pull requests was detected.  During the incident, approximately 9% of pull requests (less than the 20% first reported) experienced staleness of up to 7m. The root cause was identified to be an increase in the latency of a downstream dependency causing pull request workers to saturate their available capacity, resulting in delayed updates to PRs - no data was lost during this incident.\n\tWe mitigated this by adding additional capacity to the affected worker pool at 15:02 UTC. This allowed our background jobs to catch up with the backlog of updates and provide relief to our customers. Additionally, we have significantly increased the performance of the downstream service to prevent recurrence\nOct  9, 14:52 UTC\nUpdate - We are investigating delays for commits showing up on Pull Requests page loads in the web UI. As a result of this,  about 20% of pull requests are currently showing stale data of up-to 7m. We are currently investigating contributing factors right now.\nOct  9, 14:51 UTC\nInvestigating - We are investigating reports of degraded performance for Pull Requests",
          "link": "https://www.githubstatus.com/incidents/gtsz1l2jc96n",
          "publishedOn": "2023-10-09T15:18:49.000Z",
          "wordCount": 5180,
          "title": "Incident with Pull Requests",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        }
      ]
    },
    {
      "title": "Slack System Status",
      "feedUrl": "https://status.slack.com/feed/rss",
      "siteUrl": "https://status.slack.com/",
      "articles": [
        {
          "id": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "author": null,
          "description": "We resolved the issue users experienced when trying to connect to Slack or send messages. We appreciate your patience while we sorted this out and apologize for any disruption to your day.",
          "link": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "publishedOn": "2023-11-04T01:27:00.000Z",
          "wordCount": 112,
          "title": "Incident: Users unable to connect to Slack or send messages",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/8c886b57284762b6",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 4:00PM PDT on October 31, 2023 to around 10:00PM PDT, customers with international data residency in the Paris, France region, experienced issues connecting to Slack and sending messages.\n\r\n\r\nA routine credential rotation caused database sync issues for the Paris, France data residency region. We reverted this code change and restarted the affected databases, resolving the issue for all impacted customers. \n\r\n\r\nOnce we had mitigated the immediate impact and restored connectivity for customers in the Paris, France data residency region, we reviewed the credential rotation for all other data residency regions to ensure the same issue would not occur anywhere else.",
          "link": "https://status.slack.com//2023-10/8c886b57284762b6",
          "publishedOn": "2023-11-02T03:45:50.000Z",
          "wordCount": 391,
          "title": "Outage: Issues for customers enrolled in the French data region",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "author": null,
          "description": "Customers should no longer be experiencing any connection issues with Slack. Apologies for the trouble today and thank you for your patience.",
          "link": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "publishedOn": "2023-10-24T21:08:02.000Z",
          "wordCount": 195,
          "title": "Incident: A small number of users are having problems loading Slack.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/ad8f0e62516e8812",
          "author": null,
          "description": "Issue summary:\n\r\n\r\nOn Friday, October 6 2023, from 1:52 AM PDT to 2:12 AM PDT, some users were unable to load Slack. This was caused by an issue where certain backend Slack processes were pulling data from our database rather than their cache which caused strain on our servers.\n\r\n\r\nWe rolled out a change to how these processes load data, and the issue was resolved for all users.",
          "link": "https://status.slack.com//2023-10/ad8f0e62516e8812",
          "publishedOn": "2023-10-06T15:28:47.000Z",
          "wordCount": 232,
          "title": "Outage: Users are having trouble connecting to Slack",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        }
      ]
    },
    {
      "title": "Make Status - Incident History",
      "feedUrl": "https://status.make.com/history.rss",
      "siteUrl": "https://status.make.com",
      "articles": [
        {
          "id": "https://status.make.com/incidents/2c52p236t4rp",
          "author": null,
          "description": "Oct 12, 17:40 CEST\nResolved - This incident has been resolved.\nOct 12, 16:21 CEST\nMonitoring - We implemented workaround and we are currently monitoring the issue.\nOct 12, 16:09 CEST\nInvestigating - We are experiencing issues within our scenario features i.e. users cannot clone scenario nor move it to a folder inside list of scenarios. This is still possible when user open particular scenario.\nWe are currently working on implementing a quick workaround, we should publish the relevant steps within the next 30 minutes.",
          "link": "https://status.make.com/incidents/2c52p236t4rp",
          "publishedOn": "2023-10-12T15:40:47.000Z",
          "wordCount": 3903,
          "title": "Users cannot clone nor move scenarios",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}