{
  "sources": [
    {
      "title": "BookingSync.com news",
      "feedUrl": "https://changelog.bookingsync.com/rss",
      "siteUrl": "https://changelog.bookingsync.com",
      "articles": [
        {
          "id": "276591",
          "author": "Maud , Partnership Manager",
          "description": "New!\nÂ Â \nWe are delighted to announce our new partnership with Plum Guide. ğŸ¥³\nPlum Guide is a curated online travel agency that showcases the worldâ€™s best homes - they're like the Michelin Guide, but for homes.\nThey do this by independently vetting every home and property manager in each market and selecting only the best to feature in our collection.\nThey were recently named â€˜Best OTAâ€™ as the 2023 Shortyz awards. ğŸ†\n\n\n\n\n\nğŸ’¡ How can Plum Guide help you?\nIncremental revenue thanks to unique guests\nEditorial style listings built by their Merchandising team\nDedicated functions for individual Hosts & Property Management Companies\n\n\n\nğŸ¤© Benefits:\n2.1x longer stays than on Airbnb\nOver 50% of bookings occur outside of high season\nDedicated AM, Sales & Onboarding teams for both individual Hosts and Property Management Companies\nMajority of Plum Guide Guests have never booked on other OTAs\nHigh Quality Guests (avg. age is 45)\nGuest Cancellation Rate is <0.5%\n\n\n\nInterested in advertising your properties on Plum Guide?\nğŸ‘‰ Install the app here\n\ndedicated manual page or book a call with your Account Manager here.  \nWe hope that this new channel will help you get more bookings from travelers around the world!\nDonâ€™t hesitate to contact the Plum Guide team supply@plumguide.com or our support team for any questions.\n\n\nBest regards,",
          "link": "https://changelog.bookingsync.com/new-plum-guide-is-now-available-on-smily!-276591",
          "publishedOn": "2023-10-12T16:07:05.000Z",
          "wordCount": 464,
          "title": "NEW - Plum Guide is now available on Smily! ğŸ‰",
          "imageUrl": "https://cloud.headwayapp.co/changelogs_images/images/big/000/116/376-84bdc4de7ba351ee521a89528433b6b6d5876cfd.png"
        }
      ]
    },
    {
      "title": "Security Bulletins on Tailscale",
      "feedUrl": "https://tailscale.com/security-bulletins/index.xml",
      "siteUrl": "https://tailscale.com/security-bulletins/",
      "articles": [
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "author": null,
          "description": "Description: Privilege escalation bugs in the Tailscale\nKubernetes operatorâ€™s API proxy allowed authenticated tailnet clients\nto send Kubernetes API requests as the operatorâ€™s service account.\nTailscale Kubernetes operator version v1.53.37 fixes the issue and\nusers of the operator who enable the API proxy functionality should\nupdate as described below.\nWhat happened?\nThe Tailscale Kubernetes operator can optionally act as an API server\nproxy\nfor the clusterâ€™s Kubernetes API. This proxy allows authenticated\ntailnet users to use their tailnet identity in Kubernetes\nauthentication and RBAC rules. The API server proxy uses\nimpersonation\nheaders\nto translate tailnet identities to Kubernetes identities.\nThe operator prior to v1.53.37 has two bugs in the forwarding logic,\nwhich affects different â€¦",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-008/",
          "publishedOn": "2023-11-01T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-008",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        },
        {
          "id": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "author": null,
          "description": "Description: Microsoft Defender is flagging Tailscale 1.46.1 as malware.\nThese classifications are false positives, and we are working with Microsoft to\nresolve the situation.\nAs of 2023-10-27 1:05 AM UTC, we have confirmed that Microsoft have addressed\nthe false positive, meaning Defender no longer flags Tailscale 1.46.1 as\nmalware. A rescan of tailscaled.exe 1.46.1 on VirusTotal confirms this.\nWhat happened?\nMicrosoft Defender was flagging Tailscale 1.46.1 as malware. This caused\nDefender to quarantine the binaries, meaning they could not run.\nWe submitted Tailscale 1.46.1 to Microsoft to investigate the false positive,\nwho then updated Defender to avoid flagging this release as malware at\n2023-10-27 1:05 AM UTC.\nWho is affected?\nPeople using Defender and Tailscale 1.46.1.\nWhat is the impact?\nTailscale will not run on affected machines.\nWhat do I need to do?\nTo resolve this issue on your own tailnet, you can take either or both of 2\napproaches:\nUpdate to a newer version of Tailscale. Newer versions are not affected by this problem.\nCreate an exception in Microsoft Defender. Microsoft has published instructions explaining how to do this.\nUpdate Microsoft Defender.",
          "link": "https://tailscale.com/security-bulletins/#ts-2023-007/",
          "publishedOn": "2023-10-26T00:00:00.000Z",
          "wordCount": 4677,
          "title": "TS-2023-007",
          "imageUrl": "https://tailscale.com/files/images/og-image.png"
        }
      ]
    },
    {
      "title": "Airbnb API Status - Incident History",
      "feedUrl": "https://airbnbapi.statuspage.io/history.rss",
      "siteUrl": "https://airbnbapi.statuspage.io",
      "articles": [
        {
          "id": "https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt",
          "author": null,
          "description": "Nov  9, 14:49 PST\nResolved - This incident has been resolved.\nNov  8, 10:38 PST\nIdentified - We have observed a higher number of intermittent 500 errors on the GET Resolutions API. We are actively working on resolving this issue and plan to deploy a fix tomorrow (Nov 9, 2023). Thank you for your understanding and patience as we work to rectify this situation.",
          "link": "https://airbnbapi.statuspage.io/incidents/ztkbs9kly4mt",
          "publishedOn": "2023-11-09T22:49:38.000Z",
          "wordCount": 3883,
          "title": "Intermittent 500 errors on the GET Resolutions API",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "author": null,
          "description": "Oct 31, 10:00 PDT\nResolved - This incident has been resolved.\nOct 30, 18:14 PDT\nMonitoring - Today (Oct 30, 2023) between 1:00 PM and 3:00 PM PDT we encountered an incident that impacted our webhooks functionality. As a result, some reservations may have been missing webhooks during this time period. The issue has been resolved, and our team is actively monitoring the results to ensure the stability of our systems. If you have noticed any reservations missing webhooks, we recommend using the GET Reservations API to retrieve the details of those reservations. Alternatively, you can contact us with a list of affected reservations, and we will assist you in backfilling the missing information.\nWe apologize for any inconvenience this may have caused and appreciate your understanding.",
          "link": "https://airbnbapi.statuspage.io/incidents/875yz70rnb1p",
          "publishedOn": "2023-10-31T17:00:19.000Z",
          "wordCount": 3930,
          "title": "Issue Affecting Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Twilio Status - Incident History",
      "feedUrl": "https://status.twilio.com/history.rss",
      "siteUrl": "https://status.twilio.com",
      "articles": [
        {
          "id": "https://status.twilio.com/incidents/b7x07g9vfx6m",
          "author": null,
          "description": "THIS IS A SCHEDULED EVENT Nov 10, 21:00 - 22:00 PST\nNov  2, 08:49 PDT\nScheduled - Our SMS carrier partner in Spain is conducting a planned maintenance from 10 November 2023 at 21:00 PST until 10 November 2023 at 22:00 PST. During the maintenance window, there could be intermittent delays delivering SMS to and from Spain handsets via a subset of long codes.",
          "link": "https://status.twilio.com/incidents/b7x07g9vfx6m",
          "publishedOn": "2023-11-11T05:00:00.000Z",
          "wordCount": 7245,
          "title": "Spain SMS Carrier Partner Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "DigitalOcean Status - Incident History",
      "feedUrl": "https://status.digitalocean.com/history.rss",
      "siteUrl": "http://status.digitalocean.com",
      "articles": [
        {
          "id": "https://status.digitalocean.com/incidents/hxcw6mrw1ywv",
          "author": null,
          "description": "Nov  8, 18:07 UTC\nResolved - Our Engineering team has seen no recurrences and performance has remained stable since 16:40 UTC. This incident is fully resolved.\nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience!\nNov  8, 17:23 UTC\nUpdate - Our Engineering team identified a configuration that was responsible for the recurrence we saw. From 16:20 - 16:40 UTC, connectivity between SFO2 and the rest of DigitalOcean's network was impacted.\nAs of 16:40 UTC, all impact has subsided and users should no longer face any issues.\nWe are monitoring the situation closely and will share an update once the issue is completely resolved.\nNov  8, 16:32 UTC\nUpdate - Our Engineering team is seeing a recurrence of network alerts that indicate we're expâ€¦",
          "link": "https://status.digitalocean.com/incidents/hxcw6mrw1ywv",
          "publishedOn": "2023-11-08T18:07:20.000Z",
          "wordCount": 6263,
          "title": "Networking Connectivity Between NYC and SFO",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "author": null,
          "description": "Nov  4, 08:18 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting App Platform Deployments. \nFrom 11:54 on Nov 2nd to 06:42 on Nov 4th UTC, App Platform users may have experienced delays when deploying new apps or when deploying updates to existing Apps. Our Upstream provider and the Engineering team closely worked together to resolve the issue. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel. Thank you for your patience and we apologize for any inconvenience.\nNov  4, 06:53 UTC\nUpdate - As per the recent update from our Upstream provider, they fully recovered the services usedâ€¦",
          "link": "https://status.digitalocean.com/incidents/bw3r3j9b5ph5",
          "publishedOn": "2023-11-04T08:18:05.000Z",
          "wordCount": 6442,
          "title": "App Platform Deployments",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "author": null,
          "description": "Nov  3, 23:14 UTC\nResolved - Our Engineering team has confirmed the full resolution of issue impacting network connectivity in our SFO regions. \nUsers should no longer experience any latency or timeout issue with any of the Droplet based services. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nNov  3, 20:58 UTC\nMonitoring - As of 19:55 UTC, our Engineering team has confirmed that a fix has been implemented by our upstream carrier to mitigate the cause of the issue impacting network connectivity in our SFO region. \nWe are closely monitoring the situation and will update as soon as we have more information from the provider.\nNov  3, 19:43 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting network connectivity in our SFO region. Upstream congestion with a network provider between Los Angeles and Dallas is impacting traffic traversing out of our SFO datacentres.\nA case has been opened by our team with the provider. Our team has attempted to shift traffic to improve the situation, but unfortunately, we continue to see approximately 10% of customer traffic impacted by this issue.\nOur team is working on an option to shift to an alternate provider if this issue is not able to be resolved by the provider in a timely manner. We will share another update once we have further information from the provider or we have an update from our Engineering team.\nNov  3, 19:27 UTC\nInvestigating - As of 17:40 UTC, our Engineering team is investigating an issue impacting networking in the SFO regions. During this time, a subset of users may experience packet loss/latency and timeouts with Droplet based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nz33vs0sjhqy",
          "publishedOn": "2023-11-03T23:14:12.000Z",
          "wordCount": 6235,
          "title": "Networking in SFO Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "author": null,
          "description": "Nov  2, 11:50 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting the Cloud Control Panel, API, and multiple services.\nFrom 05:05 - 08:40 UTC, users may have encountered errors with the Cloud Control Panel and public API while attempting to create new user registrations, or while making payments. Users also may have experienced issues with processing Droplet and Managed Kubernetes cluster creations along with Droplet-based events and experienced latencies while accessing our Cloud Control Panel along with the DigitalOcean Container Registry. Our Upstream provider and the Engineering team closely worked together to resolve the issues. \nThe impact has been completely subsided and users should no longer see any issues with the impacted services.\nIf youâ€¦",
          "link": "https://status.digitalocean.com/incidents/y3q3gh220jyj",
          "publishedOn": "2023-11-02T11:50:30.000Z",
          "wordCount": 6571,
          "title": "Multiple services down and API availability",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "author": null,
          "description": "Oct 27, 14:47 UTC\nResolved - As of 14:30 UTC, our Engineering team has resolved the issue impacting Spaces in our SGP1 and SFO3 regions. Users should no longer experience slowness or timeouts when trying to create or access their Spaces resources in the SGP1 and SFO3 regions. \nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nOct 27, 14:03 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue impacting Spaces in our SGP1 and SFO3 regions and is monitoring the situation closely. We will post an update as soon as the issue is fully resolved.\nOct 27, 12:04 UTC\nUpdate - Our Engineering team is continuing to investigate an issue impacting Object Storage in our SGP1 region. Additionally, we have become aware that this issue has also impacted Object Storage in the SFO3 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 27, 11:25 UTC\nInvestigating - As of 10:22 UTC, our Engineering team is investigating an issue with Object Storage in our SGP1 region. During this time, users may encounter difficulties accessing Spaces, creating new buckets, and uploading files to and from Spaces buckets. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/nwm1sn0gjfc3",
          "publishedOn": "2023-10-27T14:47:58.000Z",
          "wordCount": 6209,
          "title": "Object Storage - SGP1 and SFO3",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "author": null,
          "description": "Oct 26, 15:52 UTC\nResolved - As of 15:45 UTC, our Engineering team has confirmed the full resolution of the issue that impacted network reachability in the LON1 region. All services and resources should now be fully reachable.\nIf you continue to experience problems, please open a ticket with our support team from within your Cloud Control Panel. \nThank you for your patience and we apologize for any inconvenience.\nOct 26, 15:18 UTC\nMonitoring - The network issues affecting our LON1 region have been mitigated. Users should no longer experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will continue to monitor network conditions for a period of time to establish a return to pre-incident conditions.\nOct 26, 14:25 UTC\nIdentified - Our Engineering team has identified the cause of the issue impacting the networking in the LON1 region and is actively working on a fix. During this time, users may still experience packet loss/latency, timeouts, and related issues with Droplet-based services in these regions, including Droplets, Managed Kubernetes, and Managed Database. \nWe will post an update as soon as additional information is available.\nOct 26, 12:50 UTC\nInvestigating - As of 11:40 UTC, our Engineering team is investigating an issue impacting the networking in the LON1 region. During this time, a subset of users may experience packet loss/latency, timeouts, and related issues with Droplet-based services in this region, including Droplets, Managed Kubernetes, and Managed Database. \nWe will share an update once we have further information.",
          "link": "https://status.digitalocean.com/incidents/1zckjwhpq1xy",
          "publishedOn": "2023-10-26T15:52:41.000Z",
          "wordCount": 6179,
          "title": "Network Connectivity in LON1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "author": null,
          "description": "Oct 24, 23:00 UTC\nCompleted - The scheduled maintenance has been completed.\nOct 24, 21:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nOct 20, 19:40 UTC\nUpdate - After a thorough review by the team performing this maintenance, we have determined that our initial messaging does not convey the complete scope and potential impact of this event. Existing infrastructure will continue running without issue during this maintenance window. However, users may experience increased latency with some platform operations, including: \nCloud Control Panel and API operations\nEvent processing\nDroplet creates, resizes, rebuilds, and power events\nManaged Kubernetes reconciliation and scaling\nLoad Balancer operations\nContainer Registry operations\nApp â€¦",
          "link": "https://status.digitalocean.com/incidents/y5rwfhsl0gqw",
          "publishedOn": "2023-10-24T23:00:07.000Z",
          "wordCount": 6222,
          "title": "Core Infrastructure Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "author": null,
          "description": "Oct 24, 09:15 UTC\nResolved - Our Engineering team has confirmed the full resolution of the issue impacting network connectivity in multiple regions. The impact has been completely subsided and the network connectivity is back to normal for all the impacted services.\nIf you continue to experience problems, please open a ticket with our support team from your Cloud Control Panel.\nThank you for your patience and we apologize for any inconvenience.\nOct 24, 09:03 UTC\nMonitoring - Our Engineering team has received communication from the upstream provider that a fix to resolve the networking issue has been implemented. We are currently monitoring the situation closely and will share an update as soon as the issue is fully resolved.\nOct 24, 07:41 UTC\nIdentified - Our Engineering team has identified the cause of issues impacting networking in multiple regions. The issues are a direct result of traffic congestion from our upstream providers, which is in the process of being repaired.\nAt this time, a subset of users will continue to experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nWe apologize for the inconvenience and will share an update once we have more information.\nOct 24, 06:11 UTC\nInvestigating - As of 05:30 UTC, our Engineering team is investigating an issue impacting the networking in multiple regions. During this time, users may experience intermittent packet loss or increased latency while interacting with the resources in the affected regions.\nAt the moment, all the droplet-based services appear to be impacted and the users can expect to see brief connectivity issues and interrupted traffic flows. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/gp9bzm1hnlk4",
          "publishedOn": "2023-10-24T09:15:10.000Z",
          "wordCount": 6208,
          "title": "Networking in multiple regions.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/f38zfs58jkl8",
          "author": null,
          "description": "Oct 17, 17:33 UTC\nResolved - As of 16:10 UTC, our Engineering team has confirmed the full resolution of the issue impacting our App platform service where users were not able to access their Apps via Cloud Panel. Apps should be loading fine https://cloud.digitalocean.com/apps without any errors. \nWe appreciate your patience during the process and if you continue to experience any issues  please open a ticket with our support team.\nOct 17, 16:51 UTC\nMonitoring - Our Engineering team has deployed a fix to resolve the ongoing issue with accessing Apps list via Cloud panel for our App platform service. As of 16:10 UTC the situation started to improve and the users should be able to access their apps via Cloud panel UI without any issues. \nWe are monitoring the situation closely and will post an update once the issue is completely resolved.\nOct 17, 16:19 UTC\nIdentified - Our Engineering team has identified an issue impacting App platform service in all regions. During this time users may experience issues while loading apps via https://cloud.digitalocean.com/apps and the requests appear to be timing out. New App deployments via API or doctl and the existing deployed apps are not impacted by this incident.  \nOur team is working to mitigate the issue and we will provide an update as soon as possible.",
          "link": "https://status.digitalocean.com/incidents/f38zfs58jkl8",
          "publishedOn": "2023-10-17T17:33:44.000Z",
          "wordCount": 6143,
          "title": "App Platform Accessibility via Cloud",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/7548dwrnthz8",
          "author": null,
          "description": "Oct 17, 06:48 UTC\nResolved - As of 06:25 UTC, our Engineering team has confirmed that the issue impacting the newly created Managed Database Clusters has been fully resolved. \nUsers will no longer experience issues with the newly created Managed Database Cluster.\nIf you continue to experience any issues with Managed Database Clusters please open a ticket with our support team. Thank you for your patience.\nOct 17, 06:24 UTC\nMonitoring - As of 06:15 UTC, our Engineering team has identified the issue impacting our Managed Database product and a fix has been implemented to mitigate the issue. Currently, new users may no longer experience issues with the newly created Managed Database Cluster due to hostname resolution.\nUsers who already have a Managed Database cluster are not impacted by this incident.\nThank you for your patience and we apologize for the inconvenience caused. We are monitoring the situation and will post an update once the incident is completely resolved.\nOct 17, 04:22 UTC\nInvestigating - Our Engineering team is currently investigating reports of issues impacting a subset of users using our Managed Database Clusters. \nDuring this time, hostnames for the newly created Managed Database Cluster are not resolving. However, previously created Clusters are not impacted due to the same.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/7548dwrnthz8",
          "publishedOn": "2023-10-17T06:48:06.000Z",
          "wordCount": 6149,
          "title": "Managed Database Cluster",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/yxfmyjl0kmpr",
          "author": null,
          "description": "Oct 16, 23:14 UTC\nResolved - From 22:45 to 23:00 UTC, our Engineering team has reported an issue with DigitalOcean Control Panel and API.\n During that time, Customers may have experienced intermittent timeout errors while using DigitalOcean Control Panel and API. \nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.",
          "link": "https://status.digitalocean.com/incidents/yxfmyjl0kmpr",
          "publishedOn": "2023-10-16T23:14:32.000Z",
          "wordCount": 5980,
          "title": "DigitalOcean Control Panel and API",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/k58m2k4qcchf",
          "author": null,
          "description": "Oct 15, 01:04 UTC\nResolved - Our Engineering team has confirmed the full resolution of the networking issue in the SFO regions. If you continue to see issues with latency or packet loss in the SFO region please reach out directly to our support team for assistance.\nOct 14, 20:30 UTC\nUpdate - Our Engineering team is continuing to monitor the networking issue in the SFO regions. So far, we haven't observed any major spike in latency or packet loss with network connections going in or out of the SFO regions. However, we will post an update as soon as the issue is fully resolved.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 14, 15:25 UTC\nMonitoring - Our Engineering team has detected a recurrence of the networking issue identified in a previous incident today: \nhttps://status.digitalocean.com/incidents/bhpslzd37517\nOur team is actively monitoring the situation and has implemented traffic routing changes where applicable to alleviate the latency. Some users may still experience packet loss or increased latency accessing the resources in the SFO region from certain ISPs.\nWe apologize for any inconvenience caused and will provide updates as the situation progresses.",
          "link": "https://status.digitalocean.com/incidents/k58m2k4qcchf",
          "publishedOn": "2023-10-15T01:04:23.000Z",
          "wordCount": 6118,
          "title": "Network Latency in SFO Region",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/bhpslzd37517",
          "author": null,
          "description": "Oct 14, 11:46 UTC\nResolved - Our Engineering team has confirmed full resolution of the issue with networking in affected regions. Users should no longer experience timeouts or delays when connecting to or from these regions. \nIf you continue to experience problems, please open a ticket with our support team.\nThank you for your patience and we apologize for any inconvenience.\nOct 14, 11:10 UTC\nMonitoring - As of 10:55 UTC, our Engineering team has implemented a fix to address the networking problem in the affected regions and is currently monitoring the situation. Users should no longer face timeouts or encounter delays when connecting to or from these regions. We will post an update as soon as the issue is fully resolved.\nOct 14, 10:30 UTC\nUpdate - As of 10:27 UTC, our Engineering team is continuing to investigate an issue with networking in our SFO regions. Additionally, it has come to our attention that this issue has affected other regions, specifically SGP1, SYD1, and NYC3. Users may encounter timeouts or experience delays in network connections going in and out of these regions. Our Engineers are actively working on isolating the root cause of the issue. While we don't have an exact timeframe for a resolution yet however we will be providing updates as developments occur.\nWe apologize for the inconvenience and thank you for your patience and continued support.\nOct 14, 09:47 UTC\nUpdate - We are continuing to investigate this issue.\nOct 14, 09:12 UTC\nInvestigating - As of 8:30 UTC, our Engineering team is investigating an issue with networking in our SFO regions. During this time, users may experience timeouts or latency with network connections going in or out of the SFO regions. We apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/bhpslzd37517",
          "publishedOn": "2023-10-14T11:46:50.000Z",
          "wordCount": 6231,
          "title": "Network Latency in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/lh3j71zp57hx",
          "author": null,
          "description": "Oct 12, 19:17 UTC\nResolved - Our Engineering team has confirmed the resolution of the issue that impacted the Spaces CDN. Objects should be accessible over the CDN endpoint without any issues. However, HTTP/2 is temporarily unavailable due to upstream issues, and due to this HTTP/2 requests should automatically be re-negotiated to 1.1, but in case you experience failures please open a ticket with our support team.\nThank you for your patience and we apologize for any inconvenience.\nOct 12, 16:10 UTC\nUpdate - After our upstream provider implemented a remediation step to resolve the issue with the Spaces CDN, the Spaces CDN is serving the objects stored in the Spaces bucket without any errors or performance issues. However, we are still monitoring the situation closely and we'll share more inâ€¦",
          "link": "https://status.digitalocean.com/incidents/lh3j71zp57hx",
          "publishedOn": "2023-10-12T19:17:29.000Z",
          "wordCount": 6454,
          "title": "Spaces CDN in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Notion Status - Incident History",
      "feedUrl": "https://status.notion.so/history.rss",
      "siteUrl": "https://status.notion.so",
      "articles": [
        {
          "id": "https://status.notion.so/incidents/n1sp5244k09v",
          "author": null,
          "description": "Nov  8, 07:28 PST\nResolved - The incident has been resolved. Time of resolution Nov 8 2023 7:28AM PST\nNov  8, 07:28 PST\nMonitoring - Our AI provider has implemented a fix and we are seeing Notion AI recover gradually since 7:28AM PST. We are currently monitoring the situation.\nNov  8, 06:57 PST\nUpdate - The issue has been identified now and a fix is being worked on for this issue.\nNov  8, 06:29 PST\nIdentified - One of our AI providers is down. We are working with them on a fix.",
          "link": "https://status.notion.so/incidents/n1sp5244k09v",
          "publishedOn": "2023-11-08T15:28:28.000Z",
          "wordCount": 3403,
          "title": "Notion AI is down",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/d38fkw0dxb29",
          "author": null,
          "description": "Oct 17, 11:59 PDT\nResolved - Notification health should be back to normal. Engineering will continue to monitor system health.\nOct 17, 11:40 PDT\nInvestigating - We are experiencing service degradation related to notifications. Notifications may be delayed, and notification badge counts may not be accurate.",
          "link": "https://status.notion.so/incidents/d38fkw0dxb29",
          "publishedOn": "2023-10-17T18:59:19.000Z",
          "wordCount": 3365,
          "title": "Notion is experiencing service degradation related to notifications",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.notion.so/incidents/55frj2c42yvq",
          "author": null,
          "description": "Oct 12, 11:14 PDT\nResolved - Users were unable to log in using the Notion mobile and desktop apps to enterprise workspaces requiring SAML SSO login. This issue is resolved now.\nOct 12, 11:10 PDT\nUpdate - We are experiencing an issue with SAML SSO login in the Notion mobile and desktop apps, and we are investigating the cause.\nOct 12, 11:05 PDT\nInvestigating - We are experiencing an issue with SAML SSO and we are investigating the cause.",
          "link": "https://status.notion.so/incidents/55frj2c42yvq",
          "publishedOn": "2023-10-12T18:14:45.000Z",
          "wordCount": 3403,
          "title": "Notion is experiencing an issue with SAML SSO login",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Rippling Status - Incident History",
      "feedUrl": "https://status.rippling.com/history.rss",
      "siteUrl": "https://status.rippling.com",
      "articles": [
        {
          "id": "https://status.rippling.com/incidents/54s1f3rs3n56",
          "author": null,
          "description": "Nov 10, 23:59 UTC\nResolved - This incident has been resolved.\nNov  9, 19:56 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nNov  9, 18:41 UTC\nUpdate - We are continuing to investigate this issue.\nNov  9, 18:39 UTC\nUpdate - We are continuing to investigate this issue.\nNov  9, 18:34 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/54s1f3rs3n56",
          "publishedOn": "2023-11-10T23:59:07.000Z",
          "wordCount": 5044,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/w8r2ldkc0rcj",
          "author": null,
          "description": "Nov  7, 07:51 UTC\nResolved - This incident has been fully resolved.\nNov  6, 19:08 UTC\nMonitoring - The Rippling app has mostly recovered but there are still a few lags in performance that we're further monitoring and investigating.\nNov  6, 18:33 UTC\nInvestigating - We are continuing to see degraded performance in the Rippling app, so we are continuing to investigate.\nNov  6, 17:28 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nNov  6, 17:25 UTC\nInvestigating - There are issues loading Rippling. We are working on a fix.",
          "link": "https://status.rippling.com/incidents/w8r2ldkc0rcj",
          "publishedOn": "2023-11-07T07:51:01.000Z",
          "wordCount": 5074,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "author": null,
          "description": "Oct 27, 15:51 UTC\nResolved - The issue has now been resolved. All admins are able to access their Admin Account view as expected.\nOct 27, 14:50 UTC\nIdentified - We are aware of an issue where 'Admin Account' is not visible in the account dropdown for Super Admins. After they navigate to their 'Employee Account' view they can't navigate back to their admin view. \nThe root cause has been identified and the issue should be resolved in the next hour.",
          "link": "https://status.rippling.com/incidents/fjw9lbztvc7f",
          "publishedOn": "2023-10-27T15:51:44.000Z",
          "wordCount": 5077,
          "title": "Admin account not visible in the account dropdown for Super Admins",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "author": null,
          "description": "Oct 25, 21:10 UTC\nResolved - This incident has been resolved.\nOct 25, 21:02 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:52 UTC\nIdentified - An issue has been identified and we are working on a fix.\nOct 25, 20:46 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 25, 20:22 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 25, 20:17 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 25, 20:14 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/vnm5lc6f0b91",
          "publishedOn": "2023-10-25T21:10:01.000Z",
          "wordCount": 5089,
          "title": "Issues loading Rippling",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "author": null,
          "description": "Oct 24, 22:51 UTC\nResolved - This incident has been resolved.\nOct 24, 22:08 UTC\nUpdate - We are continuing to monitor for any further issues.\nOct 24, 22:07 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nOct 24, 21:58 UTC\nIdentified - The issue has been identified and a fix is being implemented.\nOct 24, 21:57 UTC\nInvestigating - Customers are experiencing intermittent issues using single sign-on from Rippling (IdP-initiated SAML) to third-party applications. We are investigating this issue.",
          "link": "https://status.rippling.com/incidents/vlnmlxcw85r8",
          "publishedOn": "2023-10-24T22:51:30.000Z",
          "wordCount": 5090,
          "title": "Issues with single sign-on from Rippling to third-party applications",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        }
      ]
    },
    {
      "title": "Google Workspace Status Dashboard Updates",
      "feedUrl": "https://www.google.com/appsstatus/dashboard/en/feed.atom",
      "siteUrl": "https://www.google.com/appsstatus/dashboard/",
      "articles": []
    },
    {
      "title": "GitHub Status - Incident History",
      "feedUrl": "https://www.githubstatus.com/history.rss",
      "siteUrl": "https://www.githubstatus.com",
      "articles": [
        {
          "id": "https://www.githubstatus.com/incidents/m61vxgn4kvh2",
          "author": null,
          "description": "Nov  7, 14:25 UTC\nResolved - This incident has been resolved.\nNov  7, 14:25 UTC\nUpdate - API Requests is operating normally.\nNov  7, 14:15 UTC\nUpdate - Response times stabilized back to normal at 13:58 UTC.  We are continuing to monitor the slow dependency to ensure it's stable before resolving this incident.\nNov  7, 14:03 UTC\nUpdate - We're seeing intermittent spikes in latency of API requests and page loads.  We are investigating but do not have an ETA at this time.\nNov  7, 13:50 UTC\nUpdate - We are investigating reports of issues with service(s): Issues, API Requests. We will continue to keep users updated on progress towards mitigation.\nNov  7, 13:46 UTC\nUpdate - Issues is experiencing degraded performance. We are continuing to investigate.\nNov  7, 13:44 UTC\nInvestigating - We are investigating reports of degraded performance for API Requests",
          "link": "https://www.githubstatus.com/incidents/m61vxgn4kvh2",
          "publishedOn": "2023-11-07T14:25:40.000Z",
          "wordCount": 5143,
          "title": "Incident with API Requests and Issues",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "author": null,
          "description": "Nov  3, 19:21 UTC\nResolved - A performance and resilience optimization to the authorization microservice contained a memory leak that was exposed under high traffic. This resulted in a number of pages returning 404â€™s that should not have. Testing the build in our canary ring did not expose the service to sufficient traffic to discover the leak, allowing it to graduate to production at 6:37 PM UTC.  The memory leak under high load caused pods to crash repeatedly starting at 6:42 PM UTC, failing authorization checks. These failures triggered alerts at 6:44 PM UTC. Rolling back the authorization service change was delayed as parts of the deployment infrastructure relied on the authorization service and required manual intervention to complete. Rollback completed at 7:08 PM UTC and all impacteâ€¦",
          "link": "https://www.githubstatus.com/incidents/xb30mby9fs5x",
          "publishedOn": "2023-11-03T19:21:48.000Z",
          "wordCount": 5723,
          "title": "Incident with Git Operations, Issues, Pull Requests, Actions, API Requests, Codespaces, Packages, Pages and Webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "author": null,
          "description": "Oct 25, 22:15 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 21:37 UTC\nUpdate - The observed Copilot API error rate is around 5% of the requests. As a result, some of the Copilot code suggestions are skipped or not delivered on time.\nOct 25, 21:19 UTC\nUpdate - We are seeing an impact in the US region as well. We continue the investigation.\nOct 25, 20:53 UTC\nUpdate - Copilot is experiencing intermittent issues in our Japan region. Engineers are currently investigating.\nOct 25, 20:50 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/sbjdwn6mvht7",
          "publishedOn": "2023-10-25T22:15:54.000Z",
          "wordCount": 5213,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "author": null,
          "description": "Oct 25, 13:02 UTC\nResolved - Copilot completions are currently hosted in 4 regions globally: Central US, France, Switzerland and Japan. Users are typically routed to the nearest geographic region, but may be routed to other regions when the nearest region is unhealthy.\nBeginning at 2023-10-25 09:13 UTC, Copilot began experiencing outages of individual regions, lasting 12 minutes per region. These outages were due to the nodes hosting the completion model getting unhealthy due to a recent upgrade. There were intermittent outages in multiple regions with a subset of Copilot users experiencing completion errors. The outages were partial and varied across the different regions.\nIn order to prevent similar incidents from occurring in the future, we are focusing on improving our global load balancing of completion traffic during regional failures, in addition to determining and preventing the root cause of these outages.\nOct 25, 12:56 UTC\nUpdate - We have applied a fix to help with Copilot performance. Initial signals show good recovery. We will continue to monitor for the time being and resolve when confident the issue has been resolved.\nOct 25, 12:29 UTC\nUpdate - We are investigating degraded performance in Europe for Copilot. We will continue to keep users updated on progress towards mitigation.\nOct 25, 12:10 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/p351mbywbp0t",
          "publishedOn": "2023-10-25T13:02:51.000Z",
          "wordCount": 5202,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "author": null,
          "description": "Oct 22, 16:07 UTC\nResolved - This incident has been resolved.\nFrom 11:21 to 16:07 UTC some GitHub customers experienced errors cloning via workflows or via the command line.\nA third-party configuration change resulted in an unexpected behavior to our systems that resulted in Git clone failures. Once we detected the change we were able to disable it, and our systems started operating normally.\nWith the incident mitigated, we are working with our third-party provider to improve subsequent configuration change rollouts.\nOct 22, 15:58 UTC\nUpdate - We have mitigated the cause of the issue and are awaiting positive confirmation from impacted customers that the issue is fully resolved.\nOct 22, 15:34 UTC\nUpdate - We are currently investigating reports from some customers encountering errors when cloning repositories via workflows or via the command line. We do not currently have an ETA for resolution. Next update in 30 minutes.\nOct 22, 15:16 UTC\nInvestigating - We are investigating reports of degraded performance for Git Operations",
          "link": "https://www.githubstatus.com/incidents/lw4dvwltm025",
          "publishedOn": "2023-10-22T16:07:58.000Z",
          "wordCount": 5148,
          "title": "Incident with Git Operations",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/gkrfrz6r7flc",
          "author": null,
          "description": "Oct 17, 13:49 UTC\nResolved - From 10:59 UTC to 13:48 UTC, GitHub Codespaces service was down or degraded due to an outage in our authentication service.This issue impacted 67% of users over this time period.\nOur service auth layer experienced throttling with our third party dependencies due to higher load. This impacted all user facing scenarios for Codespaces service. Our automated regional failover kicked in, but it failed to mitigate as this issue impacted the service globally. We mitigated manually by reducing load on external dependency.\nWith the incident mitigated, we are working to assess and implement scaling improvements to make our service more resilient with increasing load.\nOct 17, 13:46 UTC\nUpdate - Codespaces is experiencing degraded performance. We are continuing to investigate.\nOct 17, 13:24 UTC\nUpdate - We are continuing with efforts to mitigate Codespaces issues   and are beginning to see some Codespace creations succeed.\nOct 17, 12:18 UTC\nUpdate - We have identified an issue impacting most Codespaces operations and are working on a mitigation.\nOct 17, 11:18 UTC\nUpdate - Codespaces is experiencing degraded availability. We are continuing to investigate.\nOct 17, 11:14 UTC\nInvestigating - We are investigating reports of degraded performance for Codespaces",
          "link": "https://www.githubstatus.com/incidents/gkrfrz6r7flc",
          "publishedOn": "2023-10-17T13:49:00.000Z",
          "wordCount": 5185,
          "title": "Incident with Codespaces",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        }
      ]
    },
    {
      "title": "Slack System Status",
      "feedUrl": "https://status.slack.com/feed/rss",
      "siteUrl": "https://status.slack.com/",
      "articles": [
        {
          "id": "https://status.slack.com//2023-11/16fed44d7948cf49",
          "author": null,
          "description": "We resolved the issue with user presence unexpectedly changing for some users. Affected users may need to complete a hard refresh of their Slack app (Cmd/Ctrl + Shift + R) to capture the fix.\n\r\n\r\nWe apologize for any inconvenience this has caused and appreciate your patience while we sorted this out.",
          "link": "https://status.slack.com//2023-11/16fed44d7948cf49",
          "publishedOn": "2023-11-10T23:12:00.000Z",
          "wordCount": 188,
          "title": "Incident: User presence is unexpectedly changing",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/ea3a2a3e32e79902",
          "author": null,
          "description": "Issue summary:\n\r\nFrom 10:00 PM PST on November 7, 2023 to 1:15 PM PST on November 8, 2023, some users experienced issues with their user status not updating, removing previews, and being unable to mark channels as read. Some keyboard shortcuts were also affected and were unable to be used.\n\r\n\r\nWe determined that a recent code change was the root cause for the unexpected behaviour with these features.\n\r\n\r\nTo restore functionality, we reverted the related code. We then did some additional testing and monitoring to confirm all issues were fully resolved and Slack was operating as expected.",
          "link": "https://status.slack.com//2023-11/ea3a2a3e32e79902",
          "publishedOn": "2023-11-09T04:29:31.000Z",
          "wordCount": 413,
          "title": "Incident: Issues with user status, read state, and file previews",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/2b97ec921a81988a",
          "author": null,
          "description": "This issue is now resolved and users should no longer encounter issues in huddles. We apologize for any disruption this brought to your day.",
          "link": "https://status.slack.com//2023-11/2b97ec921a81988a",
          "publishedOn": "2023-11-08T20:19:37.000Z",
          "wordCount": 111,
          "title": "Incident: Some users may be experiencing issues with huddles",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "author": null,
          "description": "Issue summary:\n\r\nOn November 4, 2023, between 5:09 PM PDT and 5:20 PM PDT, many customers were unable send messages or to connect to Slack.\n\r\n\r\nA routine code change introduced a database error that prevented cached data from being cleared correctly, resulting in severe performance issues.\n\r\n\r\nWe rolled back the code change and refreshed all affected servers, resolving the issue for all users.",
          "link": "https://status.slack.com//2023-11/ef3e4b0ebcf16d8d",
          "publishedOn": "2023-11-08T01:59:22.000Z",
          "wordCount": 176,
          "title": "Outage: Users unable to connect to Slack or send messages",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/8c886b57284762b6",
          "author": null,
          "description": "Issue summary:\n\r\n\r\nFrom 5:01 PM PDT on October 31, 2023 to around 5:54 PM PDT, customers with international data residency in the Paris, France region, experienced issues connecting to Slack and sending messages.\n\r\n\r\nA routine credential rotation caused database sync issues for the Paris, France data residency region. We reverted this code change and restarted the affected databases, resolving the issue for all impacted customers. \n\r\n\r\nOnce we had mitigated the immediate impact and restored connectivity for customers in the Paris, France data residency region, we reviewed the credential rotation for all other data residency regions to ensure the same issue would not occur anywhere else.\n\r\n\r\nPlease note that the start and end time of the incident have been edited for accuracy.",
          "link": "https://status.slack.com//2023-10/8c886b57284762b6",
          "publishedOn": "2023-11-02T03:45:50.000Z",
          "wordCount": 409,
          "title": "Outage: Issues for customers enrolled in the French data region",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "author": null,
          "description": "Customers should no longer be experiencing any connection issues with Slack. Apologies for the trouble today and thank you for your patience.",
          "link": "https://status.slack.com//2023-10/2ef86432e31615ea",
          "publishedOn": "2023-10-24T21:08:02.000Z",
          "wordCount": 195,
          "title": "Incident: A small number of users are having problems loading Slack.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        }
      ]
    },
    {
      "title": "Make Status - Incident History",
      "feedUrl": "https://status.make.com/history.rss",
      "siteUrl": "https://status.make.com",
      "articles": [
        {
          "id": "https://status.make.com/incidents/g6gklsv4shx6",
          "author": null,
          "description": "Nov  7, 12:43 CET\nResolved - This incident has been resolved.\nNov  6, 16:30 CET\nMonitoring - We have noticed a degradation of performance on eu1.make.celonis.com. A fix has been applied and the system is fully operational again.",
          "link": "https://status.make.com/incidents/g6gklsv4shx6",
          "publishedOn": "2023-11-07T11:43:11.000Z",
          "wordCount": 3844,
          "title": "Failing to execute scenarios",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.make.com/incidents/2c52p236t4rp",
          "author": null,
          "description": "Oct 12, 17:40 CEST\nResolved - This incident has been resolved.\nOct 12, 16:21 CEST\nMonitoring - We implemented workaround and we are currently monitoring the issue.\nOct 12, 16:09 CEST\nInvestigating - We are experiencing issues within our scenario features i.e. users cannot clone scenario nor move it to a folder inside list of scenarios. This is still possible when user open particular scenario.\nWe are currently working on implementing a quick workaround, we should publish the relevant steps within the next 30 minutes.",
          "link": "https://status.make.com/incidents/2c52p236t4rp",
          "publishedOn": "2023-10-12T15:40:47.000Z",
          "wordCount": 3903,
          "title": "Users cannot clone nor move scenarios",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}