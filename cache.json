{
  "sources": [
    {
      "title": "BookingSync.com news",
      "feedUrl": "https://changelog.bookingsync.com/rss",
      "siteUrl": "https://changelog.bookingsync.com",
      "articles": [
        {
          "id": "287075",
          "author": "Megan, Product Manager",
          "description": "Improvement\n  \nWe've enhancing the payment descriptions that are displayed in your Stripe or BookingPay account on your transactions, to make them clearer and more useful for you, based on your valuable feedback.\nHere's What's Changing:\nThe description will now include the booking reference:\nOld Format: \"Payment for Beautiful Example Rental from Monday 27 May 2024, 04:00 PM to Sunday 02 Jun 2024, 10:00 AM\"\n​\n\n\nNew Format: \"Payment for booking XYZ123 at Beautiful Example Rental from 27 May 2024 to 02 Jun 2024\"\n\n\nWhy? To streamline tracking and management of your bookings, making reconciliation easier.\nThank you for being a valuable member of the Smily community. We're thrilled to keep enhancing your property management journey!",
          "link": "https://changelog.bookingsync.com/new-improved-payment-description-format-287075",
          "publishedOn": "2024-03-01T12:58:56.000Z",
          "wordCount": 341,
          "title": "New improved payment description format",
          "imageUrl": null
        },
        {
          "id": "286877",
          "author": "Ella, Chief Customer Officer (CCO) & Cofounder",
          "description": "Communications\n  \nIn today’s Smily newsletter, we directly address Vrbo performance optimization, spotlighting essential optimization tips shared by our Partner Vrbo. We also introduce our latest releases, designed to streamline your operations, and preview upcoming events where Smily will be featured. This edition delivers focused insights and recent developments to strengthen your property management strategies. 🙌\n🔍Optimization tips: Enhance your listings on Vrbo\nOur partner Vrbo, has generously provided a selection of their top optimization practices to elevate your overall listing's performance. Below are the main highlights, along with guidance on implementing these strategies through your Smily account:\nOptimize listing content - What makes a Vrbo listing attractive 💙\nKey amenitie…",
          "link": "https://changelog.bookingsync.com/unlock-your-rental's-potential-expert-vrbo-strategies-smily-innovations-upcomming-events!-286877",
          "publishedOn": "2024-02-28T10:21:46.000Z",
          "wordCount": 1004,
          "title": "Unlock your rental's potential: Expert Vrbo Strategies, Smily Innovations & Upcomming Events! 🗝️",
          "imageUrl": null
        },
        {
          "id": "286647",
          "author": "Basile, Product Manager",
          "description": "New!\n  \nAre you connected to VRBO OTA?\nStarting today at 2pm UTC, we will be fetching guest reviews and ratings from VRBO, and you'll be able to respond to them directly from Smily interface :)\n🚧 Pain Points Addressed:\nNo need to visit VRBO to check and respond to guest reviews.\nPreviously, you couldn't rate a guest.\n🚀 Benefits for you:\nManage all your reviews in one place.\nEasily read and respond to VRBO guest reviews from our Review section on the Smily interface.\nRate guests with ease.\nℹ️ Information and next steps:\nWe will fetch all your past reviews.\n\n\nYou will be able to rate all past bookings matching those criteria:\n--> Bookings without guest review that ended in the last 365 days;\n--> Bookings for which a guest review is already present and 14 days haven’t passed since the guest made the review;\n\n\nMake use of this new feature and ensure to leave reviews for guests. This is an important criterion for VRBO and will impact your property ranking.\n\n\nLastly, you can use our \"automated reviews\" feature, released at the end of last year, for VRBO reviews too.\n\n\nHave a great review day,\nYour Smily team :)",
          "link": "https://changelog.bookingsync.com/manage-your-vrbo-guest-review-from-smily-interface-286647",
          "publishedOn": "2024-02-26T10:54:27.000Z",
          "wordCount": 432,
          "title": "Manage your VRBO guest review from Smily interface",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "Security Bulletins on Tailscale",
      "feedUrl": "https://tailscale.com/security-bulletins/index.xml",
      "siteUrl": "https://tailscale.com/security-bulletins/",
      "articles": []
    },
    {
      "title": "Airbnb API Status - Incident History",
      "feedUrl": "https://airbnbapi.statuspage.io/history.rss",
      "siteUrl": "https://airbnbapi.statuspage.io",
      "articles": [
        {
          "id": "https://airbnbapi.statuspage.io/incidents/hjdlmbqk7krr",
          "author": null,
          "description": "Mar  7, 14:40 PST\nResolved - This incident has been resolved as of March 6th, 2024, 12:08 AM PST. Error rates have returned to normal levels. Please retry any failed requests.\nWe apologize for the inconvenience caused and thank you for your patience and understanding.\nMar  6, 12:38 PST\nMonitoring - We've identified an increased number of 500 errors across multiple endpoints starting today 11:52 AM PST.This issue has been mitigated at 12:08 AM PST. \nPlease apply any failed requests during the incident.",
          "link": "https://airbnbapi.statuspage.io/incidents/hjdlmbqk7krr",
          "publishedOn": "2024-03-07T22:40:08.000Z",
          "wordCount": 3892,
          "title": "500 Errors Across Multiple Endpoints",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/5qkdf7vy8v6v",
          "author": null,
          "description": "Mar  5, 13:22 PST\nResolved - This incident has been resolved.\nMar  4, 04:41 PST\nMonitoring - We've mitigated the issue at 4:10AM PDT and will continue to monitor it.\nMar  4, 02:49 PST\nInvestigating - We are investigating an increased number of 500 errors across multiple availability related endpoints. These errors started on March 01, 2024.\nOur engineering teams are actively working towards a resolution to restore service fully.",
          "link": "https://airbnbapi.statuspage.io/incidents/5qkdf7vy8v6v",
          "publishedOn": "2024-03-05T21:22:24.000Z",
          "wordCount": 3884,
          "title": "500 Errors Across Multiple Availability Endpoints",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/47y20gmnl3h9",
          "author": null,
          "description": "Mar  4, 16:16 PST\nResolved - This incident has been resolved.\nError rates have returned to normal levels. Please retry any failed requests.\nWe apologize for the inconvenience caused and thank you for your patience and understanding.\nMar  4, 14:10 PST\nMonitoring - The issue has been mitigated at 1:45 PM PDT. Please apply any failed requests during the incident.\nMar  4, 13:24 PST\nInvestigating - We are actively investigating an increased number of 500 errors across multiple endpoints. These errors started today (March 4, 2024) around 12:57 PM PDT. Our engineering teams are working to get everything up and running again and we will update you with the latest information as soon as possible.",
          "link": "https://airbnbapi.statuspage.io/incidents/47y20gmnl3h9",
          "publishedOn": "2024-03-05T00:16:15.000Z",
          "wordCount": 3925,
          "title": "500 Errors Across Multiple Endpoints",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/5vnndg8lvchr",
          "author": null,
          "description": "Mar  4, 14:25 PST\nResolved - This incident has been resolved. Don't hesitate to contact us if you need further assistance.\nMar  4, 11:17 PST\nMonitoring - We recently experienced an incident that affected all reservation-related webhooks between 9:54 AM and 10:44 AM PT on March 4, 2024. During this time, you may have noticed reservations being requested, confirmed, altered, and canceled on your listings, but no corresponding webhooks were received. The issue has been resolved, and we will be sending all the missing webhooks in the upcoming hours. In the meantime, if you need to retrieve reservation information for your listings, you can use the GET reservations API. Please note that this incident only affected reservation-related webhooks.\nWe apologize for the inconvenience, and please don't hesitate to contact us if you need further assistance.",
          "link": "https://airbnbapi.statuspage.io/incidents/5vnndg8lvchr",
          "publishedOn": "2024-03-04T22:25:36.000Z",
          "wordCount": 3944,
          "title": "Issue with reservation related webhooks",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://airbnbapi.statuspage.io/incidents/qfy422ht70s1",
          "author": null,
          "description": "Feb 28, 17:05 PST\nCompleted - The scheduled maintenance has been completed.\nFeb 28, 17:00 PST\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nFeb 22, 23:23 PST\nScheduled - Please note upcoming maintenance occurring on Wednesday, 2/28/2024 at 5:00 PM PT, that will result in ~30-60 second WRITE downtime to CreateMessage API. Reads will not be affected.\nAlerts and errors pertaining to Messaging on Wednesday, 2/28/2024 around 5:00 PM PT, are to be expected. Please consider any corrective actions needed to mitigate potential data loss during this ~30-60 second period.",
          "link": "https://airbnbapi.statuspage.io/incidents/qfy422ht70s1",
          "publishedOn": "2024-02-29T01:05:56.000Z",
          "wordCount": 3910,
          "title": "CreateMessage API - WRITE downtime",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Twilio Status - Incident History",
      "feedUrl": "https://status.twilio.com/history.rss",
      "siteUrl": "https://status.twilio.com",
      "articles": []
    },
    {
      "title": "DigitalOcean Status - Incident History",
      "feedUrl": "https://status.digitalocean.com/history.rss",
      "siteUrl": "http://status.digitalocean.com",
      "articles": [
        {
          "id": "https://status.digitalocean.com/incidents/6w4ffgm83798",
          "author": null,
          "description": "THIS IS A SCHEDULED EVENT Mar 17, 02:00 - 05:00 UTC\nMar 12, 16:11 UTC\nScheduled - Start: 2024-03-17 02:00 UTC\nEnd: 2024-03-17 05:00 UTC\nDuring the above maintenance window, there will be maintenance performed on our ticketing system.\nExpected Impact:\nDuring the course of the maintenance, users will be unable to submit support tickets, update existing tickets, or receive replies to existing tickets. Users will also be unable to log into https://cloudsupport.digitalocean.com/s/ or open the Support Portal from within the Cloud Control Panel.\nAny tickets submitted during the maintenance via alternate methods (such as replying to an email chain or via our webform) will be saved and entered into our ticketing system at the completion of the maintenance.\nOur Support Team will also be impacted by this maintenance and will be unable to enter the ticketing system to receive any tickets or reply to existing tickets. As soon as the vendor maintenance has completed, our team will address all support queries as quickly as possible.\nWe appreciate your patience throughout this process.",
          "link": "https://status.digitalocean.com/incidents/6w4ffgm83798",
          "publishedOn": "2024-03-17T02:00:00.000Z",
          "wordCount": 6346,
          "title": "DigitalOcean Support Portal Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/2nng21f5nvfn",
          "author": null,
          "description": "Mar 15, 23:54 UTC\nResolved - Our Engineering team has confirmed full resolution of DNS resolution issue in the BLR1 region. \nWe appreciate your patience throughout this process and if you continue to experience problems, please open a ticket with our support team for further review.\nMar 15, 23:27 UTC\nMonitoring - Our Engineering team has rolled out a fix for the DNS resolution issue in the BLR1 region. Users should now be able to create Droplets with firewall rules.\nWe'll post an update once the incident is fully resolved.\nMar 15, 23:09 UTC\nInvestigating - Our Engineering team is currently investigating issues with DNS resolution in BLR1 region. During this time, customers may experience issues while creating new Droplets with firewall rules in the BLR1 region.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/2nng21f5nvfn",
          "publishedOn": "2024-03-15T23:54:14.000Z",
          "wordCount": 6328,
          "title": "Droplet Creation in BLR1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/65lj1w0mxvmb",
          "author": null,
          "description": "Mar 12, 11:43 UTC\nResolved - As of 10:09 UTC, our Engineering team has confirmed the full resolution of the issue impacting Droplet connectivity and Event processing in multiple regions.\nUsers should no longer see issues with their Droplets and Droplet-related services.\nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident.\nMar 12, 10:07 UTC\nMonitoring - Our Engineering team has confirmed that the issue impacting Droplet connectivity in multiple regions has been mitigated.\nAt this time, users should no longer see issues when connecting to their Droplet and Droplet-related services.\nWe will further monitor this incident and will post an update as soon as the issue is fully resolved.\nMar 12, 09:29 UTC\nIdentifi…",
          "link": "https://status.digitalocean.com/incidents/65lj1w0mxvmb",
          "publishedOn": "2024-03-12T11:43:59.000Z",
          "wordCount": 6544,
          "title": "Droplet connectivity and Event processing in multiple regions.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/1755wn45yqyw",
          "author": null,
          "description": "Mar  7, 12:21 UTC\nResolved - Our Engineering team identified and resolved an issue that affected Spaces availability in the SFO2 region.\nFrom 11:38 UTC to 11:58 UTC, users may have encountered errors while accessing Spaces objects and creating new buckets in the SFO2 region.\nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.",
          "link": "https://status.digitalocean.com/incidents/1755wn45yqyw",
          "publishedOn": "2024-03-07T12:21:18.000Z",
          "wordCount": 6250,
          "title": "Spaces Availability in SFO2",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/l9d26ggtbw8h",
          "author": null,
          "description": "Mar  6, 19:09 UTC\nResolved - Our Engineering team has identified and resolved an issue in the TOR1 region that impacted the network connectivity for a subset of Droplets and Droplet-based services for a brief duration.\nFrom 17:38 - 17:47 UTC, users might have experienced delays or errors while accessing and connecting to their resources in the TOR1 region from the public internet or from other resources in TOR1. Swift action was taken by our Engineering team that restored service and all services in TOR1 are operating correctly. \nWe apologize for the inconvenience. If you have any questions or continue to experience issues, please reach out via a Support ticket on your account.",
          "link": "https://status.digitalocean.com/incidents/l9d26ggtbw8h",
          "publishedOn": "2024-03-06T19:09:05.000Z",
          "wordCount": 6289,
          "title": "Network Connectivity in TOR1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/s5v9qwsvlxjv",
          "author": null,
          "description": "Mar  5, 23:41 UTC\nResolved - Our Engineering team has confirmed that this incident has been fully resolved.\nIf you continue to experience any issues with Managed Database Clusters please open a ticket with our support team. Thank you for your patience.\nMar  5, 20:28 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the issue with our Managed Databases services. At this time, we're observing error rates returning to pre-incident levels and seeing operations such as create/fork/restore succeed. Trusted sources updates are also functioning normally, so connectivity to Database clusters from newly added resources to trusted sources is restored. \nWe are monitoring the situation closely and will post an update as soon as we confirm the issue is fully resolved.\nMar  5, 17:39 …",
          "link": "https://status.digitalocean.com/incidents/s5v9qwsvlxjv",
          "publishedOn": "2024-03-05T23:41:12.000Z",
          "wordCount": 6699,
          "title": "Managed Databases Control Plane and Connectivity",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/gqwgnn1x1m4z",
          "author": null,
          "description": "Mar  5, 15:00 UTC\nCompleted - The scheduled maintenance has been completed.\nMar  5, 14:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nMar  5, 11:30 UTC\nScheduled - Start: 2024-03-05 14:00 UTC\nEnd:  2024-03-05 15:00 UTC\nDuring the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration. This maintenance was previously attempted on 2024-02-19 but the changes were reverted after our Engineers encountered unexpected issues, resulting from the maintenance. Our team has performed a thorough examination of the previous attempt and is confident in performing this maintenance, as well as measures to mitigate any negative outcomes. \nExpected impact:\nAs part of this maintenance, event processing in BLR1 will be delayed for a period of up to 15 minutes during the one-hour window. During this period, users will experience a delay with creating, destroying, or modifying new or existing DO services in BLR1(such as Droplets, DBaaS/DOKS clusters, etc.), existing services that are running should not be impacted.\nIf you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket",
          "link": "https://status.digitalocean.com/incidents/gqwgnn1x1m4z",
          "publishedOn": "2024-03-05T15:00:56.000Z",
          "wordCount": 6375,
          "title": "BLR1 Network Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/dj2p47c4zss1",
          "author": null,
          "description": "Feb 29, 05:50 UTC\nResolved - Our Engineering team identified and resolved an issue that was affecting the booting of Droplets from the Recovery ISO.\nFrom 00:20 UTC to 05:24 UTC, users might have experienced errors when attempting to boot Droplets from the Recovery ISO.\nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience and we apologize for any inconvenience.",
          "link": "https://status.digitalocean.com/incidents/dj2p47c4zss1",
          "publishedOn": "2024-02-29T05:50:18.000Z",
          "wordCount": 6260,
          "title": "Droplet Recovery Image",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/cd70bby9qynm",
          "author": null,
          "description": "Feb 28, 22:53 UTC\nResolved - Our Engineering team has confirmed that creation, forking, and restoration of PostgreSQL clusters on v16 is functioning correctly.\nUpgrades for lower-versioned PostgreSQL clusters to v16 remain unavailable at this time and users will see errors if they attempt to perform that upgrade. Our Engineering team continues to work on making upgrades to v16 available again, but we expect this to take some time.\nIf you continue to experience issues or have any questions, please open a ticket with our support team.\nFeb 28, 20:56 UTC\nMonitoring - After testing, teams have determined that PostgreSQL v16 is safe for new creations, as well as forks and restores for existing clusters. At this time, v16 is re-enabled in our Cloud Control Panel and users creating, forking, or re…",
          "link": "https://status.digitalocean.com/incidents/cd70bby9qynm",
          "publishedOn": "2024-02-28T22:53:47.000Z",
          "wordCount": 6751,
          "title": "Managed Databases creation - PostgresSQL v16",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/g4sdvwy4z4zm",
          "author": null,
          "description": "Feb 28, 22:49 UTC\nResolved - Our Engineering team has confirmed full resolution of the issue with networking in our SFO2 region. \nIf you continue to experience problems, please open a ticket with our support team. Thank you for your patience throughout this incident!\nFeb 28, 22:23 UTC\nMonitoring - Our Engineering team has confirmed that the faulty network hardware component was the cause of this issue. From 21:39 - 22:11 UTC, this component was not functioning correctly, causing networking issues for a subset of customers in our SFO2 region, as well as internal alerts in our SFO1/SFO3 regions. \nAt this time, all services should now be operating normally. We will monitor this incident for a short period of time to confirm full resolution.\nFeb 28, 22:17 UTC\nIdentified - Our Engineering team has identified the cause of the issue with networking in our SFO regions to be related to an issue with a network hardware component in SFO2. They have isolated that component and we're observing error rates returning to pre-incident levels at this time. \nWe are continuing to look into this failure, but users should be seeing recovery on their services. We'll provide another update soon.\nFeb 28, 22:06 UTC\nInvestigating - Our Engineering team is currently investigating internal alerts and customer reports for an increase in networking errors in our SFO regions for Droplets and Droplet-based services. We will provide an update as soon as we have further information.",
          "link": "https://status.digitalocean.com/incidents/g4sdvwy4z4zm",
          "publishedOn": "2024-02-28T22:49:41.000Z",
          "wordCount": 6426,
          "title": "SFO Networking",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/7rzlkzs0ksqs",
          "author": null,
          "description": "Feb 27, 20:00 UTC\nCompleted - The scheduled maintenance has been completed.\nFeb 27, 16:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nFeb 20, 15:19 UTC\nScheduled - Start: 2024-02-27 16:00 UTC\nEnd:  2024-02-27  20:00 UTC\n\nDuring the above window, our Networking team will be making changes to core networking infrastructure, to improve performance and scalability in the AMS3 region. \nExpected impact:\nThese upgrades are designed and tested to be seamless and we do not expect any impact to customer traffic due to this maintenance. If an unexpected issue arises, affected Droplets and Droplet-based services may experience increased latency or a brief disruption in network traffic. We will endeavor to keep any such impact to a minimum.\nIf you have any questions related to this issue please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket",
          "link": "https://status.digitalocean.com/incidents/7rzlkzs0ksqs",
          "publishedOn": "2024-02-27T20:00:57.000Z",
          "wordCount": 6327,
          "title": "AMS3 Network Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/n9211k9rmml7",
          "author": null,
          "description": "Feb 21, 20:43 UTC\nResolved - Our Engineering team has confirmed the resolution of the issue impacting the Container Registry in multiple regions. \nEverything involving the Container Registry should now be functioning normally. \nWe appreciate your patience throughout the process and if you continue to experience problems, please open a ticket with our support team for further review.\nFeb 21, 18:03 UTC\nMonitoring - Our Engineering team has identified an internal operation within the Container Registry service which was placing load on the service, leading to latency and errors. The team has paused that operation in order to resolve the issue impacting the Container Registry in multiple regions. Users should not be facing any latency issues while interacting with their Container registries and also while building their Apps. \nWe are actively monitoring the situation to ensure stability and will provide an update once the incident has been fully resolved. \nThank you for your patience and we apologize for the inconvenience.\nFeb 21, 15:41 UTC\nInvestigating - Our Engineering team is investigating an issue with the DigitalOcean Container Registry service. Beginning around 20:00 UTC on February 20, there has been an uptick in 401 errors for image pulls from the Container Registry service.\nDuring this time, a subset of customers may experience latency or see 401 errors while interacting with Container Registries. This issue also impacts App Platform builds and users may encounter delays while building their Apps or experience timeout errors in builds as a result. Users utilizing Container Registry for images for deployment to Managed Kubernetes clusters may also see latency or failures to deploy.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/n9211k9rmml7",
          "publishedOn": "2024-02-21T20:43:48.000Z",
          "wordCount": 6477,
          "title": "Container Registry Latency in Multiple Regions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/rccs80k3p2pb",
          "author": null,
          "description": "Feb 20, 07:07 UTC\nResolved - As of 06:25 am UTC, our Engineering team has confirmed the resolution of the issue impacting Spaces availability in the BLR1 region.\nUsers should no longer experience issues with their Spaces resources in the BLR1 region.\nIf you continue to experience problems, please open a ticket with our support team. We apologize for any inconvenience.\nFeb 20, 06:39 UTC\nMonitoring - Our Engineering team has implemented a fix to resolve the Spaces availability issues in the BLR1 region and is monitoring the situation. \nUsers should no longer encounter errors when accessing Spaces in the BLR1 region and should be able to create new Spaces buckets from the cloud control panel. \nWe will post an update as soon as the issue is fully resolved.\nFeb 20, 05:46 UTC\nInvestigating - Our Engineering team is investigating an issue with Spaces availability in the BLR1 region. During this time users may encounter errors when accessing Spaces objects and creating new buckets in the BLR1 region. \nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/rccs80k3p2pb",
          "publishedOn": "2024-02-20T07:07:10.000Z",
          "wordCount": 6367,
          "title": "Spaces Availability in BLR1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/5z0npmmmnc1h",
          "author": null,
          "description": "Feb 19, 17:00 UTC\nCompleted - The scheduled maintenance has been completed.\nFeb 19, 14:00 UTC\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nFeb 16, 14:52 UTC\nScheduled - Start: 2024-02-19 14:00 UTC\nEnd:  2024-02-19 17:00 UTC\nHello,\nDuring the above window, we will be performing maintenance in our BLR1 region as part of a firewall migration.\nExpected impact:\nAs part of this maintenance, event processing in BLR1 will be disabled for a period of up to 15 minutes during the three-hour window. During this period, users won't be able to create, destroy, or modify new or existing DO services in BLR1 (such as Droplets, DBaaS/DOKS clusters, etc.).\nIf you have any questions related to this issue, please send us a ticket from your cloud support page. https://cloudsupport.digitalocean.com/s/createticket\n\nThank you,\nTeam DigitalOcean",
          "link": "https://status.digitalocean.com/incidents/5z0npmmmnc1h",
          "publishedOn": "2024-02-19T17:00:56.000Z",
          "wordCount": 6318,
          "title": "BLR1 Network Maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/q2sl05rc7mgp",
          "author": null,
          "description": "Feb 19, 15:50 UTC\nResolved - From 15:50 - 16:46, our team received customer reports of issues impacting multiple products in our BLR1 region, including the accessibility of Managed Databases and Managed Kubernetes clusters and general network connectivity disruption. These issues may be related to a scheduled maintenance event in the region, per our status post linked below:\nhttps://status.digitalocean.com/incidents/5z0npmmmnc1h\nOur team continues to review customer reports and diagnose the impact related to this maintenance. In the meantime, we have rolled back the maintenance process and all services should now be responding normally. If you experience any further issues, please open a ticket with our Support team. Thank you for your patience and we apologize for any inconvenience.",
          "link": "https://status.digitalocean.com/incidents/q2sl05rc7mgp",
          "publishedOn": "2024-02-19T15:50:00.000Z",
          "wordCount": 6295,
          "title": "Multiple Products Impacted in BLR1",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.digitalocean.com/incidents/yp30nqsv310k",
          "author": null,
          "description": "Feb 18, 19:08 UTC\nResolved - As of 17:47 UTC, our Engineering team has confirmed the full resolution of the problem impacting the Managed Kubernetes service in our NYC3 region. The Cilium pods inside the clusters should be functioning normally. \nIf you continue to experience problems, please open a ticket with our Support team. \nThank you for your patience and we apologize for the inconvenience.\nFeb 18, 18:09 UTC\nMonitoring - Our Engineering team has deployed the fix for the issue with Managed Kubernetes service where users were experiencing network connectivity issues with Cilium pods being restarted inside the clusters. Cilium pods should now be functioning normally. \nWe are monitoring the situation and will post another update once we confirm the fix resolves this incident.\nFeb 18, 16:57 UTC\nInvestigating - Our Engineering team is investigating an issue with our Managed Kubernetes service in NYC3 region. \nDuring this time users may experience network connectivity issues specifically with the Cilium pods inside their clusters.\nWe apologize for the inconvenience and will share an update once we have more information.",
          "link": "https://status.digitalocean.com/incidents/yp30nqsv310k",
          "publishedOn": "2024-02-18T19:08:07.000Z",
          "wordCount": 6363,
          "title": "Managed Kubernetes in NYC3",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Notion Status - Incident History",
      "feedUrl": "https://status.notion.so/history.rss",
      "siteUrl": "https://status.notion.so",
      "articles": [
        {
          "id": "https://status.notion.so/incidents/gnzy4hx09dq6",
          "author": null,
          "description": "Feb 23, 16:52 PST\nResolved - Our engineering team identified the issue and released a fix to resolve it.\nFeb 23, 16:42 PST\nInvestigating - Customers may experience an error when duplicating some templates.\nOur team is investigating the root cause now and we will share updates as soon as possible.",
          "link": "https://status.notion.so/incidents/gnzy4hx09dq6",
          "publishedOn": "2024-02-24T00:52:27.000Z",
          "wordCount": 3355,
          "title": "Some template duplication may fail",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    },
    {
      "title": "Rippling Status - Incident History",
      "feedUrl": "https://status.rippling.com/history.rss",
      "siteUrl": "https://status.rippling.com",
      "articles": [
        {
          "id": "https://status.rippling.com/incidents/b20xb0qlwd48",
          "author": null,
          "description": "Mar 15, 20:18 UTC\nResolved - This incident has been resolved.\nMar 15, 19:51 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nMar 15, 18:55 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://status.rippling.com/incidents/b20xb0qlwd48",
          "publishedOn": "2024-03-15T20:18:17.000Z",
          "wordCount": 5290,
          "title": "Device management functionality is degraded",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/j0y55xs3d05q",
          "author": null,
          "description": "Mar  7, 18:09 UTC\nResolved - This incident has been resolved.\nMar  6, 22:57 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nMar  6, 16:58 UTC\nUpdate - We are continuing to work on a fix for this issue. This affects users who have their accounts created prior to their start date.\nMar  6, 16:51 UTC\nIdentified - The issue has been identified and a fix is being implemented.",
          "link": "https://status.rippling.com/incidents/j0y55xs3d05q",
          "publishedOn": "2024-03-07T18:09:18.000Z",
          "wordCount": 5330,
          "title": "Delays creating accounts in third-party apps",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        },
        {
          "id": "https://status.rippling.com/incidents/5pv7fqwgvtck",
          "author": null,
          "description": "Feb 27, 01:21 UTC\nResolved - This incident has been resolved.\nFeb 26, 23:24 UTC\nMonitoring - A fix has been implemented and we are monitoring the results.\nFeb 26, 22:06 UTC\nIdentified - The issue has been identified and a fix is being implemented.",
          "link": "https://status.rippling.com/incidents/5pv7fqwgvtck",
          "publishedOn": "2024-02-27T01:21:53.000Z",
          "wordCount": 5288,
          "title": "Issues registering an account",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/248858/rippling_favicoon.png"
        }
      ]
    },
    {
      "title": "Google Workspace Status Dashboard Updates",
      "feedUrl": "https://www.google.com/appsstatus/dashboard/en/feed.atom",
      "siteUrl": "https://www.google.com/appsstatus/dashboard/",
      "articles": [
        {
          "id": "https://www.google.com/appsstatus/dashboard/incidents/Nr8avWyykjurdUqXYSDW",
          "author": null,
          "description": "<p> Incident began at <strong>2024-03-06 22:32</strong> and ended at <strong>2024-03-06 23:29</strong> <span>(times are in <strong>Coordinated Universal Time (UTC)</strong>).</span></p><div class=\"cBIRi14aVDP__status-update-text\"><h1>Incident Report</h1>\n<h2>Summary</h2>\n<p>On Wednesday, 06 March 2024, Google Workspace users experienced elevated errors while accessing user or device setting pages within the Chrome Admin Console for a duration of 1 hour, 27 minutes.</p>\n<p>To our Google Workspace customers who were impacted during this disruption, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform’s performance and availability.</p>\n<h2>Root Cause</h2>\n<p>Google Workspace Admin Console uses a backend authentication service to grant privileged access to internal accounts. As part of ongoing efforts to migrate the account authorization workflow to a new backend authentication service, Google engineers identified and cleaned up invalid account role assignments in the configuration.</p>\n<p>During this clean up effort, engineers identified a privilege which was undefined in the application manifest and removed it from the configuration. Upon further investigation, engineering confirmed the privilege was mapped to an access control setting used by the Admin Console for Chrome. This change resulted in users being unable to access Chrome user and device settings page.</p>\n<h2>Remediation and Prevention</h2>\n<p>Google engineers were alerted to the outage via an internal monitoring alert on Wednesday, 06 March 2024 at 15:04 US/Pacific and immediately started an investigation. Once the nature and scope of the issue became clear, Google engineers initiated a rollback of the configuration change at 15:36 and the incident was fully resolved at 15:59 once the rollback completed.</p>\n<p>Google is committed to preventing a repeat of this issue in the future and is completing the following actions:</p>\n<ul>\n<li>Improve procedural documentation on how to safely cleanup access control settings from authorization configuration.</li>\n<li>Complete thorough investigation of privilege usage for the Admin Console to prevent further issues of this type.</li>\n<li>Identify and register a new owner for this privilege in the application manifest and create permission mapping for faster verification in the future.</li>\n</ul>\n<p>Google is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.</p>\n<h2>Detailed Description of Impact</h2>\n<p>On Wednesday, 06 March 2024 from 14:32 to 15:59 US/Pacific, Google Workspace users attempting to work within the Admin Console encountered an HTTP 500 error when accessing user or device setting pages within the console.</p>\n<hr>\n</div><hr><p>Affected products: Admin Console</p>",
          "link": "https://www.google.com/appsstatus/dashboard/incidents/Nr8avWyykjurdUqXYSDW",
          "publishedOn": "2024-03-13T21:26:43.000Z",
          "wordCount": 1131,
          "title": "RESOLVED: **Summary:**\nAdmin Console is experiencing elevated errors with the Chrome setting pages.\n**Description:**\nMitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2024-03-06 16:15 US/Pacific.\nWe will provide more information by Wednesday, 2024-03-06 16:45 US/Pacific.\n**Customer Symptoms:**\nThe impacted customers would encounter a HTTP 500 error while accessing user or device setting pages within Chrome Admin Console. Additionally, customers would not be able to view or change any policies.\n**Workaround:**\nNone at this time.",
          "imageUrl": null
        }
      ]
    },
    {
      "title": "GitHub Status - Incident History",
      "feedUrl": "https://www.githubstatus.com/history.rss",
      "siteUrl": "https://www.githubstatus.com",
      "articles": [
        {
          "id": "https://www.githubstatus.com/incidents/m07hfg0gly20",
          "author": null,
          "description": "Mar 15, 20:28 UTC\nResolved - This incident has been resolved.\nMar 15, 20:27 UTC\nUpdate - Actions is operating normally.\nMar 15, 20:09 UTC\nUpdate - Pages is experiencing degraded performance. We are continuing to investigate.\nMar 15, 20:07 UTC\nInvestigating - We are investigating reports of degraded performance for Actions",
          "link": "https://www.githubstatus.com/incidents/m07hfg0gly20",
          "publishedOn": "2024-03-15T20:28:06.000Z",
          "wordCount": 5308,
          "title": "Incident with Actions and Pages",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/9ym5p2sg6w5v",
          "author": null,
          "description": "Mar 15, 20:24 UTC\nResolved - This incident has been resolved.\nMar 15, 20:21 UTC\nUpdate - Codespaces is operating normally.\nMar 15, 20:20 UTC\nUpdate - API Requests is operating normally.\nMar 15, 20:17 UTC\nUpdate - We rolled back the most recent deployment and are seeing improvements across all services, and will continue to monitor for additional impact.\nMar 15, 20:11 UTC\nUpdate - API Requests is experiencing degraded performance. We are continuing to investigate.\nMar 15, 20:03 UTC\nUpdate - Codespaces is experiencing degraded availability. We are continuing to investigate.\nMar 15, 20:03 UTC\nUpdate - API Requests is experiencing degraded availability. We are continuing to investigate.\nMar 15, 20:00 UTC\nUpdate - API Requests is experiencing degraded performance. We are continuing to investigate.\nMar 15, 19:55 UTC\nInvestigating - We are investigating reports of degraded performance for Codespaces",
          "link": "https://www.githubstatus.com/incidents/9ym5p2sg6w5v",
          "publishedOn": "2024-03-15T20:24:44.000Z",
          "wordCount": 5412,
          "title": "Incident with Codespaces and API Requests",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/q3gl44chgxwv",
          "author": null,
          "description": "Mar 13, 01:58 UTC\nResolved - From March 12, 2024 23:39 UTC to March 13, 2024 1:58 UTC, some Pull Requests updates were delayed and did not reflect the latest code that had been pushed. On average, 20% of Pull Requests page loads were out of sync and up to 30% of Pull Requests were impacted at peak. An internal component of our job queueing system was incorrectly handling invalid messages, resulting in stalled processing.\nWe mitigated the incident by shipping a fix to handle the edge case gracefully and allow processing to continue.\nOnce the fix was deployed at 1:47 UTC, our systems fully caught up with pending background jobs at 1:58 UTC.\nWe’re working to improve resiliency to invalid messages in our system to prevent future delays for these pull request updates. We are also reviewing our monitoring and observability to identify and remediate these types of failure cases faster.\n\nMar 13, 01:58 UTC\nUpdate - Pull Requests is operating normally.\nMar 13, 01:53 UTC\nUpdate - We believe we've found a mitigation and are currently monitoring systems for recovery.\nMar 13, 01:18 UTC\nUpdate - We're continuing to investigate delays in PR updates. Next update in 30 minutes.\nMar 13, 00:47 UTC\nUpdate - We're continuing to investigate an elevated number of pull requests that are out of sync on page load.\nMar 13, 00:12 UTC\nUpdate - We're continuing to investigate an elevated number of pull requests that are out of sync on page load.\nMar 12, 23:39 UTC\nUpdate - We're seeing an elevated number of pull requests that are out of sync on page load.\nMar 12, 23:39 UTC\nInvestigating - We are investigating reports of degraded performance for Pull Requests",
          "link": "https://www.githubstatus.com/incidents/q3gl44chgxwv",
          "publishedOn": "2024-03-13T01:58:49.000Z",
          "wordCount": 5544,
          "title": "Incident with Pull Requests",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/fr6fgvdn41sw",
          "author": null,
          "description": "Mar 12, 01:00 UTC\nResolved - On March 11, 2024 starting at 22:45 UTC and ending on March 12, 2024 00:48 UTC various GitHub services were degraded and returned intermittent errors for users. During this incident, the following customer impacts occurred: API error rates as high as 1%, Copilot error rates as high as 17%, and Secret Scanning and 2FA using GitHub Mobile error rates as high as 100% followed by a drop in error rates to 30% starting at 22:55 UTC. This elevated error rate was due to a degradation of our centralized authentication service upon which many other services depend.\nThe issue was caused by a deployment of network related configuration that was inadvertently applied to the incorrect environment. This error was detected within 4 minutes and a rollback was initiated. While e…",
          "link": "https://www.githubstatus.com/incidents/fr6fgvdn41sw",
          "publishedOn": "2024-03-12T01:00:51.000Z",
          "wordCount": 5780,
          "title": "Incident with API Requests, Git Operations, Webhooks and Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/bgnqgwyj76l4",
          "author": null,
          "description": "Mar 11, 19:22 UTC\nResolved - On March 11, 2024 between at 18:44 UTC and 19:10 UTC, GitHub Actions performance was degraded and some users experienced errors when trying to queue workflows. Approximately 3.7% of runs queued during this time were unable to start.\nThe issue was partially caused by a deployment of an internal system Actions relies on to process workflow run events. The pausing of the queue processing during this deployment for about 3 minutes caused a spike in queued workflow runs. When this queue began to be processed, the high number of queued workflows overwhelmed a secret-initialization component of the workflow invocation system. The errors generated by this overwhelmed system ultimately delayed workflow invocation. Through our alerting system, we received initial indications of an issue at approximately 18:44 UTC. However, we did not initially see impact on our run start delays and run queuing availability metrics until approximately 18:52 UTC. As the large queue of workflow run events burned down, we saw recovery in our key customer impact measures by 19:11 UTC, but waited to declare the incident resolved at 19:22 UTC while verifying there was no further customer impact.\nWe are working on various measures to reduce spikes in queue build up during deployments of our queueing system, and have scaled up the workers which handle secret generation and storage during the workflow invocation process.\n\nMar 11, 19:21 UTC\nUpdate - Actions experienced a period of decreased workflow run throughput, and we are seeing recovery now.  We are in the process of investigating the cause.\nMar 11, 19:02 UTC\nInvestigating - We are investigating reports of degraded performance for Actions",
          "link": "https://www.githubstatus.com/incidents/bgnqgwyj76l4",
          "publishedOn": "2024-03-11T19:22:16.000Z",
          "wordCount": 5518,
          "title": "Incident with Actions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/7f4bllnv3h1n",
          "author": null,
          "description": "Mar 11, 10:20 UTC\nResolved - On March 11, 2024, between 06:30 UTC and 11:45 UTC the Copilot Chat service was degraded and customers may have encountered errors or timed out requests for chat interactions. On average, the error rate was 10% and peaked at 45% of requests to the service for short periods of time.\nThis was due to a gap in handling an edge case for messages returned from the underlying language models. We mitigated the incident by applying a fix to the handling of the streaming response.\nWe are working to update monitoring to reduce time to detection and increase resiliency to message format changes.\nMar 11, 10:02 UTC\nUpdate - We are deploying mitigations for the failures we have been observing in some chat requests for Copilot. We will continue to monitor and update.\nMar 11, 09:03 UTC\nUpdate - We are seeing an elevated failure rate for chat requests for Copilot. We are investigating and will continue to keep users updated on progress towards mitigation.\nMar 11, 08:14 UTC\nInvestigating - We are investigating reports of degraded performance for Copilot",
          "link": "https://www.githubstatus.com/incidents/7f4bllnv3h1n",
          "publishedOn": "2024-03-11T10:20:15.000Z",
          "wordCount": 5431,
          "title": "Incident with Copilot",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/7x5z8plb48t6",
          "author": null,
          "description": "Mar  1, 17:42 UTC\nResolved - On March 1, 2024, between 17:00 UTC and 17:42 UTC, we saw elevated failure rates (from 1 to 10%) for Copilot, Actions, Pages, and Git for various APIs.\nThis incident was triggered by a newly-discovered failure mode of a deployment pipeline to one of our compute clusters when it could not write a specific configuration file. This caused a drop in the amount of resources available in this cluster, which was mitigated by a redeployment.\nWe have addressed the specific scenario to ensure resources are properly written and retrieved and added safeguards to ensure the deployment does not proceed if there is an issue of this type.  We are also reviewing our systems to more effectively route traffic toward healthy clusters during an outage and adding more safeguards on cluster resource adjustments.\n\nMar  1, 17:42 UTC\nUpdate - Git Operations is operating normally.\nMar  1, 17:41 UTC\nUpdate - Actions and Pages are operating normally.\nMar  1, 17:36 UTC\nUpdate - Copilot is operating normally.\nMar  1, 17:34 UTC\nUpdate - Pages is experiencing degraded performance. We are continuing to investigate.\nMar  1, 17:34 UTC\nUpdate - One of our clusters is experiencing problems, and we are working on restoring the cluster at this time.\nMar  1, 17:30 UTC\nInvestigating - We are investigating reports of degraded performance for API Requests, Copilot, Git Operations and Actions",
          "link": "https://www.githubstatus.com/incidents/7x5z8plb48t6",
          "publishedOn": "2024-03-01T17:42:41.000Z",
          "wordCount": 5512,
          "title": "Incident with API Requests, Copilot, Git Operations, Actions and Pages",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/wcl1sw4mzg60",
          "author": null,
          "description": "Mar  1, 16:12 UTC\nResolved - On March 1, 2024, between 14:17 UTC and 15:54 UTC the service that sends messages from our event stream into our background job processing service was degraded and delayed the transmission of jobs for processing.  No data or jobs were lost.  From 14:17 to 14:41 UTC, there was a partial degradation, where customers would experience intermittent delays with PRs and Actions.  From 14:41 to 15:24 UTC, 36% of PRs users saw stale data, and 100% of in progress Actions workflows did not see updates , even though the workflows were succeeding.  At 15:24 UTC, we mitigated the incident by redeploying our service and jobs began to burn down, with full job catchup by 15:54 UTC. This was due to under provisioned memory and lack of memory based back pressure in the service, which overwhelmed consumers and led to OutOfMemory crashes.\nWe have adjusted memory configurations to prevent this problem, and are analyzing and adjusting our alert sensitivity to reduce our time to detection of issues like this one in the future.\n\nMar  1, 16:12 UTC\nUpdate - Issues, Pull Requests and Actions are operating normally.\nMar  1, 15:48 UTC\nUpdate - We're seeing our background job queue sizes trend down, and expect full recovery in the next 15 minutes.\nMar  1, 15:39 UTC\nUpdate - Issues is experiencing degraded performance. We are continuing to investigate.\nMar  1, 15:27 UTC\nUpdate - We're continuing to investigate issues with background jobs that have impacted Actions and Pull Requests. We have a mitigation in place and are monitoring for recovery.\nMar  1, 14:51 UTC\nUpdate - We're investigating issues with background jobs that are causing sporadic delays in pull request synchronization and reduced Actions throughput.\nMar  1, 14:39 UTC\nInvestigating - We are investigating reports of degraded performance for Pull Requests and Actions",
          "link": "https://www.githubstatus.com/incidents/wcl1sw4mzg60",
          "publishedOn": "2024-03-01T16:12:23.000Z",
          "wordCount": 5575,
          "title": "Incident with Pull Requests, Actions and Issues",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/5lc3f39mjcq8",
          "author": null,
          "description": "Feb 29, 12:27 UTC\nResolved - On February 29, 2024, between 9:32 and 11:54 UTC, queuing in our background job service caused processing delays to Webhooks, Actions, and Issues. Nearly 95% of delays occurred between 11:05 and 11:27 UTC, with 5% during the remainder of the incident. During this incident, the following customer impacts occurred: 50% of webhooks experienced delays of up to 5m, 1% of webhooks experienced delays of 17m at peak; Actions: on average, 7% of customers experienced delays, with a peak of 44%; and many Issues saw a delay in appearing in searches. At 9:32 UTC our automated failover successfully routed traffic to a secondary cluster. But an improper restoration to primary at 10:32 UTC caused a significant increase in queued jobs until 11:21 UTC, when a correction was made and healthy services began burning down the backlog until full resolution.\nWe have made improvements to the automation and reliability of our fallback process to prevent recurrence. We also have larger work already in progress to improve the overall reliability of our job processing platform.\n\nFeb 29, 12:21 UTC\nUpdate - We're seeing recovery and are going to take time to verify that all systems are back in a working state.\nFeb 29, 12:19 UTC\nUpdate - Issues is operating normally.\nFeb 29, 12:18 UTC\nUpdate - Webhooks is operating normally.\nFeb 29, 11:05 UTC\nUpdate - We're continuing to investigate delayed background jobs. We've seen partial recovery for Issues, and there is ongoing impact to actions, notifications and webhooks.\nFeb 29, 10:58 UTC\nUpdate - Actions is experiencing degraded performance. We are continuing to investigate.\nFeb 29, 10:36 UTC\nUpdate - We're seeing issues related to background jobs, which are causing delays for webhook delivery and search indexing, and other updates.\nFeb 29, 10:33 UTC\nInvestigating - We are investigating reports of degraded performance for Issues and Webhooks",
          "link": "https://www.githubstatus.com/incidents/5lc3f39mjcq8",
          "publishedOn": "2024-02-29T12:27:17.000Z",
          "wordCount": 5579,
          "title": "Incident with Issues, Webhooks and Actions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/f9ntcnd3fdvs",
          "author": null,
          "description": "Feb 26, 21:40 UTC\nResolved - On Monday, February 26th, from 20:45 UTC to 21:39 UTC, GitHub Packages reported an outage indicating a degradation in GitHub Container Registry and NPM package upload functionality. Upon investigation, we found a misconfigured observability metric which inadvertently pulled in data from a newly provisioned test environment. All failures being reported were traced back to this test environment. We confirmed that there was no real customer impact to GitHub Packages during this incident. We have since reconfigured our observability metrics to accurately report based on environment.\nFeb 26, 21:20 UTC\nUpdate - We are seeing some recovery in NPM and GitHub Container Registry functionality, but are maintaining red status until we are certain issues won’t recur.\nFeb 26, 21:03 UTC\nUpdate - NPM and GitHub Container Registry services are degraded, particularly the upload functionality. Investigation is underway.\nFeb 26, 21:01 UTC\nInvestigating - We are currently investigating this issue.",
          "link": "https://www.githubstatus.com/incidents/f9ntcnd3fdvs",
          "publishedOn": "2024-02-26T21:40:00.000Z",
          "wordCount": 5410,
          "title": "We are investigating reports of degraded performance.",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/78qz8zwhx9tj",
          "author": null,
          "description": "Feb 26, 19:37 UTC\nResolved - On February 26, 2024, between 18:34 UTC and 19:37 UTC our background job service was degraded and caused job start delays up to 15 minutes. Users experienced delays in Webhooks, Actions, and some UI updates (e.g. a delay in UI updates on pull requests). This was due to capacity problems with our job queueing service, and a failure of our automated failover system.\nWe mitigated the incident by manually failing over to our secondary cluster.   No data was lost - recovery began at 18:55 UTC, when the backlog of enqueued jobs began to process.\nWe are actively working to repair our failover automation and expand the capacity of our background job queuing service to prevent issues like this in the future.\n\nFeb 26, 19:37 UTC\nUpdate - Actions and Pull Requests are operating normally.\nFeb 26, 19:37 UTC\nUpdate - Webhooks and Issues are operating normally.\nFeb 26, 19:05 UTC\nUpdate - Issues is experiencing degraded performance. We are continuing to investigate.\nFeb 26, 18:57 UTC\nUpdate - Pull Requests is experiencing degraded performance. We are continuing to investigate.\nFeb 26, 18:55 UTC\nUpdate - We have deployed a fix for issues affecting Webhooks, Actions, and some other services. We are beginning to see recovery and will continue to monitor and fix as needed.\nFeb 26, 18:55 UTC\nUpdate - Webhooks is experiencing degraded performance. We are continuing to investigate.\nFeb 26, 18:48 UTC\nUpdate - Actions is experiencing degraded performance. We are continuing to investigate.\nFeb 26, 18:47 UTC\nInvestigating - We are investigating reports of degraded performance for Webhooks",
          "link": "https://www.githubstatus.com/incidents/78qz8zwhx9tj",
          "publishedOn": "2024-02-26T19:37:32.000Z",
          "wordCount": 5543,
          "title": "Incident with Webhooks, Actions, Pull Requests and Issues",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        },
        {
          "id": "https://www.githubstatus.com/incidents/wn6s1w8vkk1y",
          "author": null,
          "description": "Feb 21, 17:30 UTC\nResolved - On Wednesday February 21, 2024, 17:07 UTC, we deployed a configuration change to one of our services inside of Actions. At 17:14 UTC we noticed an increase in exceptions that impacted approximately 85% of runs at that time. \nAt 17:18 UTC, we reverted the deployment and our service immediately recovered. During this timeframe, customers may have noticed their workflows failed to trigger or workflows were queued but did not progress.\nTo prevent this issue in the future we are improving our deployment observability tooling to detect errors earlier in the deployment pipeline.\nFeb 21, 17:20 UTC\nInvestigating - We are investigating reports of degraded performance for Actions",
          "link": "https://www.githubstatus.com/incidents/wn6s1w8vkk1y",
          "publishedOn": "2024-02-21T17:30:06.000Z",
          "wordCount": 5356,
          "title": "Incident with Actions",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/pages-twitter_logos/original/36420/GitHub-Mark-120px-plus.png"
        }
      ]
    },
    {
      "title": "Slack System Status",
      "feedUrl": "https://status.slack.com/feed/rss",
      "siteUrl": "https://slack-status.com",
      "articles": [
        {
          "id": "https://slack-status.com/2024-03/070ad43095b540ab",
          "author": null,
          "description": "Issue summary: \r\nFrom 12:40 PM to 4:55 PM PDT on March 13, 2024, users from one Enterprise Grid organization may have experienced slowness or connectivity issues when using Slack.\r\n\r\nA routine infrastructure process inadvertently caused an increase in requests to the database hosting the affected Enterprise Grid organization. This resulted in slower than usual response times for user actions in Slack, and some connectivity failures. \r\n\r\nWe identified the cause of the problem and manually increased the database capacity, restoring normal functionality for impacted users. \r\n\r\nThank you for bearing with us while we resolved this issue.",
          "link": "https://slack-status.com/2024-03/070ad43095b540ab",
          "publishedOn": "2024-03-15T01:35:34.000Z",
          "wordCount": 264,
          "title": "Incident: Reports of slowness and connectivity issues in Slack",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/e954ecf8fbab9711",
          "author": null,
          "description": "Issue summary: \r\nOn March 13, 2024 from 5:02 PM PDT until 9:48 PM PDT some users may have encountered \"Something's gone awry\" or \"Slack isn't loading\" errors when attempting to access their certain app configuration pages. \r\n\r\nWe determined that a recent code change caused this issue. We reverted the code change and the issue was resolved.",
          "link": "https://slack-status.com/2024-03/e954ecf8fbab9711",
          "publishedOn": "2024-03-14T11:01:37.000Z",
          "wordCount": 245,
          "title": "Incident: User are receiving errors when attempting to access certain app configuration pages.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/811378903bd22810",
          "author": null,
          "description": "Issue summary:\r\nOn March 11, 2024 from 2:43 PM PDT until 3:43 PM PDT some users in workspaces using International Data Residency (IDR) experienced issues with several Slack features, including messages loading or sending, channels appearing blank, canvases loading, huddles connecting, and trouble with some workflows were also noted.\r\n\r\nWe identified a spike in error rates that was tied to a recent change to a backend service configuration logic. This change resulted in a cache discrepancy between services causing the errors. We rolled back the recent changes which immediately reduced error rates, resolving the issue for affected users. We are working on modifying the service handler in an effort to reduce the likelihood of this occurring in the future.",
          "link": "https://slack-status.com/2024-03/811378903bd22810",
          "publishedOn": "2024-03-12T20:18:00.000Z",
          "wordCount": 315,
          "title": "Incident: Trouble loading multiple features in Slack",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/c60bb5e0edd2cd89",
          "author": null,
          "description": "Issue Summary:\r\nOn March 11, 2024 at 3:00 PDT to March 12, 2024 at 12:00 AM PDT, some users had trouble completing workflows that contained either text or drop-down fields due to a bug that caused the cursor to revert to the previous field in the workflow.\r\n\r\nUpon investigation, we discovered a recent code change that introduced this bug. We rolled back the change and pushed a fix to all users resolving the issue.\r\n\r\nThank you again for bearing with us in the meantime.",
          "link": "https://slack-status.com/2024-03/c60bb5e0edd2cd89",
          "publishedOn": "2024-03-12T19:21:05.000Z",
          "wordCount": 335,
          "title": "Incident: Issue with focus in workflows",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/1f506e0a906e75bb",
          "author": null,
          "description": "Issue summary:\r\n\r\nOn March 6, 2024 from 7:45 PM PST to around 9:33 PM PST customers reported that they were receiving a 500 error when viewing the legacy Incoming Webhook custom integration page.\r\n\r\nWe identified a backend code change that inadvertently changed data that the Incoming Webhook page needed to load which consequently caused the page to respond with an HTTP 500 error.\r\n\r\nWe rolled back the change, correcting the issue and all customers should now be able to access the Incoming Webhook page as expected.",
          "link": "https://slack-status.com/2024-03/1f506e0a906e75bb",
          "publishedOn": "2024-03-08T14:35:09.000Z",
          "wordCount": 259,
          "title": "Incident: Issues with legacy custom integration configuration pages",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/b9c8573e14eec256",
          "author": null,
          "description": "Issue summary:\r\nOn March 5, 2024 from 6:05 PM PST to March 6, 2024 8:17 AM PST customers on the Windows operating systems reported that Huddles were freezing when screen sharing.\r\nWe determined that a recent change that was made to the video codec of Huddles, which may have been moving more of the video encoding to the computer's Graphic Processing Unit. This would cause performance problems & freezing during a screen share.\r\nAfter narrowing down the exact cause we rolled back the change which restored full functionality to Huddles screen sharing.",
          "link": "https://slack-status.com/2024-03/b9c8573e14eec256",
          "publishedOn": "2024-03-08T10:54:49.000Z",
          "wordCount": 244,
          "title": "Incident: Huddles not working for some Windows users",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/a20334cecc6fff7c",
          "author": null,
          "description": "Issue summary:\r\nOn March 5, 2024 from 3:36 PM PST until 9:36 PM PST some customers were unable to visit the app detail page in App Directory, instead seeing a page displaying a \"Server Error\" message.\r\n\r\nWe traced the issue to a recent backend code change and reverted the change, which fixed this issue.",
          "link": "https://slack-status.com/2024-03/a20334cecc6fff7c",
          "publishedOn": "2024-03-06T07:23:50.000Z",
          "wordCount": 214,
          "title": "Incident: Some customers may be unable to access the App Directory",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/9c34d28b9b3f4e7b",
          "author": null,
          "description": "Issue summary:\r\nFrom March 1, 2024 at 12:13 PM PST to March 4, 2024 at 7:06 PM PST, Enterprise Grid admins and owners may have experienced issues inviting members to workspaces. \r\n\r\nWe made a code change that inadvertently introduced a logic error, resulting in the incorrect workspaces being listed in the invite modal. We reverted the code change, restoring normal invite functionality for all affected users.",
          "link": "https://slack-status.com/2024-03/9c34d28b9b3f4e7b",
          "publishedOn": "2024-03-06T05:00:19.000Z",
          "wordCount": 280,
          "title": "Incident: 'Invite people' popup menu in Manage Members page populates name of wrong workspace.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-03/629a8aa472f5f9c2",
          "author": null,
          "description": "Issue summary:\r\nOn March 4, 2024, from 4:18 PM PST until 10:17 PM PST some users experienced an issue when sharing their screen during a huddle. Attempting to share their screen would fail without any error or warning.\r\n\r\nAn incorrect piece of code was identified as the cause of the screen share issue. We safely reverted the problematic code, resolving the issue that users experienced. To capture the fix, a hard refresh (by pressing Cmd/Ctrl + Shift + R) may be required.",
          "link": "https://slack-status.com/2024-03/629a8aa472f5f9c2",
          "publishedOn": "2024-03-05T04:32:04.000Z",
          "wordCount": 343,
          "title": "Incident: Some customers are unable to share their screen during a huddle",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/35134eda119581f5",
          "author": null,
          "description": "Issue summary: \r\nOn February 29, 2024 from 1:28 PM PST to around 4:36 PM PST, users were unable to create or update some workflows.\r\n\r\nWe determined that a recent code change had inadvertently introduced a logic problem, resulting in errors for workflows that used event triggers. We reverted this code change. However, rolling back reintroduced a separate issue that we'd resolved earlier in the day. \r\n\r\nWe prepared a tailored revert to roll back the code causing the workflow problem while also including the fix for the previous issue. This second revert resolved both problems, restoring full functionality to Slack. \r\n\r\nThank you for being patient with us.",
          "link": "https://slack-status.com/2024-02/35134eda119581f5",
          "publishedOn": "2024-03-01T05:48:33.000Z",
          "wordCount": 378,
          "title": "Incident: Users unable to create or edit workflows",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/939919184ea7655b",
          "author": null,
          "description": "Issue summary: \r\nOn February 29, 2024 at 8:00 AM PST to 9:55 AM PST, users saw errors when attempting to trigger and manage legacy workflows.\r\n\r\nUpon investigation, we discovered a code change prevented part of the infrastructure that supports workflows from functioning correctly.\r\n\r\nTo resolve the issue, we reverted the code change and all workflows were restored.",
          "link": "https://slack-status.com/2024-02/939919184ea7655b",
          "publishedOn": "2024-02-29T20:47:52.000Z",
          "wordCount": 223,
          "title": "Incident: Workflows are unable to be viewed or triggered",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/2ea2fda2874858b7",
          "author": null,
          "description": "Issue Summary: \r\nOn February 27, 2024 from around 8:30 PM PST to 10:06 PM PST Workspace Owners and Admins were experiencing issues while setting up two-factor authentication during sign in on iOS devices.\r\n\r\nThe issue was caused by a two-factor authentication feature deploy to users on the Pro subscription. After identifying the root cause the feature was was rolled back and this allowed all users to log back into Slack.",
          "link": "https://slack-status.com/2024-02/2ea2fda2874858b7",
          "publishedOn": "2024-02-28T05:55:13.000Z",
          "wordCount": 235,
          "title": "Incident: Trouble signing in on mobile for some Owners and Admins",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/cedce965afd47ebb",
          "author": null,
          "description": "Issue summary: \r\nOn February 26, 2024, from 9:21 AM PST to 9:16 AM PST on February 27, some users encountered the close button disappearing in the right pane where threads and profiles are viewed.\r\n\r\nWe discovered this was caused by a missing check in the change workspace process that prevented the right pane elements from rendering properly. Once the cause was identified, a change was made to the logic for drawing Slack's interface when switching workspaces. Once the change was rolled out, the issue was resolved.",
          "link": "https://slack-status.com/2024-02/cedce965afd47ebb",
          "publishedOn": "2024-02-27T15:41:56.000Z",
          "wordCount": 303,
          "title": "Incident: Close button for right pane not appearing for some users",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/b1389c8cd318a7d1",
          "author": null,
          "description": "Issue summary:\r\nFrom 12:30 PM PST to 1:05 PM PST on February 26, 2024, users were unable to create workflows.\r\n\r\nWe determined that the related API method needed updating in order to eliminate inconsistencies that were causing elevated replication lag.\r\n\r\nWe deployed an update to the API method correcting this issue, and all workflow creation will now work as expected.",
          "link": "https://slack-status.com/2024-02/b1389c8cd318a7d1",
          "publishedOn": "2024-02-27T02:50:48.000Z",
          "wordCount": 245,
          "title": "Incident: Users are not able to create workflows",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/4c0af99013428a7a",
          "author": null,
          "description": "Issue summary:\r\nOn February 26, 2024 from 9:00 AM PST to 9:36 AM PST, some users experienced problems sending and loading messages in Slack. Others may have completed these actions successfully but more slowly than expected.\r\n\r\nEarlier today, we deployed a routine update to part of our network infrastructure. During this work, we deprovisioned some of our proxy servers and provisioned a new set. Some of our webapp servers took longer than expected to sync with the new list of proxy servers, resulting in a spike in connection errors. This coincided with the regular 9:00 AM PST increase in traffic to our servers, which may have exacerbated impact.\r\n\r\nBy around 9:36 AM PST, error rates had decreased and user impact had completely subsided, most likely due to our automatic scaling functionality. However, we're continuing to analyze data from this event to fully understand the root cause and prevent similar issues in the future.",
          "link": "https://slack-status.com/2024-02/4c0af99013428a7a",
          "publishedOn": "2024-02-27T01:47:26.000Z",
          "wordCount": 304,
          "title": "Incident: Issues sending and loading messages",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/b4d41d5daac6431c",
          "author": null,
          "description": "Issue Summary:\r\n\r\nFrom 2:58 PM PST on February 22, 2024, until 9:47 PM PST on February 25, 2024, some users were unable to access their workspace billing history.\r\n\r\nWe traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users.\r\n\r\nThank you very much for your patience while we resolved this.",
          "link": "https://slack-status.com/2024-02/b4d41d5daac6431c",
          "publishedOn": "2024-02-26T14:30:45.000Z",
          "wordCount": 292,
          "title": "Incident: Issues with the billing history page",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/548b72776056c03b",
          "author": null,
          "description": "Issue summary:\r\nFrom 2:54 PM to 2:59 PM PDT on February 22, 2024, some users were having issues loading Threads.\r\n\r\nWe carried out a thorough investigation and observed trends, but found no evidence of an issue on our side. By around 2:59 PM PDT, error rates had already subsided and we received confirmation from several users that they could load Threads again by reloading Slack.\r\n\r\nWhilst no remedial action was taken on our end, we're analyzing data from this event to understand ways to mitigate similar issues that may occur in the future.\r\n\r\nWe're confident that there will be no further impact to users.",
          "link": "https://slack-status.com/2024-02/548b72776056c03b",
          "publishedOn": "2024-02-23T03:05:59.000Z",
          "wordCount": 291,
          "title": "Incident: Threads aren't loading for some users",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/83f4c18fc4d11c93",
          "author": null,
          "description": "Issue summary:\r\nOn February 21, 2024 from 6:30 PM PST to 8:00 PM PST, customers using the JAWS or NVDA screen readers on Windows may have experienced issues with announcements for unread messages in the Quick Switcher (Ctrl + K). \r\n\r\nA recent code change inadvertently introduced a conflict with an HTML label for an interactive element.\r\n\r\nWe reverted the code change, fixing the issue and restoring normal announcement behaviour.",
          "link": "https://slack-status.com/2024-02/83f4c18fc4d11c93",
          "publishedOn": "2024-02-23T01:46:26.000Z",
          "wordCount": 263,
          "title": "Incident: Screen readers are not announcing unread messages when using the Ctrl + K command on Windows.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-02/e04276624660caf9",
          "author": null,
          "description": "Issue Summary:\r\n\r\nBetween 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. \r\n\r\nWe traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. \r\n\r\nThank you for your patience while we resolved this and we apologize for any disruption to your work day.",
          "link": "https://slack-status.com/2024-02/e04276624660caf9",
          "publishedOn": "2024-02-22T14:32:08.000Z",
          "wordCount": 207,
          "title": "Incident: Trouble with password resets",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        },
        {
          "id": "https://slack-status.com/2024-01/f39851209d6c471a",
          "author": null,
          "description": "Issue Summary:\r\n\r\nBetween 1:18 AM PST on February 21, 2024, and 5:35 AM PST on February 22, 2024, some users experienced issues resetting their passwords, especially when trying to set up two-factor authentication. \r\n\r\nWe traced the issue to a recent backend code change and reverted the change, which fixed the issue for all affected users. \r\n\r\nThank you for your patience while we resolved this and we apologize for any disruption to your work day.",
          "link": "https://slack-status.com/2024-01/f39851209d6c471a",
          "publishedOn": "2024-02-22T14:31:13.000Z",
          "wordCount": 409,
          "title": "Incident: Some users are unable to upload, download, and view files in Slack.",
          "imageUrl": "https://status.slack.com/img/v2_rebrand/slack_hash_256.png"
        }
      ]
    },
    {
      "title": "Make Status - Incident History",
      "feedUrl": "https://status.make.com/history.rss",
      "siteUrl": "https://status.make.com",
      "articles": [
        {
          "id": "https://status.make.com/incidents/1vm1k0hxtbdb",
          "author": null,
          "description": "Mar  4, 11:12 CET\nResolved - We are still in contact with our third-party provider to gather more information and prevent this issue from happening again. Meanwhile, the workaround that was implemented has been working as expected. The incident has been resolved.\nMar  1, 17:44 CET\nMonitoring - We have successfully implemented a workaround, and the content of the affected pages is now accessible. However, we are still awaiting a response from the third-party service and further insight into the issue. Currently, the entire webpage is operational. We will continue to monitor the situation closely until we receive additional information from the third party.\nMar  1, 16:51 CET\nUpdate - We are still investigating the issue, and our initial suspicion is that it may be related to one of our third-party services. We anticipate providing the next update within the next 2 hours, or as soon as more information becomes available.\nMar  1, 16:02 CET\nInvestigating - Our Help and API documentation websites are currently unreachable due to unknown reasons. We are currently investigating the root cause. The rest of the platform's functionalities are unaffected. We will provide an update in the next 30 minutes. Sorry for the inconvenience.",
          "link": "https://status.make.com/incidents/1vm1k0hxtbdb",
          "publishedOn": "2024-03-04T10:12:14.000Z",
          "wordCount": 4012,
          "title": "Help website is currently unreachable",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.make.com/incidents/smcl14qd1qxw",
          "author": null,
          "description": "Feb 27, 00:06 CET\nResolved - This incident has been resolved.\nFeb 26, 21:58 CET\nMonitoring - A fix has been implemented and we are monitoring the results.\nFeb 26, 21:34 CET\nInvestigating - We're encountering issues executing scenarios on eu2.make.com. Users may experience delays and errors when attempting to execute any scenarios. There are no other issues with scenario creation or general connectivity to Make. We're currently investigating this issue and will update this Statuspage within the next hour, or as more information comes to hand.",
          "link": "https://status.make.com/incidents/smcl14qd1qxw",
          "publishedOn": "2024-02-26T23:06:42.000Z",
          "wordCount": 3904,
          "title": "Problem with executing scenarios",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        },
        {
          "id": "https://status.make.com/incidents/6hnyx0v9zjbh",
          "author": null,
          "description": "Feb 26, 08:00 CET\nCompleted - The scheduled maintenance has been completed.\nFeb 26, 06:00 CET\nIn progress - Scheduled maintenance is currently in progress. We will provide updates as necessary.\nFeb 15, 16:40 CET\nScheduled - Hello,\nPlease note there is scheduled infrastructure maintenance on 26.02.2024, between 6:00-8:00 AM CET, when you can expect a slower scenario processing. Specifically, scenarios using a datastore module might see reduced performance which can cause its longer execution.\nWe will inform you once the maintenance is completed.\nThank you very much for understanding.\nKind regards,\nMake team",
          "link": "https://status.make.com/incidents/6hnyx0v9zjbh",
          "publishedOn": "2024-02-26T07:00:56.000Z",
          "wordCount": 3902,
          "title": "Scheduled infrastructure maintenance",
          "imageUrl": "https://dka575ofm4ao0.cloudfront.net/assets/logos/favicon-2b86ed00cfa6258307d4a3d0c482fd733c7973f82de213143b24fc062c540367.png"
        }
      ]
    }
  ],
  "cliVersion": "1.15.1"
}